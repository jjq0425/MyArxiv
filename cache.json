{"2025-05-16T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2505.11459v1","updated":"2025-05-16T17:13:45Z","published":"2025-05-16T17:13:45Z","title":"ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks","summary":"  The integration of large language models (LLMs) into a wide range of\napplications has highlighted the critical role of well-crafted system prompts,\nwhich require extensive testing and domain expertise. These prompts enhance\ntask performance but may also encode sensitive information and filtering\ncriteria, posing security risks if exposed. Recent research shows that system\nprompts are vulnerable to extraction attacks, while existing defenses are\neither easily bypassed or require constant updates to address new threats. In\nthis work, we introduce ProxyPrompt, a novel defense mechanism that prevents\nprompt leakage by replacing the original prompt with a proxy. This proxy\nmaintains the original task's utility while obfuscating the extracted prompt,\nensuring attackers cannot reproduce the task or access sensitive information.\nComprehensive evaluations on 264 LLM and system prompt pairs show that\nProxyPrompt protects 94.70% of prompts from extraction attacks, outperforming\nthe next-best defense, which only achieves 42.80%.\n","authors":["Zhixiong Zhuang","Maria-Irina Nicolae","Hui-Po Wang","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2505.11459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11449v1","updated":"2025-05-16T17:05:25Z","published":"2025-05-16T17:05:25Z","title":"LLMs unlock new paths to monetizing exploits","summary":"  We argue that Large language models (LLMs) will soon alter the economics of\ncyberattacks. Instead of attacking the most commonly used software and\nmonetizing exploits by targeting the lowest common denominator among victims,\nLLMs enable adversaries to launch tailored attacks on a user-by-user basis. On\nthe exploitation front, instead of human attackers manually searching for one\ndifficult-to-identify bug in a product with millions of users, LLMs can find\nthousands of easy-to-identify bugs in products with thousands of users. And on\nthe monetization front, instead of generic ransomware that always performs the\nsame attack (encrypt all your data and request payment to decrypt), an\nLLM-driven ransomware attack could tailor the ransom demand based on the\nparticular content of each exploited device.\n  We show that these two attacks (and several others) are imminently practical\nusing state-of-the-art LLMs. For example, we show that without any human\nintervention, an LLM finds highly sensitive personal information in the Enron\nemail dataset (e.g., an executive having an affair with another employee) that\ncould be used for blackmail. While some of our attacks are still too expensive\nto scale widely today, the incentives to implement these attacks will only\nincrease as LLMs get cheaper. Thus, we argue that LLMs create a need for new\ndefense-in-depth approaches.\n","authors":["Nicholas Carlini","Milad Nasr","Edoardo Debenedetti","Barry Wang","Christopher A. Choquette-Choo","Daphne Ippolito","Florian Tramèr","Matthew Jagielski"],"pdf_url":"https://arxiv.org/pdf/2505.11449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15592v2","updated":"2025-05-16T16:01:57Z","published":"2025-04-22T05:13:54Z","title":"Yet Another Diminishing Spark: Low-level Cyberattacks in the Israel-Gaza\n  Conflict","summary":"  We report empirical evidence of web defacement and DDoS attacks carried out\nby low-level cybercrime actors in the Israel-Gaza conflict. Our quantitative\nmeasurements indicate an immediate increase in such cyberattacks following the\nHamas-led assault and the subsequent declaration of war. However, the surges\nwaned quickly after a few weeks, with patterns resembling those observed in the\naftermath of the Russian invasion of Ukraine. The scale of attacks and\ndiscussions within the hacking community this time was both significantly lower\nthan those during the early days of the Russia-Ukraine war, and attacks have\nbeen prominently one-sided: many pro-Palestinian supporters have targeted\nIsrael, while attacks on Palestine have been much less significant. Beyond\ntargeting these two, attackers also defaced sites of other countries to express\ntheir war support. Their broader opinions are also largely disparate, with far\nmore support for Palestine and many objections expressed toward Israel.\n","authors":["Anh V. Vu","Alice Hutchings","Ross Anderson"],"pdf_url":"https://arxiv.org/pdf/2504.15592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03203v5","updated":"2025-05-16T15:48:40Z","published":"2025-02-05T14:23:43Z","title":"FSLH: Flexible Mechanized Speculative Load Hardening","summary":"  The Spectre speculative side-channel attacks pose formidable threats for\nsecurity. Research has shown that code following the cryptographic\nconstant-time discipline can be efficiently protected against Spectre v1 using\na selective variant of Speculative Load Hardening (SLH). SLH was, however, not\nstrong enough for protecting non-cryptographic code, leading to the\nintroduction of Ultimate SLH, which provides protection for arbitrary programs,\nbut has too large overhead for general use, since it conservatively assumes\nthat all data is secret. In this paper we introduce a flexible SLH notion that\nachieves the best of both worlds by generalizing both Selective and Ultimate\nSLH. We give a suitable security definition for such transformations protecting\narbitrary programs: any transformed program running with speculation should not\nleak more than what the source program leaks sequentially. We formally prove\nusing the Rocq prover that two flexible SLH variants enforce this relative\nsecurity guarantee. As easy corollaries we also obtain that, in our setting,\nUltimate SLH enforces our relative security notion, and two selective SLH\nvariants enforce speculative constant-time security.\n","authors":["Jonathan Baumann","Roberto Blanco","Léon Ducruet","Sebastian Harwig","Catalin Hritcu"],"pdf_url":"https://arxiv.org/pdf/2502.03203v5.pdf","comment":"CSF'25 camera-ready version extended version with appendices"},{"id":"http://arxiv.org/abs/2501.16466v3","updated":"2025-05-16T14:55:52Z","published":"2025-01-27T19:58:29Z","title":"On the Feasibility of Using LLMs to Autonomously Execute Multi-host\n  Network Attacks","summary":"  LLMs have shown preliminary promise in some security tasks and CTF\nchallenges. Real cyberattacks are often multi-host network attacks, which\ninvolve executing a number of steps across multiple hosts such as conducting\nreconnaissance, exploiting vulnerabilities, and using compromised hosts to\nexfiltrate data. To date, the extent to which LLMs can autonomously execute\nmulti-host network attacks} is not well understood. To this end, our first\ncontribution is MHBench, an open-source multi-host attack benchmark with 10\nrealistic emulated networks (from 25 to 50 hosts). We find that popular LLMs\nincluding modern reasoning models (e.g., GPT4o, Gemini 2.5 Pro, Sonnet 3.7\nThinking) with state-of-art security-relevant prompting strategies (e.g.,\nPentestGPT, CyberSecEval3) cannot autonomously execute multi-host network\nattacks. To enable LLMs to autonomously execute such attacks, our second\ncontribution is Incalmo, an high-level abstraction layer. Incalmo enables LLMs\nto specify high-level actions (e.g., infect a host, scan a network). Incalmo's\ntranslation layer converts these actions into lower-level primitives (e.g.,\ncommands to exploit tools) through expert agents. In 9 out of 10 networks in\nMHBench, LLMs using Incalmo achieve at least some of the attack goals. Even\nsmaller LLMs (e.g., Haiku 3.5, Gemini 2 Flash) equipped with Incalmo achieve\nall goals in 5 of 10 environments. We also validate the key role of high-level\nactions in Incalmo's abstraction in enabling LLMs to autonomously execute such\nattacks.\n","authors":["Brian Singer","Keane Lucas","Lakshmi Adiga","Meghna Jain","Lujo Bauer","Vyas Sekar"],"pdf_url":"https://arxiv.org/pdf/2501.16466v3.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.11320v1","updated":"2025-05-16T14:42:51Z","published":"2025-05-16T14:42:51Z","title":"Understanding and Characterizing Obfuscated Funds Transfers in Ethereum\n  Smart Contracts","summary":"  Scam contracts on Ethereum have rapidly evolved alongside the rise of DeFi\nand NFT ecosystems, utilizing increasingly complex code obfuscation techniques\nto avoid early detection. This paper systematically investigates how\nobfuscation amplifies the financial risks of fraudulent contracts and\nundermines existing auditing tools. We propose a transfer-centric obfuscation\ntaxonomy, distilling seven key features, and introduce ObfProbe, a framework\nthat performs bytecode-level smart contract analysis to uncover obfuscation\ntechniques and quantify obfuscation complexity via Z-score ranking. In a\nlarge-scale study of 1.03 million Ethereum contracts, we isolate over 3 000\nhighly obfuscated contracts and identify two scam archetypes, three high-risk\ncontract categories, and MEV bots that employ a variety of obfuscation\nmaneuvers such as inline assembly, dead code insertion, and deep function\nsplitting. We further show that obfuscation substantially increases both the\nscale of financial damage and the time until detection. Finally, we evaluate\nSourceP, a state-of-the-art Ponzi detection tool, on obfuscated versus\nnon-obfuscated samples and observe its accuracy drop from approximately 80\npercent to approximately 12 percent in real-world scenarios. These findings\nhighlight the urgent need for enhanced anti-obfuscation analysis techniques and\nbroader community collaboration to stem the proliferation of scam contracts in\nthe expanding DeFi ecosystem.\n","authors":["Zhang Sheng","Tan Kia Quang","Shen Wang","Shengchen Duan","Kai Li","Yue Duan"],"pdf_url":"https://arxiv.org/pdf/2505.11320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06556v5","updated":"2025-05-16T14:35:18Z","published":"2024-09-10T14:37:43Z","title":"Adversary Resilient Learned Bloom Filters","summary":"  A learned Bloom filter (LBF) combines a classical Bloom filter (CBF) with a\nlearning model to reduce the amount of memory needed to represent a given set\nwhile achieving a target false positive rate (FPR). Provable security against\nadaptive adversaries that advertently attempt to increase FPR has been studied\nfor CBFs. However, achieving adaptive security for LBFs is an open problem. In\nthis paper, we close this gap and show how to achieve adaptive security for\nLBFs. In particular, we define several adaptive security notions capturing\nvarying degrees of adversarial control, including full and partial adaptivity,\nin addition to LBF extensions of existing adversarial models for CBFs,\nincluding the Always-Bet and Bet-or-Pass notions. We propose two secure LBF\nconstructions, PRP-LBF and Cuckoo-LBF, and formally prove their security under\nthese models, assuming the existence of one-way functions. Based on our\nanalysis and use case evaluations, our constructions achieve strong security\nguarantees while maintaining competitive FPR and memory overhead.\n","authors":["Ghada Almashaqbeh","Allison Bishop","Hayder Tirmazi"],"pdf_url":"https://arxiv.org/pdf/2409.06556v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16116v2","updated":"2025-05-16T12:00:59Z","published":"2025-04-18T16:40:39Z","title":"DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across\n  the Web3 Domain","summary":"  Large Language Models (LLMs) have achieved impressive performance in diverse\nnatural language processing tasks, but specialized domains such as Web3 present\nnew challenges and require more tailored evaluation. Despite the significant\nuser base and capital flows in Web3, encompassing smart contracts,\ndecentralized finance (DeFi), non-fungible tokens (NFTs), decentralized\nautonomous organizations (DAOs), on-chain governance, and novel\ntoken-economics, no comprehensive benchmark has systematically assessed LLM\nperformance in this domain. To address this gap, we introduce the DMind\nBenchmark, a holistic Web3-oriented evaluation suite covering nine critical\nsubfields: fundamental blockchain concepts, blockchain infrastructure, smart\ncontract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and\nsecurity vulnerabilities. Beyond multiple-choice questions, DMind Benchmark\nfeatures domain-specific tasks such as contract debugging and on-chain numeric\nreasoning, mirroring real-world scenarios. We evaluated 26 models, including\nChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable\nperformance gaps in specialized areas like token economics and\nsecurity-critical contract analysis. While some models excel in blockchain\ninfrastructure tasks, advanced subfields remain challenging. Our benchmark\ndataset and evaluation pipeline are open-sourced on\nhttps://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in\nHugging Face's trending dataset charts within a week of release.\n","authors":["Enhao Huang","Pengyu Sun","Zixin Lin","Alex Chen","Joey Ouyang","Hobert Wang","Dong Dong","Gang Zhao","James Yi","Frank Li","Ziang Ling","Lowes Yang"],"pdf_url":"https://arxiv.org/pdf/2504.16116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11154v1","updated":"2025-05-16T11:55:12Z","published":"2025-05-16T11:55:12Z","title":"MPMA: Preference Manipulation Attack Against Model Context Protocol","summary":"  Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem.\n","authors":["Zihan Wang","Hongwei Li","Rui Zhang","Yu Liu","Wenbo Jiang","Wenshu Fan","Qingchuan Zhao","Guowen Xu"],"pdf_url":"https://arxiv.org/pdf/2505.11154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11097v1","updated":"2025-05-16T10:28:30Z","published":"2025-05-16T10:28:30Z","title":"Verifiably Forgotten? Gradient Differences Still Enable Data\n  Reconstruction in Federated Unlearning","summary":"  Federated Unlearning (FU) has emerged as a critical compliance mechanism for\ndata privacy regulations, requiring unlearned clients to provide verifiable\nProof of Federated Unlearning (PoFU) to auditors upon data removal requests.\nHowever, we uncover a significant privacy vulnerability: when gradient\ndifferences are used as PoFU, honest-but-curious auditors may exploit\nmathematical correlations between gradient differences and forgotten samples to\nreconstruct the latter. Such reconstruction, if feasible, would face three key\nchallenges: (i) restricted auditor access to client-side data, (ii) limited\nsamples derivable from individual PoFU, and (iii) high-dimensional redundancy\nin gradient differences. To overcome these challenges, we propose Inverting\nGradient difference to Forgotten data (IGF), a novel learning-based\nreconstruction attack framework that employs Singular Value Decomposition (SVD)\nfor dimensionality reduction and feature extraction. IGF incorporates a\ntailored pixel-level inversion model optimized via a composite loss that\ncaptures both structural and semantic cues. This enables efficient and\nhigh-fidelity reconstruction of large-scale samples, surpassing existing\nmethods. To counter this novel attack, we design an orthogonal obfuscation\ndefense that preserves PoFU verification utility while preventing sensitive\nforgotten data reconstruction. Experiments across multiple datasets validate\nthe effectiveness of the attack and the robustness of the defense. The code is\navailable at https://anonymous.4open.science/r/IGF.\n","authors":["Fuyao Zhang","Wenjie Li","Yurong Hao","Xinyu Yan","Yang Cao","Wei Yang Bryan Lim"],"pdf_url":"https://arxiv.org/pdf/2505.11097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11094v1","updated":"2025-05-16T10:26:15Z","published":"2025-05-16T10:26:15Z","title":"Blockchain-Enabled Decentralized Privacy-Preserving Group Purchasing for\n  Energy Plans","summary":"  Retail energy markets are increasingly consumer-oriented, thanks to a growing\nnumber of energy plans offered by a plethora of energy suppliers, retailers and\nintermediaries. To maximize the benefits of competitive retail energy markets,\ngroup purchasing is an emerging paradigm that aggregates consumers' purchasing\npower by coordinating switch decisions to specific energy providers for\ndiscounted energy plans. Traditionally, group purchasing is mediated by a\ntrusted third-party, which suffers from the lack of privacy and transparency.\nIn this paper, we introduce a novel paradigm of decentralized\nprivacy-preserving group purchasing, empowered by privacy-preserving blockchain\nand secure multi-party computation, to enable users to form a coalition for\ncoordinated switch decisions in a decentralized manner, without a trusted\nthird-party. The coordinated switch decisions are determined by a competitive\nonline algorithm, based on users' private consumption data and current energy\nplan tariffs. Remarkably, no private user consumption data will be revealed to\nothers in the online decision-making process, which is carried out in a\ntransparently verifiable manner to eliminate frauds from dishonest users and\nsupports fair mutual compensations by sharing the switching costs to\nincentivize group purchasing. We implemented our decentralized group purchasing\nsolution as a smart contract on Solidity-supported blockchain platform (e.g.,\nEthereum), and provide extensive empirical evaluation.\n","authors":["Sid Chi-Kin Chau","Yue Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.11094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11058v1","updated":"2025-05-16T09:56:03Z","published":"2025-05-16T09:56:03Z","title":"Side Channel Analysis in Homomorphic Encryption","summary":"  Homomorphic encryption provides many opportunities for privacy-aware\nprocessing, including with methods related to machine learning. Many of our\nexisting cryptographic methods have been shown in the past to be susceptible to\nside channel attacks. With these, the implementation of the cryptographic\nmethods can reveal information about the private keys used, the result, or even\nthe original plaintext. An example of this includes the processing of the RSA\nexponent using the Montgomery method, and where 0's and 1's differ in their\nprocessing time for modular exponentiation. With FHE, we typically use lattice\nmethods, and which can have particular problems in their implementation in\nrelation to side channel leakage. This paper aims to outline a range of\nweaknesses within FHE implementations as related to side channel analysis. It\noutlines a categorization for side-channel analysis, some case studies, and\nmitigation strategies.\n","authors":["Baraq Ghaleb","William J Buchanan"],"pdf_url":"https://arxiv.org/pdf/2505.11058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11049v1","updated":"2025-05-16T09:46:10Z","published":"2025-05-16T09:46:10Z","title":"GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning","summary":"  To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/\n","authors":["Yue Liu","Shengfang Zhai","Mingzhe Du","Yulin Chen","Tri Cao","Hongcheng Gao","Cheng Wang","Xinfeng Li","Kun Wang","Junfeng Fang","Jiaheng Zhang","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2505.11049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09921v2","updated":"2025-05-16T09:36:29Z","published":"2025-05-15T03:11:57Z","title":"PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative\n  In-Context Optimization","summary":"  Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at https://github.com/redwyd/PrivacyJailbreak.\n","authors":["Yidan Wang","Yanan Cao","Yubing Ren","Fang Fang","Zheng Lin","Binxing Fang"],"pdf_url":"https://arxiv.org/pdf/2505.09921v2.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2505.09924v2","updated":"2025-05-16T09:33:36Z","published":"2025-05-15T03:12:36Z","title":"From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models","summary":"  The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\nhttps://github.com/redwyd/SymMark.\n","authors":["Yidan Wang","Yubing Ren","Yanan Cao","Binxing Fang"],"pdf_url":"https://arxiv.org/pdf/2505.09924v2.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2505.11016v1","updated":"2025-05-16T09:10:07Z","published":"2025-05-16T09:10:07Z","title":"GoLeash: Mitigating Golang Software Supply Chain Attacks with Runtime\n  Policy Enforcement","summary":"  Modern software supply chain attacks consist of introducing new, malicious\ncapabilities into trusted third-party software components, in order to\npropagate to a victim through a package dependency chain. These attacks are\nespecially concerning for the Go language ecosystem, which is extensively used\nin critical cloud infrastructures. We present GoLeash, a novel system that\napplies the principle of least privilege at the package-level granularity, by\nenforcing distinct security policies for each package in the supply chain. This\nfiner granularity enables GoLeash to detect malicious packages more precisely\nthan traditional sandboxing that handles security policies at process- or\ncontainer-level. Moreover, GoLeash remains effective under obfuscation, can\novercome the limitations of static analysis, and incurs acceptable runtime\noverhead.\n","authors":["Carmine Cesarano","Martin Monperrus","Roberto Natella"],"pdf_url":"https://arxiv.org/pdf/2505.11016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10983v1","updated":"2025-05-16T08:29:56Z","published":"2025-05-16T08:29:56Z","title":"GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on\n  Genomic Foundation Models","summary":"  We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,\nGenoArmory offers the first comprehensive evaluation framework to\nsystematically assess the vulnerability of GFMs to adversarial attacks.\nMethodologically, we evaluate the adversarial robustness of five\nstate-of-the-art GFMs using four widely adopted attack algorithms and three\ndefense strategies. Importantly, our benchmark provides an accessible and\ncomprehensive framework to analyze GFM vulnerabilities with respect to model\narchitecture, quantization schemes, and training datasets. Additionally, we\nintroduce GenoAdv, a new adversarial sample dataset designed to improve GFM\nsafety. Empirically, classification models exhibit greater robustness to\nadversarial perturbations compared to generative models, highlighting the\nimpact of task type on model vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features.\n","authors":["Haozheng Luo","Chenghao Qiu","Yimin Wang","Shang Wu","Jiahao Yu","Han Liu","Binghui Wang","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2505.10983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10965v1","updated":"2025-05-16T08:03:02Z","published":"2025-05-16T08:03:02Z","title":"Privacy and Confidentiality Requirements Engineering for Process Data","summary":"  The application and development of process mining techniques face significant\nchallenges due to the lack of publicly available real-life event logs. One\nreason for companies to abstain from sharing their data are privacy and\nconfidentiality concerns. Privacy concerns refer to personal data as specified\nin the GDPR and have been addressed in existing work by providing\nprivacy-preserving techniques for event logs. However, the concept of\nconfidentiality in event logs not pertaining to individuals remains unclear,\nalthough they might contain a multitude of sensitive business data. This work\naddresses confidentiality of process data based on the privacy and\nconfidentiality engineering method (PCRE). PCRE interactively explores privacy\nand confidentiality requirements regarding process data with different\nstakeholders and defines privacy-preserving actions to address possible\nconcerns. We co-construct and evaluate PCRE based on structured interviews with\nprocess analysts in two manufacturing companies. PCRE is generic, hence\napplicable in different application domains. The goal is to systematically\nscrutinize process data and balance the trade-off between privacy and utility\nloss.\n","authors":["Fabian Haertel","Juergen Mangler","Nataliia Klievtsova","Celine Mader","Eugen Rigger","Stefanie Rinderle-Ma"],"pdf_url":"https://arxiv.org/pdf/2505.10965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10942v1","updated":"2025-05-16T07:28:15Z","published":"2025-05-16T07:28:15Z","title":"Nosy Layers, Noisy Fixes: Tackling DRAs in Federated Learning Systems\n  using Explainable AI","summary":"  Federated Learning (FL) has emerged as a powerful paradigm for collaborative\nmodel training while keeping client data decentralized and private. However, it\nis vulnerable to Data Reconstruction Attacks (DRA) such as \"LoKI\" and \"Robbing\nthe Fed\", where malicious models sent from the server to the client can\nreconstruct sensitive user data. To counter this, we introduce DRArmor, a novel\ndefense mechanism that integrates Explainable AI with targeted detection and\nmitigation strategies for DRA. Unlike existing defenses that focus on the\nentire model, DRArmor identifies and addresses the root cause (i.e., malicious\nlayers within the model that send gradients with malicious intent) by analyzing\ntheir contribution to the output and detecting inconsistencies in gradient\nvalues. Once these malicious layers are identified, DRArmor applies defense\ntechniques such as noise injection, pixelation, and pruning to these layers\nrather than the whole model, minimizing the attack surface and preserving\nclient data privacy. We evaluate DRArmor's performance against the advanced\nLoKI attack across diverse datasets, including MNIST, CIFAR-10, CIFAR-100, and\nImageNet, in a 200-client FL setup. Our results demonstrate DRArmor's\neffectiveness in mitigating data leakage, achieving high True Positive and True\nNegative Rates of 0.910 and 0.890, respectively. Additionally, DRArmor\nmaintains an average accuracy of 87%, effectively protecting client privacy\nwithout compromising model performance. Compared to existing defense\nmechanisms, DRArmor reduces the data leakage rate by 62.5% with datasets\ncontaining 500 samples per client.\n","authors":["Meghali Nandi","Arash Shaghaghi","Nazatul Haque Sultan","Gustavo Batista","Raymond K. Zhao","Sanjay Jha"],"pdf_url":"https://arxiv.org/pdf/2505.10942v1.pdf","comment":"Accepted to AsiaCCS 2025"},{"id":"http://arxiv.org/abs/2505.10932v1","updated":"2025-05-16T07:11:04Z","published":"2025-05-16T07:11:04Z","title":"Enhanced Multiuser CSI-Based Physical Layer Authentication Based on\n  Information Reconciliation","summary":"  This paper presents a physical layer authentication (PLA) technique using\ninformation reconciliation in multiuser communication systems. A cost-effective\nsolution for low-end Internet of Things networks can be provided by PLA. In\nthis work, we develop an information reconciliation scheme using Polar codes\nalong with a quantization strategy that employs an arbitrary number of bits to\nenhance the performance of PLA. We employ the principle of Slepian-Wolf coding\nto reconcile channel measurements spread in time. Numerical results show that\nour approach works very well and outperforms competing approaches, achieving\nmore than 99.80% increase in detection probability for false alarm\nprobabilities close to 0.\n","authors":["Atsu Kokuvi Angélo Passah","Arsenia Chorti","Rodrigo C. de Lamare"],"pdf_url":"https://arxiv.org/pdf/2505.10932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10924v1","updated":"2025-05-16T06:56:42Z","published":"2025-05-16T06:56:42Z","title":"A Survey on the Safety and Security Threats of Computer-Using Agents:\n  JARVIS or Ultron?","summary":"  Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents.\n","authors":["Ada Chen","Yongjiang Wu","Junyuan Zhang","Shu Yang","Jen-tse Huang","Kun Wang","Wenxuan Wang","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10903v1","updated":"2025-05-16T06:15:31Z","published":"2025-05-16T06:15:31Z","title":"On the Security Risks of ML-based Malware Detection Systems: A Survey","summary":"  Malware presents a persistent threat to user privacy and data integrity. To\ncombat this, machine learning-based (ML-based) malware detection (MD) systems\nhave been developed. However, these systems have increasingly been attacked in\nrecent years, undermining their effectiveness in practice. While the security\nrisks associated with ML-based MD systems have garnered considerable attention,\nthe majority of prior works is limited to adversarial malware examples, lacking\na comprehensive analysis of practical security risks. This paper addresses this\ngap by utilizing the CIA principles to define the scope of security risks. We\nthen deconstruct ML-based MD systems into distinct operational stages, thus\ndeveloping a stage-based taxonomy. Utilizing this taxonomy, we summarize the\ntechnical progress and discuss the gaps in the attack and defense proposals\nrelated to the ML-based MD systems within each stage. Subsequently, we conduct\ntwo case studies, using both inter-stage and intra-stage analyses according to\nthe stage-based taxonomy to provide new empirical insights. Based on these\nanalyses and insights, we suggest potential future directions from both\ninter-stage and intra-stage perspectives.\n","authors":["Ping He","Yuhao Mao","Changjiang Li","Lorenzo Cavallaro","Ting Wang","Shouling Ji"],"pdf_url":"https://arxiv.org/pdf/2505.10903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10871v1","updated":"2025-05-16T05:25:11Z","published":"2025-05-16T05:25:11Z","title":"Optimal Allocation of Privacy Budget on Hierarchical Data Release","summary":"  Releasing useful information from datasets with hierarchical structures while\npreserving individual privacy presents a significant challenge. Standard\nprivacy-preserving mechanisms, and in particular Differential Privacy, often\nrequire careful allocation of a finite privacy budget across different levels\nand components of the hierarchy. Sub-optimal allocation can lead to either\nexcessive noise, rendering the data useless, or to insufficient protections for\nsensitive information. This paper addresses the critical problem of optimal\nprivacy budget allocation for hierarchical data release. It formulates this\nchallenge as a constrained optimization problem, aiming to maximize data\nutility subject to a total privacy budget while considering the inherent\ntrade-offs between data granularity and privacy loss. The proposed approach is\nsupported by theoretical analysis and validated through comprehensive\nexperiments on real hierarchical datasets. These experiments demonstrate that\noptimal privacy budget allocation significantly enhances the utility of the\nreleased data and improves the performance of downstream tasks.\n","authors":["Joonhyuk Ko","Juba Ziani","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2505.10871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10864v1","updated":"2025-05-16T05:07:08Z","published":"2025-05-16T05:07:08Z","title":"Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign\n  Sensing with Physically Realizable Wearable Oscillators","summary":"  Recent advancements in Ultra-Wideband (UWB) radar technology have enabled\ncontactless, non-line-of-sight vital sign monitoring, making it a valuable tool\nfor healthcare. However, UWB radar's ability to capture sensitive physiological\ndata, even through walls, raises significant privacy concerns, particularly in\nhuman-robot interactions and autonomous systems that rely on radar for sensing\nhuman presence and physiological functions. In this paper, we present\nAnti-Sensing, a novel defense mechanism designed to prevent unauthorized\nradar-based sensing. Our approach introduces physically realizable\nperturbations, such as oscillatory motion from wearable devices, to disrupt\nradar sensing by mimicking natural cardiac motion, thereby misleading heart\nrate (HR) estimations. We develop a gradient-based algorithm to optimize the\nfrequency and spatial amplitude of these oscillations for maximal disruption\nwhile ensuring physiological plausibility. Through both simulations and\nreal-world experiments with radar data and neural network-based HR sensing\nmodels, we demonstrate the effectiveness of Anti-Sensing in significantly\ndegrading model accuracy, offering a practical solution for privacy\npreservation.\n","authors":["Md Farhan Tasnim Oshim","Nigel Doering","Bashima Islam","Tsui-Wei Weng","Tauhidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2505.10864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10846v1","updated":"2025-05-16T04:37:12Z","published":"2025-05-16T04:37:12Z","title":"AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models","summary":"  This paper presents AutoRAN, the first automated, weak-to-strong jailbreak\nattack framework targeting large reasoning models (LRMs). At its core, AutoRAN\nleverages a weak, less-aligned reasoning model to simulate the target model's\nhigh-level reasoning structures, generates narrative prompts, and iteratively\nrefines candidate prompts by incorporating the target model's intermediate\nreasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including\nGPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets\n(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN\nachieves remarkable success rates (approaching 100%) within one or a few turns\nacross different LRMs, even when judged by a robustly aligned external model.\nThis work reveals that leveraging weak reasoning models can effectively exploit\nthe critical vulnerabilities of much more capable reasoning models,\nhighlighting the need for improved safety measures specifically designed for\nreasoning-based models. The code for replicating AutoRAN and running records\nare available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:\nthis paper contains potentially harmful content generated by LRMs.)\n","authors":["Jiacheng Liang","Tanqiu Jiang","Yuhui Wang","Rongyi Zhu","Fenglong Ma","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10846v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.12628v2","updated":"2025-05-16T04:32:57Z","published":"2025-02-18T08:13:10Z","title":"Cryptanalysis on Lightweight Verifiable Homomorphic Encryption","summary":"  Verifiable Homomorphic Encryption (VHE) is a cryptographic technique that\nintegrates Homomorphic Encryption (HE) with Verifiable Computation (VC). It\nserves as a crucial technology for ensuring both privacy and integrity in\noutsourced computation, where a client sends input ciphertexts ct and a\nfunction f to a server and verifies the correctness of the evaluation upon\nreceiving the evaluation result f(ct) from the server. At CCS, Chatel et al.\nintroduced two lightweight VHE schemes: Replication Encoding (REP) and\nPolynomial Encoding (PE). A similar approach to REP was used by Albrecht et al.\nin Eurocrypt to develop a Verifiable Oblivious PRF scheme (vADDG). A key\napproach in these schemes is to embed specific secret information within HE\nciphertexts to verify homomorphic evaluations. This paper presents efficient\nattacks that exploit the homomorphic properties of encryption schemes. The one\nstrategy is to retrieve the secret information in encrypted state from the\ninput ciphertexts and then leverage it to modify the resulting ciphertext\nwithout being detected by the verification algorithm. The other is to exploit\nthe secret embedding structure to modify the evaluation function f into f'\nwhich works well on input values for verification purposes. Our forgery attack\non vADDG demonstrates that the proposed 80-bit security parameters in fact\noffer less than 10-bits of concrete security. Our attack on REP and PE achieves\na probability 1 attack with linear time complexity when using fully homomorphic\nencryption.\n","authors":["Jung Hee Cheon","Daehyun Jang"],"pdf_url":"https://arxiv.org/pdf/2502.12628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10838v1","updated":"2025-05-16T04:12:16Z","published":"2025-05-16T04:12:16Z","title":"LARGO: Latent Adversarial Reflection through Gradient Optimization for\n  Jailbreaking LLMs","summary":"  Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization.\n","authors":["Ran Li","Hao Wang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2505.10838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10815v1","updated":"2025-05-16T03:18:46Z","published":"2025-05-16T03:18:46Z","title":"Enhancing Secrecy Energy Efficiency in RIS-Aided Aerial Mobile Edge\n  Computing Networks: A Deep Reinforcement Learning Approach","summary":"  This paper studies the problem of securing task offloading transmissions from\nground users against ground eavesdropping threats. Our study introduces a\nreconfigurable intelligent surface (RIS)-aided unmanned aerial vehicle\n(UAV)-mobile edge computing (MEC) scheme to enhance the secure task offloading\nwhile minimizing the energy consumption of the UAV subject to task completion\nconstraints. Leveraging a data-driven approach, we propose a comprehensive\noptimization strategy that jointly optimizes the aerial MEC (AMEC)'s\ntrajectory, task offloading partitioning, UE transmission scheduling, and RIS\nphase shifts. Our objective centers on optimizing the secrecy energy efficiency\n(SEE) of UE task offloading transmissions while preserving the AMEC's energy\nresources and meeting the task completion time requirements. Numerical results\nshow that the proposed solution can effectively safeguard legitimate task\noffloading transmissions while preserving AMEC energy.\n","authors":["Aly Sabri Abdalla","Vuk Marojevic"],"pdf_url":"https://arxiv.org/pdf/2505.10815v1.pdf","comment":"This article has been accepted for publication in the IEEE 2025\n  International Conference on Communications (ICC2025)"},{"id":"http://arxiv.org/abs/2505.10812v1","updated":"2025-05-16T03:12:38Z","published":"2025-05-16T03:12:38Z","title":"RAN Tester UE: An Automated Declarative UE Centric Security Testing\n  Platform","summary":"  Cellular networks require strict security procedures and measures across\nvarious network components, from core to radio access network (RAN) and\nend-user devices. As networks become increasingly complex and interconnected,\nas in O-RAN deployments, they are exposed to a numerous security threats.\nTherefore, ensuring robust security is critical for O-RAN to protect network\nintegrity and safeguard user data. This requires rigorous testing methodologies\nto mitigate threats. This paper introduces an automated, adaptive, and scalable\nuser equipment (UE) based RAN security testing framework designed to address\nthe shortcomings of existing RAN testing solutions. Experimental results on a\n5G software radio testbed built with commercial off-the-shelf hardware and open\nsource software validate the efficiency and reproducibility of sample security\ntest procedures developed on the RAN Tester UE framework.\n","authors":["Charles Marion Ueltschey","Joshua Moore","Aly Sabri Abdalla","Vuk Marojevic"],"pdf_url":"https://arxiv.org/pdf/2505.10812v1.pdf","comment":"This article has been accepted for publication in the ACM Symposium\n  on Access Control Models and Technologies (SACMAT'25)"},{"id":"http://arxiv.org/abs/2410.08604v4","updated":"2025-05-16T02:31:25Z","published":"2024-10-11T08:00:49Z","title":"MergePrint: Merge-Resistant Fingerprints for Robust Black-box Ownership\n  Verification of Large Language Models","summary":"  Protecting the intellectual property of Large Language Models (LLMs) has\nbecome increasingly critical due to the high cost of training. Model merging,\nwhich integrates multiple expert models into a single multi-task model,\nintroduces a novel risk of unauthorized use of LLMs due to its efficient\nmerging process. While fingerprinting techniques have been proposed for\nverifying model ownership, their resistance to model merging remains\nunexplored. To address this gap, we propose a novel fingerprinting method,\nMergePrint, which embeds robust fingerprints capable of surviving model\nmerging. MergePrint enables black-box ownership verification, where owners only\nneed to check if a model produces target outputs for specific fingerprint\ninputs, without accessing model weights or intermediate outputs. By optimizing\nagainst a pseudo-merged model that simulates merged behavior, MergePrint\nensures fingerprints that remain detectable after merging. Additionally, to\nminimize performance degradation, we pre-optimize the fingerprint inputs.\nMergePrint pioneers a practical solution for black-box ownership verification,\nprotecting LLMs from misappropriation via merging, while also excelling in\nresistance to broader model theft threats.\n","authors":["Shojiro Yamabe","Futa Waseda","Tsubasa Takahashi","Koki Wataoka"],"pdf_url":"https://arxiv.org/pdf/2410.08604v4.pdf","comment":"Accepted at ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.10790v1","updated":"2025-05-16T02:05:13Z","published":"2025-05-16T02:05:13Z","title":"Neural-Inspired Advances in Integral Cryptanalysis","summary":"  The study by Gohr et.al at CRYPTO 2019 and sunsequent related works have\nshown that neural networks can uncover previously unused features, offering\nnovel insights into cryptanalysis. Motivated by these findings, we employ\nneural networks to learn features specifically related to integral properties\nand integrate the corresponding insights into optimized search frameworks.\nThese findings validate the framework of using neural networks for feature\nexploration, providing researchers with novel insights that advance established\ncryptanalysis methods.\n  Neural networks have inspired the development of more precise integral search\nmodels. By comparing the integral distinguishers obtained via neural networks\nwith those identified by classical methods, we observe that existing automated\nsearch models often fail to find optimal distinguishers. To address this issue,\nwe develop a meet in the middle search framework that balances model accuracy\nand computational efficiency. As a result, we reduce the number of active\nplaintext bits required for an 11 rounds integral distinguisher on SKINNY64/64,\nand further identify a 12 rounds key dependent integral distinguisher achieving\none additional round over the previous best-known result.\n  The integral distinguishers discovered by neural networks enable key recovery\nattacks on more rounds. We identify a 7 rounds key independent integral\ndistinguisher from neural networks with even only one active plaintext cell,\nwhich is based on linear combinations of bits. This distinguisher enables a 15\nrounds key recovery attack on SKINNYn/n, improving upon the previous record by\none round. Additionally, we discover an 8 rounds key dependent integral\ndistinguisher using neural network that further reduces the time complexity of\nkey recovery attacks against SKINNY.\n","authors":["Liu Zhang","Yiran Yao","Danping Shi","Dongchen Chai","Jian Guo","Zilong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.10790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12009v2","updated":"2025-05-16T01:49:58Z","published":"2025-01-21T10:02:51Z","title":"Ratio Attack on G+G Convoluted Gaussian Signature","summary":"  A lattice-based signature, called G+G convoluted Gaussian signature, was\nproposed in ASIACRYPT 2023 and was proved secure in the quantum random oracle\nmodel. In this paper, we propose a ratio attack on the G+G convoluted Gaussian\nsignature to recover the secret key and comment on the revised eprint paper.\nThe attack exploits the fact, proved in this paper, that the secret key can be\nobtained from the expected value of the ratio of signatures which follows a\ntruncated Cauchy distribution. Moreover, we also compute the number of\nsignatures required to successfully recover the secret key. Furthermore, we\nsimulate the ratio attack in Sagemath with a few different parameters as a\nproof-of-concept of the ratio attack. In addition, although the revised\nsignature in the revised eprint paper is secure against the ratio attack, we\nfound that either a valid signature cannot be produced or a signature can be\nforged easily for their given parameters in the eprint.\n","authors":["Chik How Tan","Theo Fanuela Prabowo","Wei Guo Foo"],"pdf_url":"https://arxiv.org/pdf/2501.12009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09604v3","updated":"2025-05-16T01:12:44Z","published":"2025-04-13T14:42:03Z","title":"Mitigating Many-Shot Jailbreaking","summary":"  Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the\nlong context windows of modern LLMs to circumvent model safety training by\nincluding in the prompt many examples of a \"fake\" assistant responding\ninappropriately before the final request. With enough examples, the model's\nin-context learning abilities override its safety training, and it responds as\nif it were the \"fake\" assistant. In this work, we probe the effectiveness of\ndifferent fine-tuning and input sanitization approaches on mitigating MSJ\nattacks, alone and in combination. We find incremental mitigation effectiveness\nfor each, and show that the combined techniques significantly reduce the\neffectiveness of MSJ attacks, while retaining model performance in benign\nin-context learning and conversational tasks. We suggest that our approach\ncould meaningfully ameliorate this vulnerability if incorporated into model\nsafety post-training.\n","authors":["Christopher M. Ackerman","Nina Panickssery"],"pdf_url":"https://arxiv.org/pdf/2504.09604v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20093v4","updated":"2025-05-16T00:36:50Z","published":"2025-03-25T22:15:50Z","title":"SoK: Decoding the Enigma of Encrypted Network Traffic Classifiers","summary":"  The adoption of modern encryption protocols such as TLS 1.3 has significantly\nchallenged traditional network traffic classification (NTC) methods. As a\nconsequence, researchers are increasingly turning to machine learning (ML)\napproaches to overcome these obstacles. In this paper, we comprehensively\nanalyze ML-based NTC studies, developing a taxonomy of their design choices,\nbenchmarking suites, and prevalent assumptions impacting classifier\nperformance. Through this systematization, we demonstrate widespread reliance\non outdated datasets, oversights in design choices, and the consequences of\nunsubstantiated assumptions. Our evaluation reveals that the majority of\nproposed encrypted traffic classifiers have mistakenly utilized unencrypted\ntraffic due to the use of legacy datasets. Furthermore, by conducting 348\nfeature occlusion experiments on state-of-the-art classifiers, we show how\noversights in NTC design choices lead to overfitting, and validate or refute\nprevailing assumptions with empirical evidence. By highlighting lessons\nlearned, we offer strategic insights, identify emerging research directions,\nand recommend best practices to support the development of real-world\napplicable NTC methodologies.\n","authors":["Nimesha Wickramasinghe","Arash Shaghaghi","Gene Tsudik","Sanjay Jha"],"pdf_url":"https://arxiv.org/pdf/2503.20093v4.pdf","comment":"Published in the Proceedings of 2025 IEEE Symposium on Security and\n  Privacy (IEEE SP)"},{"id":"http://arxiv.org/abs/2505.07584v2","updated":"2025-05-16T00:34:44Z","published":"2025-05-12T14:09:24Z","title":"SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark\n  for Large Language Models","summary":"  The increasing deployment of large language models in security-sensitive\ndomains necessitates rigorous evaluation of their resilience against\nadversarial prompt-based attacks. While previous benchmarks have focused on\nsecurity evaluations with limited and predefined attack domains, such as\ncybersecurity attacks, they often lack a comprehensive assessment of\nintent-driven adversarial prompts and the consideration of real-life\nscenario-based multi-turn attacks. To address this gap, we present\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\nmodel assessment: one-off attack, successive attack, successive reverse attack,\nalternative attack, sequential ascending attack with escalating threat levels\nand sequential descending attack with diminishing threat levels. In addition,\nwe introduce a dataset customized for the benchmark, which incorporates both\nneutral and malicious prompts, categorised across seven security domains and\nsixteen attack techniques. In applying this benchmark, we systematically\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\ninsights into the strengths and weaknesses of modern large language models in\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\npublicly available at\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\nwhich provides a groundwork for advancing research in large language model\nsecurity.\n","authors":["Huining Cui","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2505.07584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03743v2","updated":"2025-05-16T00:26:37Z","published":"2025-04-07T00:27:04Z","title":"Implementation of Shor Algorithm: Factoring a 4096-Bit Integer Under\n  Specific Constraints","summary":"  In recent years, advancements in quantum chip technology, such as Willow,\nhave contributed to reducing quantum computation error rates, potentially\naccelerating the practical adoption of quantum computing. As a result, the\ndesign of quantum algorithms suitable for real-world applications has become a\ncrucial research direction. This study focuses on the implementation of Shor\nalgorithm, aiming to improve modular computation efficiency and demonstrate the\nfactorization of a 4096-bit integer under specific constraints. Experimental\nresults, when compared with state-of-the-art (SOTA) methods, indicate a\nsignificant improvement in efficiency while enabling the factorization of\nlonger integers.\n","authors":["Abel C. H. Chen"],"pdf_url":"https://arxiv.org/pdf/2505.03743v2.pdf","comment":"in Chinese language; some typographical errors were corrected on May\n  15, 2025"},{"id":"http://arxiv.org/abs/2505.10759v1","updated":"2025-05-16T00:20:02Z","published":"2025-05-16T00:20:02Z","title":"Random Client Selection on Contrastive Federated Learning for Tabular\n  Data","summary":"  Vertical Federated Learning (VFL) has revolutionised collaborative machine\nlearning by enabling privacy-preserving model training across multiple parties.\nHowever, it remains vulnerable to information leakage during intermediate\ncomputation sharing. While Contrastive Federated Learning (CFL) was introduced\nto mitigate these privacy concerns through representation learning, it still\nfaces challenges from gradient-based attacks. This paper presents a\ncomprehensive experimental analysis of gradient-based attacks in CFL\nenvironments and evaluates random client selection as a defensive strategy.\nThrough extensive experimentation, we demonstrate that random client selection\nproves particularly effective in defending against gradient attacks in the CFL\nnetwork. Our findings provide valuable insights for implementing robust\nsecurity measures in contrastive federated learning systems, contributing to\nthe development of more secure collaborative learning frameworks\n","authors":["Achmad Ginanjar","Xue Li","Priyanka Singh","Wen Hua"],"pdf_url":"https://arxiv.org/pdf/2505.10759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11744v1","updated":"2025-05-16T23:03:23Z","published":"2025-05-16T23:03:23Z","title":"Decentralized Multi-Authority Attribute-Based Inner-Product Functional\n  Encryption: Noisy and Evasive Constructions from Lattices","summary":"  We study multi-authority attribute-based functional encryption for noisy\ninner-product functionality, and propose two new primitives: (1)\nmulti-authority attribute-based (noisy) inner-product functional encryption\n(MA-AB(N)IPFE), which generalizes existing multi-authority attribute-based IPFE\nschemes by Agrawal et al. (TCC'21), by enabling approximate inner-product\ncomputation; and (2) multi-authority attribute-based evasive inner-product\nfunctional encryption (MA-evIPFE), a relaxed variant inspired by the evasive\nIPFE framework by Hsieh et al. (EUROCRYPT'24), shifting focus from ciphertext\nindistinguishability to a more relaxed pseudorandomness-based security notion.\nTo support the above notions, we introduce two variants of lattice-based\ncomputational assumptions: evasive IPFE assumption and\nindistinguishability-based evasive IPFE assumption (IND-evIPFE). We present\nlattice-based constructions of both primitives for subset policies, building\nupon the framework of Waters et al.( TCC'22). Our schemes are proven to be\nstatically secure in the random oracle model under the standard LWE assumption\nand the newly introduced assumptions. Additionally, we show our MA-AB(N)IPFE\nscheme can be transformed via modulus switching into a noiseless MA-IPFE scheme\nthat supports exact inner-product functionality. This yields the first\nlattice-based construction of such a primitive. All our schemes support\narbitrary polynomial-size attribute policies and are secure in the random\noracle model under lattice assumptions with a sub-exponential modulus-to-noise\nratio, making them practical candidates for noise-tolerant, fine-grained access\ncontrol in multi-authority settings.\n","authors":["Jiaqi Liu","Yan Wang","Fang-Wei Fu"],"pdf_url":"https://arxiv.org/pdf/2505.11744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11741v1","updated":"2025-05-16T23:00:19Z","published":"2025-05-16T23:00:19Z","title":"Diverging Towards Hallucination: Detection of Failures in\n  Vision-Language Models via Multi-token Aggregation","summary":"  Vision-language models (VLMs) now rival human performance on many multimodal\ntasks, yet they still hallucinate objects or generate unsafe text. Current\nhallucination detectors, e.g., single-token linear probing (SLP) and P(True),\ntypically analyze only the logit of the first generated token or just its\nhighest scoring component overlooking richer signals embedded within earlier\ntoken distributions. We demonstrate that analyzing the complete sequence of\nearly logits potentially provides substantially more diagnostic information. We\nemphasize that hallucinations may only emerge after several tokens, as subtle\ninconsistencies accumulate over time. By analyzing the Kullback-Leibler (KL)\ndivergence between logits corresponding to hallucinated and non-hallucinated\ntokens, we underscore the importance of incorporating later-token logits to\nmore accurately capture the reliability dynamics of VLMs. In response, we\nintroduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box\nmethod that aggregates logits from the first ten tokens using multi-token\nlog-likelihood ratios and self-attention. Despite the challenges posed by large\nvocabulary sizes and long logit sequences, MTRE remains efficient and\ntractable. On MAD-Bench, MM-SafetyBench, MathVista, and four\ncompositional-geometry benchmarks, MTRE improves AUROC by 9.4 +/- 1.3 points\nover SLP and by 12.1 +/- 1.7 points over P(True), setting a new\nstate-of-the-art in hallucination detection for open-source VLMs.\n","authors":["Geigh Zollicoffer","Minh Vu","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2505.11741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18575v3","updated":"2025-05-16T22:42:29Z","published":"2025-04-22T17:51:03Z","title":"WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks","summary":"  Autonomous UI agents powered by AI have tremendous potential to boost human\nproductivity by automating routine tasks such as filing taxes and paying bills.\nHowever, a major challenge in unlocking their full potential is security, which\nis exacerbated by the agent's ability to take action on their user's behalf.\nExisting tests for prompt injections in web agents either over-simplify the\nthreat by testing unrealistic scenarios or giving the attacker too much power,\nor look at single-step isolated tasks. To more accurately measure progress for\nsecure web agents, we introduce WASP -- a new publicly available benchmark for\nend-to-end evaluation of Web Agent Security against Prompt injection attacks.\nEvaluating with WASP shows that even top-tier AI models, including those with\nadvanced reasoning capabilities, can be deceived by simple, low-effort\nhuman-written injections in very realistic scenarios. Our end-to-end evaluation\nreveals a previously unobserved insight: while attacks partially succeed in up\nto 86% of the case, even state-of-the-art agents often struggle to fully\ncomplete the attacker goals -- highlighting the current state of security by\nincompetence.\n","authors":["Ivan Evtimov","Arman Zharmagambetov","Aaron Grattafiori","Chuan Guo","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2504.18575v3.pdf","comment":"Code and data: https://github.com/facebookresearch/wasp"},{"id":"http://arxiv.org/abs/2306.07992v2","updated":"2025-05-16T22:22:15Z","published":"2023-06-11T19:59:35Z","title":"Securing Visually-Aware Recommender Systems: An Adversarial Image\n  Reconstruction and Detection Framework","summary":"  With rich visual data, such as images, becoming readily associated with\nitems, visually-aware recommendation systems (VARS) have been widely used in\ndifferent applications. Recent studies have shown that VARS are vulnerable to\nitem-image adversarial attacks, which add human-imperceptible perturbations to\nthe clean images associated with those items. Attacks on VARS pose new security\nchallenges to a wide range of applications such as e-Commerce and social\nnetworks where VARS are widely used. How to secure VARS from such adversarial\nattacks becomes a critical problem. Currently, there is still a lack of\nsystematic study on how to design secure defense strategies against visual\nattacks on VARS. In this paper, we attempt to fill this gap by proposing an\nadversarial image reconstruction and detection framework to secure VARS. Our\nproposed method can simultaneously (1) secure VARS from adversarial attacks\ncharacterized by local perturbations by image reconstruction based on global\nvision transformers; and (2) accurately detect adversarial examples using a\nnovel contrastive learning approach. Meanwhile, our framework is designed to be\nused as both a filter and a detector so that they can be jointly trained to\nimprove the flexibility of our defense strategy to a variety of attacks and\nVARS models. We have conducted extensive experimental studies with two popular\nattack methods (FGSM and PGD). Our experimental results on two real-world\ndatasets show that our defense strategy against visual attacks is effective and\noutperforms existing methods on different attacks. Moreover, our method can\ndetect adversarial examples with high accuracy.\n","authors":["Minglei Yin","Bin Liu","Neil Zhenqiang Gong","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2306.07992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14462v2","updated":"2025-05-16T21:49:59Z","published":"2025-03-18T17:37:22Z","title":"Blockchain with proof of quantum work","summary":"  We propose a blockchain architecture in which mining requires a quantum\ncomputer. The consensus mechanism is based on proof of quantum work, a\nquantum-enhanced alternative to traditional proof of work that leverages\nquantum supremacy to make mining intractable for classical computers. We have\nrefined the blockchain framework to incorporate the probabilistic nature of\nquantum mechanics, ensuring stability against sampling errors and hardware\ninaccuracies. To validate our approach, we implemented a prototype blockchain\non four D-Wave(TM) quantum annealing processors geographically distributed\nwithin North America, demonstrating stable operation across hundreds of\nthousands of quantum hashing operations. Our experimental protocol follows the\nsame approach used in the recent demonstration of quantum supremacy [King et\nal. Science 2025], ensuring that classical computers cannot efficiently perform\nthe same computation task. By replacing classical machines with quantum systems\nfor mining, it is possible to significantly reduce the energy consumption and\nenvironmental impact traditionally associated with blockchain mining while\nproviding a quantum-safe layer of security. Beyond serving as a proof of\nconcept for a meaningful application of quantum computing, this work highlights\nthe potential for other near-term quantum computing applications using existing\ntechnology.\n","authors":["Mohammad H. Amin","Jack Raymond","Daniel Kinn","Firas Hamze","Kelsey Hamer","Joel Pasvolsky","William Bernoudy","Andrew D. King","Samuel Kortas"],"pdf_url":"https://arxiv.org/pdf/2503.14462v2.pdf","comment":"24 pages, 16 figures: v2 - expanded literature review, additional\n  figure, larger blockchains, correction of typos"},{"id":"http://arxiv.org/abs/2505.11710v1","updated":"2025-05-16T21:37:50Z","published":"2025-05-16T21:37:50Z","title":"Co-Evolutionary Defence of Active Directory Attack Graphs via\n  GNN-Approximated Dynamic Programming","summary":"  Modern enterprise networks increasingly rely on Active Directory (AD) for\nidentity and access management. However, this centralization exposes a single\npoint of failure, allowing adversaries to compromise high-value assets.\nExisting AD defense approaches often assume static attacker behavior, but\nreal-world adversaries adapt dynamically, rendering such methods brittle. To\naddress this, we model attacker-defender interactions in AD as a Stackelberg\ngame between an adaptive attacker and a proactive defender. We propose a\nco-evolutionary defense framework that combines Graph Neural Network\nApproximated Dynamic Programming (GNNDP) to model attacker strategies, with\nEvolutionary Diversity Optimization (EDO) to generate resilient blocking\nstrategies. To ensure scalability, we introduce a Fixed-Parameter Tractable\n(FPT) graph reduction method that reduces complexity while preserving strategic\nstructure. Our framework jointly refines attacker and defender policies to\nimprove generalization and prevent premature convergence. Experiments on\nsynthetic AD graphs show near-optimal results (within 0.1 percent of optimality\non r500) and improved performance on larger graphs (r1000 and r2000),\ndemonstrating the framework's scalability and effectiveness.\n","authors":["Diksha Goel","Hussain Ahmad","Kristen Moore","Mingyu Guo"],"pdf_url":"https://arxiv.org/pdf/2505.11710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11708v1","updated":"2025-05-16T21:29:55Z","published":"2025-05-16T21:29:55Z","title":"Unveiling the Black Box: A Multi-Layer Framework for Explaining\n  Reinforcement Learning-Based Cyber Agents","summary":"  Reinforcement Learning (RL) agents are increasingly used to simulate\nsophisticated cyberattacks, but their decision-making processes remain opaque,\nhindering trust, debugging, and defensive preparedness. In high-stakes\ncybersecurity contexts, explainability is essential for understanding how\nadversarial strategies are formed and evolve over time. In this paper, we\npropose a unified, multi-layer explainability framework for RL-based attacker\nagents that reveals both strategic (MDP-level) and tactical (policy-level)\nreasoning. At the MDP level, we model cyberattacks as a Partially Observable\nMarkov Decision Processes (POMDPs) to expose exploration-exploitation dynamics\nand phase-aware behavioural shifts. At the policy level, we analyse the\ntemporal evolution of Q-values and use Prioritised Experience Replay (PER) to\nsurface critical learning transitions and evolving action preferences.\nEvaluated across CyberBattleSim environments of increasing complexity, our\nframework offers interpretable insights into agent behaviour at scale. Unlike\nprevious explainable RL methods, which are often post-hoc, domain-specific, or\nlimited in depth, our approach is both agent- and environment-agnostic,\nsupporting use cases ranging from red-team simulation to RL policy debugging.\nBy transforming black-box learning into actionable behavioural intelligence,\nour framework enables both defenders and developers to better anticipate,\nanalyse, and respond to autonomous cyber threats.\n","authors":["Diksha Goel","Kristen Moore","Jeff Wang","Minjune Kim","Thanh Thi Nguyen"],"pdf_url":"https://arxiv.org/pdf/2505.11708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11706v1","updated":"2025-05-16T21:25:12Z","published":"2025-05-16T21:25:12Z","title":"Forensics of Error Rates of Quantum Hardware","summary":"  There has been a rise in third-party cloud providers offering quantum\nhardware as a service to improve performance at lower cost. Although these\nproviders provide flexibility to the users to choose from several qubit\ntechnologies, quantum hardware, and coupling maps; the actual execution of the\nprogram is not clearly visible to the customer. The success of the user\nprogram, in addition to various other metadata such as cost, performance, &\nnumber of iterations to converge, depends on the error rate of the backend\nused. Moreover, the third-party provider and/or tools (e.g., hardware allocator\nand mapper) may hold insider/outsider adversarial agents to conserve resources\nand maximize profit by running the quantum circuits on error-prone hardware.\nThus it is important to gain visibility of the backend from various\nperspectives of the computing process e.g., execution, transpilation and\noutcomes. In this paper, we estimate the error rate of the backend from the\noriginal and transpiled circuit. For the forensics, we exploit the fact that\nqubit mapping and routing steps of the transpilation process select qubits and\nqubit pairs with less single qubit and two-qubit gate errors to minimize\noverall error accumulation, thereby, giving us clues about the error rates of\nthe various parts of the backend. We ranked qubit links into bins based on ECR\nerror rates publicly available, and compared it to the rankings derived from\nour investigation of the relative frequency of a qubit link being chosen by the\ntranspiler. For upto 83.5% of the qubit links in IBM Sherbrooke and 80% in IBM\nBrisbane, 127 qubit IBM backends, we are able to assign a bin rank which has a\ndifference upto 2 with the bin rank assigned on the basis of actual error rate\ninformation.\n","authors":["Rupshali Roy","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2505.11706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06836v2","updated":"2025-05-16T21:06:08Z","published":"2025-05-11T04:16:16Z","title":"\"Explain, Don't Just Warn!\" -- A Real-Time Framework for Generating\n  Phishing Warnings with Contextual Cues","summary":"  Anti-phishing tools typically display generic warnings that offer users\nlimited explanation on why a website is considered malicious, which can prevent\nend-users from developing the mental models needed to recognize phishing cues\non their own. This becomes especially problematic when these tools inevitably\nfail - particularly against evasive threats, and users are found to be\nill-equipped to identify and avoid them independently. To address these\nlimitations, we present PhishXplain (PXP), a real-time explainable phishing\nwarning system designed to augment existing detection mechanisms. PXP empowers\nusers by clearly articulating why a site is flagged as malicious, highlighting\nsuspicious elements using a memory-efficient implementation of LLaMA 3.2. It\nutilizes a structured two-step prompt architecture to identify phishing\nfeatures, generate contextual explanations, and render annotated screenshots\nthat visually reinforce the warning. Longitudinally implementing PhishXplain\nover a month on 7,091 live phishing websites, we found that it can generate\nwarnings for 94% of the sites, with a correctness of 96%. We also evaluated\nPhishXplain through a user study with 150 participants split into two groups:\none received conventional, generic warnings, while the other interacted with\nPXP's explainable alerts. Participants who received the explainable warnings\nnot only demonstrated a significantly better understanding of phishing\nindicators but also achieved higher accuracy in identifying phishing threats,\neven without any warning. Moreover, they reported greater satisfaction and\ntrust in the warnings themselves. These improvements were especially pronounced\namong users with lower initial levels of cybersecurity proficiency and\nawareness. To encourage the adoption of this framework, we release PhishXplain\nas a browser extension.\n","authors":["Sayak Saha Roy","Cesar Torres","Shirin Nilizadeh"],"pdf_url":"https://arxiv.org/pdf/2505.06836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13778v2","updated":"2025-05-16T20:44:43Z","published":"2024-06-19T19:04:51Z","title":"Benchmarking Unsupervised Online IDS for Masquerade Attacks in CAN","summary":"  Vehicular controller area networks (CANs) are susceptible to masquerade\nattacks by malicious adversaries. In masquerade attacks, adversaries silence a\ntargeted ID and then send malicious frames with forged content at the expected\ntiming of benign frames. As masquerade attacks could seriously harm vehicle\nfunctionality and are the stealthiest attacks to detect in CAN, recent work has\ndevoted attention to compare frameworks for detecting masquerade attacks in\nCAN. However, most existing works report offline evaluations using CAN logs\nalready collected using simulations that do not comply with the domain's\nreal-time constraints. Here we contribute to advance the state of the art by\nintroducing a benchmark study of four different non-deep learning (DL)-based\nunsupervised online intrusion detection systems (IDS) for masquerade attacks in\nCAN. Our approach differs from existing benchmarks in that we analyze the\neffect of controlling streaming data conditions in a sliding window setting. In\ndoing so, we use realistic masquerade attacks being replayed from the ROAD\ndataset. We show that although benchmarked IDS are not effective at detecting\nevery attack type, the method that relies on detecting changes in the\nhierarchical structure of clusters of time series produces the best results at\nthe expense of higher computational overhead. We discuss limitations, open\nchallenges, and how the benchmarked methods can be used for practical\nunsupervised online CAN IDS for masquerade attacks.\n","authors":["Pablo Moriano","Steven C. Hespeler","Mingyan Li","Robert A. Bridges"],"pdf_url":"https://arxiv.org/pdf/2406.13778v2.pdf","comment":"17 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.17573v2","updated":"2025-05-16T19:15:40Z","published":"2024-10-23T05:54:41Z","title":"Securing Federated Learning against Backdoor Threats with Foundation\n  Model Integration","summary":"  Federated Learning (FL) enables decentralized model training while preserving\nprivacy. Recently, the integration of Foundation Models (FMs) into FL has\nenhanced performance but introduced a novel backdoor attack mechanism.\nAttackers can exploit FM vulnerabilities to embed backdoors into synthetic data\ngenerated by FMs. During global model fusion, these backdoors are transferred\nto the global model through compromised synthetic data, subsequently infecting\nall client models. Existing FL backdoor defenses are ineffective against this\nnovel attack due to its fundamentally different mechanism compared to classic\nones. In this work, we propose a novel data-free defense strategy that\naddresses both classic and novel backdoor attacks in FL. The shared attack\npattern lies in the abnormal activations within the hidden feature space during\nmodel aggregation. Hence, we propose to constrain internal activations to\nremain within reasonable ranges, effectively mitigating attacks while\npreserving model functionality. The activation constraints are optimized using\nsynthetic data alongside FL training. Extensive experiments demonstrate its\neffectiveness against both novel and classic backdoor attacks, outperforming\nexisting defenses.\n","authors":["Xiaohuan Bi","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11611v1","updated":"2025-05-16T18:20:42Z","published":"2025-05-16T18:20:42Z","title":"Probing the Vulnerability of Large Language Models to Polysemantic\n  Interventions","summary":"  Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes.\n","authors":["Bofan Gong","Shiyang Lai","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2505.11611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11586v1","updated":"2025-05-16T17:59:53Z","published":"2025-05-16T17:59:53Z","title":"The Ripple Effect: On Unforeseen Complications of Backdoor Attacks","summary":"  Recent research highlights concerns about the trustworthiness of third-party\nPre-Trained Language Models (PTLMs) due to potential backdoor attacks. These\nbackdoored PTLMs, however, are effective only for specific pre-defined\ndownstream tasks. In reality, these PTLMs can be adapted to many other\nunrelated downstream tasks. Such adaptation may lead to unforeseen consequences\nin downstream model outputs, consequently raising user suspicion and\ncompromising attack stealthiness. We refer to this phenomenon as backdoor\ncomplications. In this paper, we undertake the first comprehensive\nquantification of backdoor complications. Through extensive experiments using 4\nprominent PTLMs and 16 text classification benchmark datasets, we demonstrate\nthe widespread presence of backdoor complications in downstream models\nfine-tuned from backdoored PTLMs. The output distribution of triggered samples\nsignificantly deviates from that of clean samples. Consequently, we propose a\nbackdoor complication reduction method leveraging multi-task learning to\nmitigate complications without prior knowledge of downstream tasks. The\nexperimental results demonstrate that our proposed method can effectively\nreduce complications while maintaining the efficacy and consistency of backdoor\nattacks. Our code is available at\nhttps://github.com/zhangrui4041/Backdoor_Complications.\n","authors":["Rui Zhang","Yun Shen","Hongwei Li","Wenbo Jiang","Hanxiao Chen","Yuan Zhang","Guowen Xu","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11586v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.11583v1","updated":"2025-05-16T17:21:55Z","published":"2025-05-16T17:21:55Z","title":"Scaling an ISO Compliance Practice: Strategic Insights from Building a\n  \\$1m+ Cybersecurity Certification Line","summary":"  The rapid exponential growth in cloud-first business models and tightened\nglobal data protection regulations have led to the exponential increase in the\nlevel of importance of ISO certifications, especially ISO/IEC 27001, 27017, and\n27018, as strategic imperative propositions for organizations wanting to build\ntrust, ensure compliance, and achieve a competitive advantage. This article\ndescribes a case study of a successful design, implementation, and scaling of a\ncybersecurity certification practice in Armanino LLP, a pioneering US\naccounting and consulting firm. In reaction to increasing client desires for\nformalized information security frameworks, I founded an industry practice from\nconception through implementation to aid mid-market and high-growth technology\nfirms. During one year, the initiative brought in over \\$1 million in new\nservice revenue, expanded our portfolio of cybersecurity clients by 150%, and\nproduced more than 20 successful ISO certifications on various verticals such\nas SaaS, healthcare, and fintech. Based on the strategic wisdom and operational\nstrategy, this paper outlines the technical architecture of the ISO service\nline from modular audit templates to certification readiness kits, from\nstakeholder enablement to integration with SOC 2 and CIS controls. The approach\ngave value to repeatability, speed, and assurance, thus making Armanino a\nreputable certification body. The lessons drawn out provide us with a flexible\ntemplate that can be utilized by firms wishing to build strong compliance\nprograms that can be tailored to address changing digital risk terrains. This\nwork adds to the increasing knowledge about audit scalability, cybersecurity\ncompliance, and ISO standardisation.\n","authors":["Nishant Sonkar"],"pdf_url":"https://arxiv.org/pdf/2505.11583v1.pdf","comment":"9 pages,2 figures"},{"id":"http://arxiv.org/abs/2505.11565v1","updated":"2025-05-16T08:40:09Z","published":"2025-05-16T08:40:09Z","title":"ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?","summary":"  While Large Language Models have shown promise in cybersecurity applications,\ntheir effectiveness in identifying security threats within cloud deployments\nremains unexplored. This paper introduces AWS Cloud Security Engineering Eval,\na novel dataset for evaluating LLMs cloud security threat modeling\ncapabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios,\neach featuring detailed architectural specifications, Infrastructure as Code\nimplementations, documented security vulnerabilities, and associated threat\nmodeling parameters. Our dataset enables systemic assessment of LLMs abilities\nto identify security risks, analyze attack vectors, and propose mitigation\nstrategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that\nGPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro\nperforming optimally in 0-shot scenarios and GPT 4.1 showing superior results\nin few-shot settings. While GPT 4.1 maintains a slight overall performance\nadvantage, Claude 3.7 Sonnet generates the most semantically sophisticated\nthreat models but struggles with threat categorization and generalization. To\npromote reproducibility and advance research in automated cybersecurity threat\nanalysis, we open-source our dataset, evaluation metrics, and methodologies.\n","authors":["Sarthak Munshi","Swapnil Pathak","Sonam Ghatode","Thenuga Priyadarshini","Dhivya Chandramouleeswaran","Ashutosh Rana"],"pdf_url":"https://arxiv.org/pdf/2505.11565v1.pdf","comment":"Submitted to the 39th Annual Conference on Neural Information\n  Processing Systems"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.11485v1","updated":"2025-05-16T17:47:58Z","published":"2025-05-16T17:47:58Z","title":"Modeling cognitive processes of natural reading with transformer-based\n  Language Models","summary":"  Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers.\n","authors":["Bruno Bianchi","Fermín Travi","Juan E. Kamienkowski"],"pdf_url":"https://arxiv.org/pdf/2505.11485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11481v1","updated":"2025-05-16T17:41:44Z","published":"2025-05-16T17:41:44Z","title":"MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and\n  Initiative in Co-creation","summary":"  Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.\n","authors":["Alayt Issak","Jeba Rezwana","Casper Harteveld"],"pdf_url":"https://arxiv.org/pdf/2505.11481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11480v1","updated":"2025-05-16T17:40:45Z","published":"2025-05-16T17:40:45Z","title":"Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning","summary":"  Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.\n","authors":["Anjiang Wei","Tarun Suresh","Huanmi Tan","Yinglun Xu","Gagandeep Singh","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2505.11480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11478v1","updated":"2025-05-16T17:40:01Z","published":"2025-05-16T17:40:01Z","title":"Automatic Reward Shaping from Confounded Offline Data","summary":"  A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist.\n","authors":["Mingxuan Li","Junzhe Zhang","Elias Bareinboim"],"pdf_url":"https://arxiv.org/pdf/2505.11478v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.11475v1","updated":"2025-05-16T17:31:19Z","published":"2025-05-16T17:31:19Z","title":"HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages","summary":"  Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Hoo-Chang Shin","Felipe Soares","Alexander Bukharin","Ellie Evans","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2505.11475v1.pdf","comment":"38 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.00262v3","updated":"2025-05-16T17:26:42Z","published":"2025-02-01T01:43:53Z","title":"INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language\n  Models on Context-Aware Hazard Detection and Edge Case Evaluation","summary":"  Autonomous driving systems face significant challenges in handling\nunpredictable edge-case scenarios, such as adversarial pedestrian movements,\ndangerous vehicle maneuvers, and sudden environmental changes. Current\nend-to-end driving models struggle with generalization to these rare events due\nto limitations in traditional detection and prediction approaches. To address\nthis, we propose INSIGHT (Integration of Semantic and Visual Inputs for\nGeneralized Hazard Tracking), a hierarchical vision-language model (VLM)\nframework designed to enhance hazard detection and edge-case evaluation. By\nusing multimodal data fusion, our approach integrates semantic and visual\nrepresentations, enabling precise interpretation of driving scenarios and\naccurate forecasting of potential dangers. Through supervised fine-tuning of\nVLMs, we optimize spatial hazard localization using attention-based mechanisms\nand coordinate regression techniques. Experimental results on the BDD100K\ndataset demonstrate a substantial improvement in hazard prediction\nstraightforwardness and accuracy over existing models, achieving a notable\nincrease in generalization performance. This advancement enhances the\nrobustness and safety of autonomous driving systems, ensuring improved\nsituational awareness and potential decision-making in complex real-world\nscenarios.\n","authors":["Dianwei Chen","Zifan Zhang","Yuchen Liu","Xianfeng Terry Yang"],"pdf_url":"https://arxiv.org/pdf/2502.00262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07279v2","updated":"2025-05-16T17:18:02Z","published":"2025-02-11T05:48:51Z","title":"Exploratory Diffusion Model for Unsupervised Reinforcement Learning","summary":"  Unsupervised reinforcement learning (URL) aims to pre-train agents by\nexploring diverse states or skills in reward-free environments, facilitating\nefficient adaptation to downstream tasks. As the agent cannot access extrinsic\nrewards during unsupervised exploration, existing methods design intrinsic\nrewards to model the explored data and encourage further exploration. However,\nthe explored data are always heterogeneous, posing the requirements of powerful\nrepresentation abilities for both intrinsic reward models and pre-trained\npolicies. In this work, we propose the Exploratory Diffusion Model (ExDM),\nwhich leverages the strong expressive ability of diffusion models to fit the\nexplored data, simultaneously boosting exploration and providing an efficient\ninitialization for downstream tasks. Specifically, ExDM can accurately estimate\nthe distribution of collected data in the replay buffer with the diffusion\nmodel and introduces the score-based intrinsic reward, encouraging the agent to\nexplore less-visited states. After obtaining the pre-trained policies, ExDM\nenables rapid adaptation to downstream tasks. In detail, we provide theoretical\nanalyses and practical algorithms for fine-tuning diffusion policies,\naddressing key challenges such as training instability and computational\ncomplexity caused by multi-step sampling. Extensive experiments demonstrate\nthat ExDM outperforms existing SOTA baselines in efficient unsupervised\nexploration and fast fine-tuning downstream tasks, especially in structurally\ncomplicated environments.\n","authors":["Chengyang Ying","Huayu Chen","Xinning Zhou","Zhongkai Hao","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.07279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11462v1","updated":"2025-05-16T17:16:27Z","published":"2025-05-16T17:16:27Z","title":"Disentangling Reasoning and Knowledge in Medical Large Language Models","summary":"  Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.\n","authors":["Rahul Thapa","Qingyang Wu","Kevin Wu","Harrison Zhang","Angela Zhang","Eric Wu","Haotian Ye","Suhana Bedi","Nevin Aresh","Joseph Boen","Shriya Reddy","Ben Athiwaratkun","Shuaiwen Leon Song","James Zou"],"pdf_url":"https://arxiv.org/pdf/2505.11462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14406v2","updated":"2025-05-16T17:13:48Z","published":"2025-04-19T21:11:40Z","title":"ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and\n  Information Sensemaking","summary":"  Synthesizing knowledge from large document collections is a critical yet\nincreasingly complex aspect of qualitative research and knowledge work. While\nAI offers automation potential, effectively integrating it into human-centric\nsensemaking workflows remains challenging. We present ScholarMate, an\ninteractive system designed to augment qualitative analysis by unifying AI\nassistance with human oversight. ScholarMate enables researchers to dynamically\narrange and interact with text snippets on a non-linear canvas, leveraging AI\nfor theme suggestions, multi-level summarization, and evidence-based theme\nnaming, while ensuring transparency through traceability to source documents.\nInitial pilot studies indicated that users value this mixed-initiative\napproach, finding the balance between AI suggestions and direct manipulation\ncrucial for maintaining interpretability and trust. We further demonstrate the\nsystem's capability through a case study analyzing 24 papers. By balancing\nautomation with human control, ScholarMate enhances efficiency and supports\ninterpretability, offering a valuable approach for productive human-AI\ncollaboration in demanding sensemaking tasks common in knowledge work.\n","authors":["Runlong Ye","Patrick Yung Kang Lee","Matthew Varona","Oliver Huang","Carolina Nobre"],"pdf_url":"https://arxiv.org/pdf/2504.14406v2.pdf","comment":"accepted at CHIWORK '25"},{"id":"http://arxiv.org/abs/2505.11454v1","updated":"2025-05-16T17:09:44Z","published":"2025-05-16T17:09:44Z","title":"HumaniBench: A Human-Centric Framework for Large Multimodal Models\n  Evaluation","summary":"  Large multimodal models (LMMs) now excel on many vision language benchmarks,\nhowever, they still struggle with human centered criteria such as fairness,\nethics, empathy, and inclusivity, key to aligning with human values. We\nintroduce HumaniBench, a holistic benchmark of 32K real-world image question\npairs, annotated via a scalable GPT4o assisted pipeline and exhaustively\nverified by domain experts. HumaniBench evaluates seven Human Centered AI\n(HCAI) principles: fairness, ethics, understanding, reasoning, language\ninclusivity, empathy, and robustness, across seven diverse tasks, including\nopen and closed ended visual question answering (VQA), multilingual QA, visual\ngrounding, empathetic captioning, and robustness tests. Benchmarking 15 state\nof the art LMMs (open and closed source) reveals that proprietary models\ngenerally lead, though robustness and visual grounding remain weak points. Some\nopen-source models also struggle to balance accuracy with adherence to\nhuman-aligned principles. HumaniBench is the first benchmark purpose built\naround HCAI principles. It provides a rigorous testbed for diagnosing alignment\ngaps and guiding LMMs toward behavior that is both accurate and socially\nresponsible. Dataset, annotation prompts, and evaluation code are available at:\nhttps://vectorinstitute.github.io/HumaniBench\n","authors":["Shaina Raza","Aravind Narayanan","Vahid Reza Khazaie","Ashmal Vayani","Mukund S. Chettiar","Amandeep Singh","Mubarak Shah","Deval Pandya"],"pdf_url":"https://arxiv.org/pdf/2505.11454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11451v1","updated":"2025-05-16T17:07:14Z","published":"2025-05-16T17:07:14Z","title":"Extracting Explainable Dates From Medical Images By Reverse-Engineering\n  UNIX Timestamps","summary":"  Dates often contribute towards highly impactful medical decisions, but it is\nrarely clear how to extract this data. AI has only just begun to be used\ntranscribe such documents, and common methods are either to trust that the\noutput produced by a complex AI model, or to parse the text using regular\nexpressions. Recent work has established that regular expressions are an\nexplainable form of logic, but it is difficult to decompose these into the\ncomponent parts that are required to construct precise UNIX timestamps. First,\nwe test publicly-available regular expressions, and we found that these were\nunable to capture a significant number of our dates. Next, we manually created\neasily-decomposable regular expressions, and we found that these were able to\ndetect the majority of real dates, but also a lot of sequences of text that\nlook like dates. Finally, we used regular expression synthesis to automatically\nidentify regular expressions from the reverse-engineered UNIX timestamps that\nwe created. We find that regular expressions created by regular expression\nsynthesis detect far fewer sequences of text that look like dates than those\nthat were manually created, at the cost of a slight increase to the number of\nmissed dates. Overall, our results show that regular expressions can be created\nthrough regular expression synthesis to identify complex dates and date ranges\nin text transcriptions. To our knowledge, our proposed way of learning\ndeterministic logic by reverse-engineering several many-one mappings and\nfeeding these into a regular expression synthesiser is a new approach.\n","authors":["Lee Harris","James Bentham","Philippe De Wilde"],"pdf_url":"https://arxiv.org/pdf/2505.11451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11449v1","updated":"2025-05-16T17:05:25Z","published":"2025-05-16T17:05:25Z","title":"LLMs unlock new paths to monetizing exploits","summary":"  We argue that Large language models (LLMs) will soon alter the economics of\ncyberattacks. Instead of attacking the most commonly used software and\nmonetizing exploits by targeting the lowest common denominator among victims,\nLLMs enable adversaries to launch tailored attacks on a user-by-user basis. On\nthe exploitation front, instead of human attackers manually searching for one\ndifficult-to-identify bug in a product with millions of users, LLMs can find\nthousands of easy-to-identify bugs in products with thousands of users. And on\nthe monetization front, instead of generic ransomware that always performs the\nsame attack (encrypt all your data and request payment to decrypt), an\nLLM-driven ransomware attack could tailor the ransom demand based on the\nparticular content of each exploited device.\n  We show that these two attacks (and several others) are imminently practical\nusing state-of-the-art LLMs. For example, we show that without any human\nintervention, an LLM finds highly sensitive personal information in the Enron\nemail dataset (e.g., an executive having an affair with another employee) that\ncould be used for blackmail. While some of our attacks are still too expensive\nto scale widely today, the incentives to implement these attacks will only\nincrease as LLMs get cheaper. Thus, we argue that LLMs create a need for new\ndefense-in-depth approaches.\n","authors":["Nicholas Carlini","Milad Nasr","Edoardo Debenedetti","Barry Wang","Christopher A. Choquette-Choo","Daphne Ippolito","Florian Tramèr","Matthew Jagielski"],"pdf_url":"https://arxiv.org/pdf/2505.11449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14889v4","updated":"2025-05-16T17:00:45Z","published":"2024-02-22T10:46:11Z","title":"COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for\n  Language Models","summary":"  Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias, based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augmented 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65, p = 3.4 * 10^{-60}$)\nand can be used to create reliable benchmarks, which would assist bias\nmitigation works.\n","authors":["Priyanshul Govil","Hemang Jain","Vamshi Krishna Bonagiri","Aman Chadha","Ponnurangam Kumaraguru","Manas Gaur","Sanorita Dey"],"pdf_url":"https://arxiv.org/pdf/2402.14889v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11439v1","updated":"2025-05-16T16:58:03Z","published":"2025-05-16T16:58:03Z","title":"SurgPose: Generalisable Surgical Instrument Pose Estimation using\n  Zero-Shot Learning and Stereo Vision","summary":"  Accurate pose estimation of surgical tools in Robot-assisted Minimally\nInvasive Surgery (RMIS) is essential for surgical navigation and robot control.\nWhile traditional marker-based methods offer accuracy, they face challenges\nwith occlusions, reflections, and tool-specific designs. Similarly, supervised\nlearning methods require extensive training on annotated datasets, limiting\ntheir adaptability to new tools. Despite their success in other domains,\nzero-shot pose estimation models remain unexplored in RMIS for pose estimation\nof surgical instruments, creating a gap in generalising to unseen surgical\ntools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation\npipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D\nmodels like the FoundationPose and SAM-6D. We advanced these models by\nincorporating vision-based depth estimation using the RAFT-Stereo method, for\nrobust depth estimation in reflective and textureless environments.\nAdditionally, we enhanced SAM-6D by replacing its instance segmentation module,\nSegment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly\nboosting segmentation accuracy in occluded and complex conditions. Extensive\nvalidation reveals that our enhanced SAM-6D surpasses FoundationPose in\nzero-shot pose estimation of unseen surgical instruments, setting a new\nbenchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the\ngeneralisability of pose estimation for unseen objects and pioneers the\napplication of RGB-D zero-shot methods in RMIS.\n","authors":["Utsav Rai","Haozheng Xu","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2505.11439v1.pdf","comment":"To be published in 2025 International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2505.11436v1","updated":"2025-05-16T16:56:40Z","published":"2025-05-16T16:56:40Z","title":"GODBench: A Benchmark for Multimodal Large Language Models in Video\n  Comment Art","summary":"  Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.\n","authors":["Chenkai Zhang","Yiming Lei","Zeming Liu","Haitao Leng","Shaoguo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11436v1.pdf","comment":"69 pages, 66 figures, accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2505.11427v1","updated":"2025-05-16T16:43:23Z","published":"2025-05-16T16:43:23Z","title":"Mergenetic: a Simple Evolutionary Model Merging Library","summary":"  Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.\n","authors":["Adrian Robert Minut","Tommaso Mencattini","Andrea Santilli","Donato Crisostomi","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2505.11427v1.pdf","comment":"Link: https://github.com/tommasomncttn/mergenetic"},{"id":"http://arxiv.org/abs/2505.11424v1","updated":"2025-05-16T16:38:01Z","published":"2025-05-16T16:38:01Z","title":"Improving Object Detection Performance through YOLOv8: A Comprehensive\n  Training and Evaluation Study","summary":"  This study evaluated the performance of a YOLOv8-based segmentation model for\ndetecting and segmenting wrinkles in facial images.\n","authors":["Rana Poureskandar","Shiva Razzagzadeh"],"pdf_url":"https://arxiv.org/pdf/2505.11424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09619v2","updated":"2025-05-16T16:36:29Z","published":"2025-04-07T14:07:05Z","title":"Predictive Models for Chronic Heart Failure","summary":"  The management of chronic Heart Failure (HF) presents significant challenges\nin modern healthcare, requiring continuous monitoring, early detection of\nexacerbations, and personalized treatment strategies. In this paper, we present\na predictive model founded on Machine Learning (ML) techniques to identify\npatients at HF risk. This model is an ensemble learning approach, a modified\nstacking technique, that uses two specialized models leveraging clinical and\nechocardiographic features and then a meta-model to combine the predictions of\nthese two models. We initially assess the model on a real dataset and the\nobtained results suggest that it performs well in the stratification of\npatients at HR risk. Specifically, we obtained high sensitivity (95\\%),\nensuring that nearly all high-risk patients are identified. As for accuracy, we\nobtained 84\\%, which can be considered moderate in some ML contexts. However,\nit is acceptable given our priority of identifying patients at risk of HF\nbecause they will be asked to participate in the telemonitoring program of the\nPrediHealth research project on which some of the authors of this paper are\nworking. The initial findings also suggest that ML-based risk stratification\nmodels can serve as valuable decision-support tools not only in the PrediHealth\nproject but also for healthcare professionals, aiding in early intervention and\npersonalized patient management. To have a better understanding of the value\nand of potentiality of our predictive model, we also contrasted its results\nwith those obtained by using three baseline models. The preliminary results\nindicate that our predictive model outperforms these baselines that flatly\nconsider features, \\ie not grouping them in clinical and echocardiographic\nfeatures.\n","authors":["Pietro Cassieri","Aiman Faiz","Anna Maria De Roberto","Claudio Pascarelli","Gianvito Mitrano","Gianluca Fimiani","Marina Garofano","Christiancarmine Esposito","Genoveffa Tortora","Mariangela Lazoi","Claudio Passino","Alessia Bramanti","Giuseppe Scanniello"],"pdf_url":"https://arxiv.org/pdf/2505.09619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11417v1","updated":"2025-05-16T16:29:21Z","published":"2025-05-16T16:29:21Z","title":"EdgeWisePersona: A Dataset for On-Device User Profiling from Natural\n  Language Interactions","summary":"  This paper introduces a novel dataset and evaluation benchmark designed to\nassess and improve small language models deployable on edge devices, with a\nfocus on user profiling from multi-session natural language interactions in\nsmart home environments. At the core of the dataset are structured user\nprofiles, each defined by a set of routines - context-triggered, repeatable\npatterns of behavior that govern how users interact with their home systems.\nUsing these profiles as input, a large language model (LLM) generates\ncorresponding interaction sessions that simulate realistic, diverse, and\ncontext-aware dialogues between users and their devices.\n  The primary task supported by this dataset is profile reconstruction:\ninferring user routines and preferences solely from interactions history. To\nassess how well current models can perform this task under realistic\nconditions, we benchmarked several state-of-the-art compact language models and\ncompared their performance against large foundation models. Our results show\nthat while small models demonstrate some capability in reconstructing profiles,\nthey still fall significantly short of large models in accurately capturing\nuser behavior. This performance gap poses a major challenge - particularly\nbecause on-device processing offers critical advantages, such as preserving\nuser privacy, minimizing latency, and enabling personalized experiences without\nreliance on the cloud. By providing a realistic, structured testbed for\ndeveloping and evaluating behavioral modeling under these constraints, our\ndataset represents a key step toward enabling intelligent, privacy-respecting\nAI systems that learn and adapt directly on user-owned devices.\n","authors":["Patryk Bartkowiak","Michal Podstawski"],"pdf_url":"https://arxiv.org/pdf/2505.11417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11416v1","updated":"2025-05-16T16:29:19Z","published":"2025-05-16T16:29:19Z","title":"MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron\n  Selection","summary":"  Modern neural networks often activate all neurons for every input, leading to\nunnecessary computation and inefficiency. We introduce Matrix-Interpolated\nDropout Layer (MID-L), a novel module that dynamically selects and activates\nonly the most informative neurons by interpolating between two transformation\npaths via a learned, input-dependent gating vector. Unlike conventional dropout\nor static sparsity methods, MID-L employs a differentiable Top-k masking\nstrategy, enabling per-input adaptive computation while maintaining end-to-end\ndifferentiability. MID-L is model-agnostic and integrates seamlessly into\nexisting architectures. Extensive experiments on six benchmarks, including\nMNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves\nup to average 55\\% reduction in active neurons, 1.7$\\times$ FLOPs savings, and\nmaintains or exceeds baseline accuracy. We further validate the informativeness\nand selectivity of the learned neurons via Sliced Mutual Information (SMI) and\nobserve improved robustness under overfitting and noisy data conditions.\nAdditionally, MID-L demonstrates favorable inference latency and memory usage\nprofiles, making it suitable for both research exploration and deployment on\ncompute-constrained systems. These results position MID-L as a general-purpose,\nplug-and-play dynamic computation layer, bridging the gap between dropout\nregularization and efficient inference.\n","authors":["Pouya Shaeri","Ariane Middel"],"pdf_url":"https://arxiv.org/pdf/2505.11416v1.pdf","comment":"Submitted in a Computer Science Conference, currently in Review"},{"id":"http://arxiv.org/abs/2502.08574v2","updated":"2025-05-16T16:27:25Z","published":"2025-02-12T17:09:13Z","title":"TANTE: Time-Adaptive Operator Learning via Neural Taylor Expansion","summary":"  Operator learning for time-dependent partial differential equations (PDEs)\nhas seen rapid progress in recent years, enabling efficient approximation of\ncomplex spatiotemporal dynamics. However, most existing methods rely on fixed\ntime step sizes during rollout, which limits their ability to adapt to varying\ntemporal complexity and often leads to error accumulation. To address this gap,\nwe propose the Time-Adaptive Transformer with Neural Taylor Expansion (TANTE),\na novel operator-learning framework that produces continuous-time predictions\nwith adaptive step sizes. TANTE predicts future states by performing a Taylor\nexpansion at the current state, where neural networks learn both the\nhigher-order temporal derivatives and the local radius of convergence. This\nallows the model to dynamically adjust its rollout based on the local behavior\nof the solution, thereby reducing cumulative error and improving computational\nefficiency. We demonstrate the effectiveness of TANTE across a wide range of\nPDE benchmarks, achieving superior accuracy and adaptability compared to\nfixed-step baselines, delivering accuracy gains of 10-50 % and speed-ups of\n30-80 % at inference.\n","authors":["Zhikai Wu","Sifan Wang","Shiyang Zhang","Sizhuang He","Min Zhu","Anran Jiao","Lu Lu","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.08574v2.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.07450v3","updated":"2025-05-16T16:21:05Z","published":"2025-05-12T11:25:54Z","title":"Prototype Augmented Hypernetworks for Continual Learning","summary":"  Continual learning (CL) aims to learn a sequence of tasks without forgetting\nprior knowledge, but gradient updates for a new task often overwrite the\nweights learned earlier, causing catastrophic forgetting (CF). We propose\nPrototype-Augmented Hypernetworks (PAH), a framework where a single\nhypernetwork, conditioned on learnable task prototypes, dynamically generates\ntask-specific classifier heads on demand. To mitigate forgetting, PAH combines\ncross-entropy with dual distillation losses, one to align logits and another to\nalign prototypes, ensuring stable feature representations across tasks.\nEvaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves\nstate-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7\n% and 4.4 % forgetting, respectively, surpassing prior methods without storing\nsamples or heads.\n","authors":["Neil De La Fuente","Maria Pilligua","Daniel Vidal","Albin Soutiff","Cecilia Curreli","Daniel Cremers","Andrey Barsky"],"pdf_url":"https://arxiv.org/pdf/2505.07450v3.pdf","comment":"CVPR 2025 (LatinX in CV)"},{"id":"http://arxiv.org/abs/2505.11409v1","updated":"2025-05-16T16:17:22Z","published":"2025-05-16T16:17:22Z","title":"Visual Planning: Let's Think Only with Images","summary":"  Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.\n","authors":["Yi Xu","Chengzu Li","Han Zhou","Xingchen Wan","Caiqi Zhang","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2505.11409v1.pdf","comment":"10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)"},{"id":"http://arxiv.org/abs/2505.11406v1","updated":"2025-05-16T16:16:32Z","published":"2025-05-16T16:16:32Z","title":"Large Language Model Use Impact Locus of Control","summary":"  As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity.\n","authors":["Jenny Xiyu Fu","Brennan Antone","Kowe Kadoma","Malte Jung"],"pdf_url":"https://arxiv.org/pdf/2505.11406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11404v1","updated":"2025-05-16T16:12:50Z","published":"2025-05-16T16:12:50Z","title":"Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert\n  Reasoner","summary":"  Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose PathoCLIP, trained on the same figure-caption corpus used for continued\npretraining. Comprehensive experimental results demonstrate that both PathoCLIP\nand Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1.\n","authors":["Wenchuan Zhang","Penghao Zhang","Jingru Guo","Tao Cheng","Jie Chen","Shuwan Zhang","Zhang Zhang","Yuhao Yi","Hong Bu"],"pdf_url":"https://arxiv.org/pdf/2505.11404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08893v4","updated":"2025-05-16T15:49:48Z","published":"2024-10-11T15:10:40Z","title":"Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and\n  Parameter Efficient","summary":"  Model-based reinforcement learning (RL) offers a solution to the data\ninefficiency that plagues most model-free RL algorithms. However, learning a\nrobust world model often requires complex and deep architectures, which are\ncomputationally expensive and challenging to train. Within the world model,\nsequence models play a critical role in accurate predictions, and various\narchitectures have been explored, each with its own challenges. Currently,\nrecurrent neural network (RNN)-based world models struggle with vanishing\ngradients and capturing long-term dependencies. Transformers, on the other\nhand, suffer from the quadratic memory and computational complexity of\nself-attention mechanisms, scaling as $O(n^2)$, where $n$ is the sequence\nlength.\n  To address these challenges, we propose a state space model (SSM)-based world\nmodel, Drama, specifically leveraging Mamba, that achieves $O(n)$ memory and\ncomputational complexity while effectively capturing long-term dependencies and\nenabling efficient training with longer sequences. We also introduce a novel\nsampling method to mitigate the suboptimality caused by an incorrect world\nmodel in the early training stages. Combining these techniques, Drama achieves\na normalised score on the Atari100k benchmark that is competitive with other\nstate-of-the-art (SOTA) model-based RL algorithms, using only a 7\nmillion-parameter world model. Drama is accessible and trainable on\noff-the-shelf hardware, such as a standard laptop. Our code is available at\nhttps://github.com/realwenlongwang/Drama.git.\n","authors":["Wenlong Wang","Ivana Dusparic","Yucheng Shi","Ke Zhang","Vinny Cahill"],"pdf_url":"https://arxiv.org/pdf/2410.08893v4.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2411.08135v2","updated":"2025-05-16T15:45:14Z","published":"2024-11-12T19:26:43Z","title":"On the Role of Speech Data in Reducing Toxicity Detection Bias","summary":"  Text toxicity detection systems exhibit significant biases, producing\ndisproportionate rates of false positives on samples mentioning demographic\ngroups. But what about toxicity detection in speech? To investigate the extent\nto which text-based biases are mitigated by speech-based systems, we produce a\nset of high-quality group annotations for the multilingual MuTox dataset, and\nthen leverage these annotations to systematically compare speech- and\ntext-based toxicity classifiers. Our findings indicate that access to speech\ndata during inference supports reduced bias against group mentions,\nparticularly for ambiguous and disagreement-inducing samples. Our results also\nsuggest that improving classifiers, rather than transcription pipelines, is\nmore helpful for reducing group bias. We publicly release our annotations and\nprovide recommendations for future toxicity dataset construction.\n","authors":["Samuel J. Bell","Mariano Coria Meglioli","Megan Richards","Eduardo Sánchez","Christophe Ropers","Skyler Wang","Adina Williams","Levent Sagun","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2411.08135v2.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2501.02406v4","updated":"2025-05-16T15:45:11Z","published":"2025-01-04T23:51:43Z","title":"Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities","summary":"  Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly challenging as text generated by\nLarge Language Models (LLMs) becomes almost indistinguishable from\nhuman-generated content. In addition, many institutions utilize in-house LLMs\nand want to ensure that external, non-sanctioned LLMs do not produce content\nwithin the institution. In this paper, we answer the following question: Given\na piece of text, can we identify whether it was produced by a particular LLM or\nnot? We model LLM-generated text as a sequential stochastic process with\ncomplete dependence on history. We then design zero-shot statistical tests to\n(i) distinguish between text generated by two different known sets of LLMs $A$\n(non-sanctioned) and $B$ (in-house), and (ii) identify whether text was\ngenerated by a known LLM or generated by any unknown model, e.g., a human or\nsome other language generation process. We prove that the type I and type II\nerrors of our test decrease exponentially with the length of the text. For\nthat, we show that if $B$ generates the text, then except with an exponentially\nsmall probability in string length, the log-perplexity of the string under $A$\nconverges to the average cross-entropy of $B$ and $A$. We then present\nexperiments using LLMs with white-box access to support our theoretical results\nand empirically examine the robustness of our results to black-box settings and\nadversarial attacks. In the black-box setting, our method achieves an average\nTPR of 82.5\\% at a fixed FPR of 5\\%. Under adversarial perturbations, our\nminimum TPR is 48.6\\% at the same FPR threshold. Both results outperform all\nnon-commercial baselines. See\nhttps://github.com/TaraRadvand74/llm-text-detection for code, data, and an\nonline demo of the project.\n","authors":["Tara Radvand","Mojtaba Abdolmaleki","Mohamed Mostagir","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2501.02406v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13837v2","updated":"2025-05-16T15:39:33Z","published":"2025-04-18T17:59:56Z","title":"Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential.\n","authors":["Yang Yue","Zhiqi Chen","Rui Lu","Andrew Zhao","Zhaokai Wang","Yang Yue","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2504.13837v2.pdf","comment":"30 pages, 27 figures"},{"id":"http://arxiv.org/abs/2505.10074v2","updated":"2025-05-16T15:33:49Z","published":"2025-05-15T08:24:47Z","title":"Leveraging Graph Retrieval-Augmented Generation to Support Learners'\n  Understanding of Knowledge Concepts in MOOCs","summary":"  Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.\n","authors":["Mohamed Abdelmagied","Mohamed Amine Chatti","Shoeb Joarder","Qurat Ul Ain","Rawaa Alatrash"],"pdf_url":"https://arxiv.org/pdf/2505.10074v2.pdf","comment":"Accepted at EMOOCs 2025"},{"id":"http://arxiv.org/abs/2505.09716v2","updated":"2025-05-16T15:28:45Z","published":"2025-05-14T18:21:21Z","title":"Out-of-distribution generalisation is hard: evidence from ARC-like tasks","summary":"  Out-of-distribution (OOD) generalisation is considered a hallmark of human\nand animal intelligence. To achieve OOD through composition, a system must\ndiscover the environment-invariant properties of experienced input-output\nmappings and transfer them to novel inputs. This can be realised if an\nintelligent system can identify appropriate, task-invariant, and composable\ninput features, as well as the composition methods, thus allowing it to act\nbased not on the interpolation between learnt data points but on the\ntask-invariant composition of those features. We propose that in order to\nconfirm that an algorithm does indeed learn compositional structures from data,\nit is not enough to just test on an OOD setup, but one also needs to confirm\nthat the features identified are indeed compositional. We showcase this by\nexploring two tasks with clearly defined OOD metrics that are not OOD solvable\nby three commonly used neural networks: a Multi-Layer Perceptron (MLP), a\nConvolutional Neural Network (CNN), and a Transformer. In addition, we develop\ntwo novel network architectures imbued with biases that allow them to be\nsuccessful in OOD scenarios. We show that even with correct biases and almost\nperfect OOD performance, an algorithm can still fail to learn the correct\nfeatures for compositional generalisation.\n","authors":["George Dimitriadis","Spyridon Samothrakis"],"pdf_url":"https://arxiv.org/pdf/2505.09716v2.pdf","comment":"Submission to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2310.18304v5","updated":"2025-05-16T15:26:15Z","published":"2023-10-27T17:53:53Z","title":"A Stability Principle for Learning under Non-Stationarity","summary":"  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory showcases the adaptivity of this approach to\nunknown non-stationarity. We prove regret bounds that are minimax optimal up to\nlogarithmic factors when the population losses are strongly convex, or\nLipschitz only. At the heart of our analysis lie two novel components: a\nmeasure of similarity between functions and a segmentation technique for\ndividing the non-stationary data sequence into quasi-stationary pieces. We\nevaluate the practical performance of our approach through real-data\nexperiments on electricity demand prediction and hospital nurse staffing.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18304v5.pdf","comment":"65 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.14162v2","updated":"2025-05-16T15:25:03Z","published":"2025-03-18T11:33:29Z","title":"EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large\n  Language Models","summary":"  Industrial Anomaly Detection (IAD) is critical to ensure product quality\nduring manufacturing. Although existing zero-shot defect segmentation and\ndetection methods have shown effectiveness, they cannot provide detailed\ndescriptions of the defects. Furthermore, the application of large multi-modal\nmodels in IAD remains in its infancy, facing challenges in balancing\nquestion-answering (QA) performance and mask-based grounding capabilities,\noften owing to overfitting during the fine-tuning process. To address these\nchallenges, we propose a novel approach that introduces a dedicated multi-modal\ndefect localization module to decouple the dialog functionality from the core\nfeature extraction. This decoupling is achieved through independent\noptimization objectives and tailored learning strategies. Additionally, we\ncontribute to the first multi-modal industrial anomaly detection training\ndataset, named Defect Detection Question Answering (DDQA), encompassing a wide\nrange of defect types and industrial scenarios. Unlike conventional datasets\nthat rely on GPT-generated data, DDQA ensures authenticity and reliability and\noffers a robust foundation for model training. Experimental results demonstrate\nthat our proposed method, Explainable Industrial Anomaly Detection Assistant\n(EIAD), achieves outstanding performance in defect detection and localization\ntasks. It not only significantly enhances accuracy but also improves\ninterpretability. These advancements highlight the potential of EIAD for\npractical applications in industrial settings.\n","authors":["Zongyun Zhang","Jiacheng Ruan","Xian Gao","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2503.14162v2.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2505.03819v2","updated":"2025-05-16T15:21:29Z","published":"2025-05-02T21:06:53Z","title":"Focus on the Likely: Test-time Instance-based Uncertainty Removal","summary":"  We ask: Does focusing on classes predicted as likely improve model\npredictions? We aim for an affirmative answer by proposing two novel test-time\nfine-tuning methods to improve uncertain model predictions. Instead of greedily\nselecting the most likely class, we introduce an additional step, \\emph{focus\non the likely classes}, to refine predictions. By applying a theoretically\nmotivated single gradient descent step with a large learning rate, we refine\npredictions when an initial forward pass indicates high uncertainty. This\naligns predictions more closely with the ideal of assigning zero probability to\nless plausible outcomes. The experimental evaluation demonstrates accuracy\ngains for one of our methods, which emphasizes shared features among likely\nclasses, across diverse text and image domain models. %Our theoretical\ndiscussion provides a deeper understanding, highlighting the varying impact of\nshared and non-shared features among (focus) classes. %Our discussion also\nsuggests an interesting view on standard, offline training vs. test-time\ntraining: Opposing optimization rationales regarding breadth of feature\ndependence are preferable during each training phase.\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2505.03819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09447v2","updated":"2025-05-16T15:20:57Z","published":"2024-01-31T23:13:38Z","title":"Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and\n  Natural Grasp Types","summary":"  This research aims to decode hand grasps from Electroencephalograms (EEGs)\nfor dexterous neuroprosthetic development and Brain-Computer Interface (BCI)\napplications, especially for patients with motor disorders. Particularly, it\nfocuses on distinguishing two complex natural power and precision grasps in\naddition to a neutral condition as a no-movement condition using a new\nEEG-based BCI platform and wavelet signal processing. Wavelet analysis involved\ngenerating time-frequency and topographic maps from wavelet power coefficients.\nThen, by using machine learning techniques with novel wavelet features, we\nachieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement\nvs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs\nPrecision, demonstrating the effectiveness of these features in EEG-based grasp\ndifferentiation. In contrast to previous studies, a critical part of our study\nwas permutation feature importance analysis, which highlighted key features for\ngrasp classification. It revealed that the most crucial brain activities during\ngrasping occur in the motor cortex, within the alpha and beta frequency bands.\nThese insights demonstrate the potential of wavelet features in real-time\nneuroprosthetic technology and BCI applications.\n","authors":["Ali Rabiee","Sima Ghafoori","Anna Cetera","Reza Abiri"],"pdf_url":"https://arxiv.org/pdf/2402.09447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20445v2","updated":"2025-05-16T15:19:42Z","published":"2025-04-29T05:36:32Z","title":"Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking\n  Neural Networks","summary":"  Spiking Neural Networks (SNNs) have emerged as a promising approach for\nenergy-efficient and biologically plausible computation. However, due to\nlimitations in existing training methods and inherent model constraints, SNNs\noften exhibit a performance gap when compared to Artificial Neural Networks\n(ANNs). Knowledge distillation (KD) has been explored as a technique to\ntransfer knowledge from ANN teacher models to SNN student models to mitigate\nthis gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence\nto align output distributions. However, conventional KL-based approaches fail\nto fully exploit the unique characteristics of SNNs, as they tend to\noveremphasize high-probability predictions while neglecting low-probability\nones, leading to suboptimal generalization. To address this, we propose\nHead-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for\nSNNs. HTA-KL introduces a cumulative probability-based mask to dynamically\ndistinguish between high- and low-probability regions. It assigns adaptive\nweights to ensure balanced knowledge transfer, enhancing the overall\nperformance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,\nour method effectively align both head and tail regions of the distribution. We\nevaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our\nmethod outperforms existing methods on most datasets with fewer timesteps.\n","authors":["Tianqing Zhang","Zixin Zhu","Kairong Yu","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.20445v2.pdf","comment":"Accepted by IJCNN2025"},{"id":"http://arxiv.org/abs/2502.17773v2","updated":"2025-05-16T15:19:23Z","published":"2025-02-25T02:07:29Z","title":"Uncertainty Quantification for LLM-Based Survey Simulations","summary":"  We investigate the use of large language models (LLMs) to simulate human\nresponses to survey questions, and perform uncertainty quantification to gain\nreliable insights. Our approach converts imperfect LLM-simulated responses into\nconfidence sets for population parameters of human responses, addressing the\ndistribution shift between the simulated and real populations. A key innovation\nlies in determining the optimal number of simulated responses: too many produce\noverly narrow confidence sets with poor coverage, while too few yield\nexcessively loose estimates. To resolve this, our method adaptively selects the\nsimulation sample size, ensuring valid average-case coverage guarantees. It is\nbroadly applicable to any LLM, irrespective of its fidelity, and any procedure\nfor constructing confidence sets. Additionally, the selected sample size\nquantifies the degree of misalignment between the LLM and the target human\npopulation. We illustrate our method on real datasets and LLMs.\n","authors":["Chengpiao Huang","Yuhang Wu","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17773v2.pdf","comment":"33 pages, 7 figures, 10 tables"},{"id":"http://arxiv.org/abs/2410.13002v2","updated":"2025-05-16T15:13:26Z","published":"2024-10-16T19:59:31Z","title":"Flex: End-to-End Text-Instructed Visual Navigation from Foundation Model\n  Features","summary":"  End-to-end learning directly maps sensory inputs to actions, creating highly\nintegrated and efficient policies for complex robotics tasks. However, such\nmodels often struggle to generalize beyond their training scenarios, limiting\nadaptability to new environments, tasks, and concepts. In this work, we\ninvestigate the minimal data requirements and architectural adaptations\nnecessary to achieve robust closed-loop performance with vision-based control\npolicies under unseen text instructions and visual distribution shifts. Our\nfindings are synthesized in Flex (Fly lexically), a framework that uses\npre-trained Vision Language Models (VLMs) as frozen patch-wise feature\nextractors, generating spatially aware embeddings that integrate semantic and\nvisual information. We demonstrate the effectiveness of this approach on a\nquadrotor fly-to-target task, where agents trained via behavior cloning on a\nsmall simulated dataset successfully generalize to real-world scenes with\ndiverse novel goals and command formulations.\n","authors":["Makram Chahine","Alex Quach","Alaa Maalouf","Tsun-Hsuan Wang","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.13002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11340v1","updated":"2025-05-16T15:07:43Z","published":"2025-05-16T15:07:43Z","title":"DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in\n  Real-World Scenarios","summary":"  Decompilers are fundamental tools for critical security tasks, from\nvulnerability discovery to malware analysis, yet their evaluation remains\nfragmented. Existing approaches primarily focus on syntactic correctness\nthrough synthetic micro-benchmarks or subjective human ratings, failing to\naddress real-world requirements for semantic fidelity and analyst usability. We\npresent DecompileBench, the first comprehensive framework that enables\neffective evaluation of decompilers in reverse engineering workflows through\nthree key components: \\textit{real-world function extraction} (comprising\n23,400 functions from 130 real-world programs), \\textit{runtime-aware\nvalidation}, and \\textit{automated human-centric assessment} using LLM-as-Judge\nto quantify the effectiveness of decompilers in reverse engineering workflows.\nThrough a systematic comparison between six industrial-strength decompilers and\nsix recent LLM-powered approaches, we demonstrate that LLM-based methods\nsurpass commercial tools in code understandability despite 52.2% lower\nfunctionality correctness. These findings highlight the potential of LLM-based\napproaches to transform human-centric reverse engineering. We open source\n\\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a\nframework to advance research on decompilers and assist security experts in\nmaking informed tool selections based on their specific requirements.\n","authors":["Zeyu Gao","Yuxin Cui","Hao Wang","Siliang Qin","Yuanda Wang","Bolun Zhang","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18002v2","updated":"2025-05-16T15:04:58Z","published":"2025-02-25T09:08:50Z","title":"A Radon-Nikodým Perspective on Anomaly Detection: Theory and\n  Implications","summary":"  Which principle underpins the design of an effective anomaly detection loss\nfunction? The answer lies in the concept of Radon-Nikod\\'ym theorem, a\nfundamental concept in measure theory. The key insight from this article is:\nMultiplying the vanilla loss function with the Radon-Nikod\\'ym derivative\nimproves the performance across the board. We refer to this as RN-Loss. We\nprove this using the setting of PAC (Probably Approximately Correct)\nlearnability.\n  Depending on the context a Radon-Nikod\\'ym derivative takes different forms.\nIn the simplest case of supervised anomaly detection, Radon-Nikod\\'ym\nderivative takes the form of a simple weighted loss. In the case of\nunsupervised anomaly detection (with distributional assumptions),\nRadon-Nikod\\'ym derivative takes the form of the popular cluster based local\noutlier factor. We evaluate our algorithm on 96 datasets, including univariate\nand multivariate data from diverse domains, including healthcare,\ncybersecurity, and finance. We show that RN-Derivative algorithms outperform\nstate-of-the-art methods on 68% of Multivariate datasets (based on F1 scores)\nand also achieves peak F1-scores on 72% of time series (Univariate) datasets.\n","authors":["Shlok Mehendale","Aditya Challa","Rahul Yedida","Sravan Danda","Santonu Sarkar","Snehanshu Saha"],"pdf_url":"https://arxiv.org/pdf/2502.18002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04522v4","updated":"2025-05-16T14:56:53Z","published":"2025-02-06T21:45:38Z","title":"ImprovNet -- Generating Controllable Musical Improvisations with\n  Iterative Corruption Refinement","summary":"  Despite deep learning's remarkable advances in style transfer across various\ndomains, generating controllable performance-level musical style transfer for\ncomplete symbolically represented musical works remains a challenging area of\nresearch. Much of this is owed to limited datasets, especially for genres such\nas jazz, and the lack of unified models that can handle multiple music\ngeneration tasks. This paper presents ImprovNet, a transformer-based\narchitecture that generates expressive and controllable musical improvisations\nthrough a self-supervised corruption-refinement training strategy. The\nimprovisational style transfer is aimed at making meaningful modifications to\none or more musical elements - melody, harmony or rhythm of the original\ncomposition with respect to the target genre. ImprovNet unifies multiple\ncapabilities within a single model: it can perform cross-genre and intra-genre\nimprovisations, harmonize melodies with genre-specific styles, and execute\nshort prompt continuation and infilling tasks. The model's iterative generation\nframework allows users to control the degree of style transfer and structural\nsimilarity to the original composition. Objective and subjective evaluations\ndemonstrate ImprovNet's effectiveness in generating musically coherent\nimprovisations while maintaining structural relationships with the original\npieces. The model outperforms Anticipatory Music Transformer in short\ncontinuation and infilling tasks and successfully achieves recognizable genre\nconversion, with 79\\% of participants correctly identifying jazz-style\nimprovisations of classical pieces. Our code and demo page can be found at\nhttps://github.com/keshavbhandari/improvnet.\n","authors":["Keshav Bhandari","Sungkyun Chang","Tongyu Lu","Fareza R. Enus","Louis B. Bradshaw","Dorien Herremans","Simon Colton"],"pdf_url":"https://arxiv.org/pdf/2502.04522v4.pdf","comment":"10 pages, 6 figures, IJCNN 2025 conference"},{"id":"http://arxiv.org/abs/2501.16466v3","updated":"2025-05-16T14:55:52Z","published":"2025-01-27T19:58:29Z","title":"On the Feasibility of Using LLMs to Autonomously Execute Multi-host\n  Network Attacks","summary":"  LLMs have shown preliminary promise in some security tasks and CTF\nchallenges. Real cyberattacks are often multi-host network attacks, which\ninvolve executing a number of steps across multiple hosts such as conducting\nreconnaissance, exploiting vulnerabilities, and using compromised hosts to\nexfiltrate data. To date, the extent to which LLMs can autonomously execute\nmulti-host network attacks} is not well understood. To this end, our first\ncontribution is MHBench, an open-source multi-host attack benchmark with 10\nrealistic emulated networks (from 25 to 50 hosts). We find that popular LLMs\nincluding modern reasoning models (e.g., GPT4o, Gemini 2.5 Pro, Sonnet 3.7\nThinking) with state-of-art security-relevant prompting strategies (e.g.,\nPentestGPT, CyberSecEval3) cannot autonomously execute multi-host network\nattacks. To enable LLMs to autonomously execute such attacks, our second\ncontribution is Incalmo, an high-level abstraction layer. Incalmo enables LLMs\nto specify high-level actions (e.g., infect a host, scan a network). Incalmo's\ntranslation layer converts these actions into lower-level primitives (e.g.,\ncommands to exploit tools) through expert agents. In 9 out of 10 networks in\nMHBench, LLMs using Incalmo achieve at least some of the attack goals. Even\nsmaller LLMs (e.g., Haiku 3.5, Gemini 2 Flash) equipped with Incalmo achieve\nall goals in 5 of 10 environments. We also validate the key role of high-level\nactions in Incalmo's abstraction in enabling LLMs to autonomously execute such\nattacks.\n","authors":["Brian Singer","Keane Lucas","Lakshmi Adiga","Meghna Jain","Lujo Bauer","Vyas Sekar"],"pdf_url":"https://arxiv.org/pdf/2501.16466v3.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.11326v1","updated":"2025-05-16T14:48:30Z","published":"2025-05-16T14:48:30Z","title":"Temporally-Grounded Language Generation: A Benchmark for Real-Time\n  Vision-Language Models","summary":"  Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.\n","authors":["Keunwoo Peter Yu","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2505.11326v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2505.11311v1","updated":"2025-05-16T14:36:30Z","published":"2025-05-16T14:36:30Z","title":"Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for\n  Aerial Combat Tactics","summary":"  Artificial intelligence (AI) is reshaping strategic planning, with\nMulti-Agent Reinforcement Learning (MARL) enabling coordination among\nautonomous agents in complex scenarios. However, its practical deployment in\nsensitive military contexts is constrained by the lack of explainability, which\nis an essential factor for trust, safety, and alignment with human strategies.\nThis work reviews and assesses current advances in explainability methods for\nMARL with a focus on simulated air combat scenarios. We proceed by adapting\nvarious explainability techniques to different aerial combat scenarios to gain\nexplanatory insights about the model behavior. By linking AI-generated tactics\nwith human-understandable reasoning, we emphasize the need for transparency to\nensure reliable deployment and meaningful human-machine interaction. By\nilluminating the crucial importance of explainability in advancing MARL for\noperational defense, our work supports not only strategic planning but also the\ntraining of military personnel with insightful and comprehensible analyses.\n","authors":["Ardian Selmonaj","Alessandro Antonucci","Adrian Schneider","Michael Rüegsegger","Matthias Sommer"],"pdf_url":"https://arxiv.org/pdf/2505.11311v1.pdf","comment":"Published as a journal chapter in NATO Journal of Science and\n  Technology"},{"id":"http://arxiv.org/abs/2505.11304v1","updated":"2025-05-16T14:31:36Z","published":"2025-05-16T14:31:36Z","title":"Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent\n  Federated Learning","summary":"  Federated learning (FL) commonly involves clients with diverse communication\nand computational capabilities. Such heterogeneity can significantly distort\nthe optimization dynamics and lead to objective inconsistency, where the global\nmodel converges to an incorrect stationary point potentially far from the\npursued optimum. Despite its critical impact, the joint effect of communication\nand computation heterogeneity has remained largely unexplored, due to the\nintrinsic complexity of their interaction. In this paper, we reveal the\nfundamentally distinct mechanisms through which heterogeneous communication and\ncomputation drive inconsistency in FL. To the best of our knowledge, this is\nthe first unified theoretical analysis of general heterogeneous FL, offering a\nprincipled understanding of how these two forms of heterogeneity jointly\ndistort the optimization trajectory under arbitrary choices of local solvers.\nMotivated by these insights, we propose Federated Heterogeneity-Aware Client\nSampling, FedACS, a universal method to eliminate all types of objective\ninconsistency. We theoretically prove that FedACS converges to the correct\noptimum at a rate of $O(1/\\sqrt{R})$, even in dynamic heterogeneous\nenvironments. Extensive experiments across multiple datasets show that FedACS\noutperforms state-of-the-art and category-specific baselines by 4.3%-36%, while\nreducing communication costs by 22%-89% and computation loads by 14%-105%,\nrespectively.\n","authors":["Shudi Weng","Chao Ren","Ming Xiao","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2505.11304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10167v2","updated":"2025-05-16T14:30:51Z","published":"2025-05-15T10:51:34Z","title":"QuXAI: Explainers for Hybrid Quantum Machine Learning Models","summary":"  The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.\n  Our code and experiments are open-sourced at:\nhttps://github.com/GitsSaikat/QuXAI\n","authors":["Saikat Barua","Mostafizur Rahman","Shehenaz Khaled","Md Jafor Sadek","Rafiul Islam","Shahnewaz Siddique"],"pdf_url":"https://arxiv.org/pdf/2505.10167v2.pdf","comment":"16 pages, 6 figures, 7 equations"},{"id":"http://arxiv.org/abs/2505.11289v1","updated":"2025-05-16T14:24:03Z","published":"2025-05-16T14:24:03Z","title":"Meta-World+: An Improved, Standardized, RL Benchmark","summary":"  Meta-World is widely used for evaluating multi-task and meta-reinforcement\nlearning agents, which are challenged to master diverse skills simultaneously.\nSince its introduction however, there have been numerous undocumented changes\nwhich inhibit a fair comparison of algorithms. This work strives to\ndisambiguate these results from the literature, while also leveraging the past\nversions of Meta-World to provide insights into multi-task and\nmeta-reinforcement learning benchmark design. Through this process we release a\nnew open-source version of Meta-World\n(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility\nof past results, is more technically ergonomic, and gives users more control\nover the tasks that are included in a task set.\n","authors":["Reginald McLean","Evangelos Chatzaroulas","Luc McCutcheon","Frank Röder","Tianhe Yu","Zhanpeng He","K. R. Zentner","Ryan Julian","J K Terry","Isaac Woungang","Nariman Farsad","Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2505.11289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23362v2","updated":"2025-05-16T14:18:06Z","published":"2025-03-30T08:39:09Z","title":"Mixture of Routers","summary":"  Supervised fine-tuning (SFT) is a milestone in aligning large language models\nwith human instructions and adapting them to downstream tasks. In particular,\nLow-Rank Adaptation (LoRA) has gained widespread attention due to its parameter\nefficiency. However, its impact on improving the performance of large models\nremains limited. Recent studies suggest that combining LoRA with\nMixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE\nadapts to the diversity and complexity of datasets by dynamically selecting the\nmost suitable experts, thereby improving task accuracy and efficiency. Despite\nimpressive results, recent studies reveal issues in the MoE routing mechanism,\nsuch as incorrect assignments and imbalanced expert allocation. Inspired by the\nprinciples of Redundancy and Fault Tolerance Theory. We innovatively integrate\nthe concept of Mixture of Experts into the routing mechanism and propose an\nefficient fine-tuning method called Mixture of Routers (MoR). It employs\nmultiple sub-routers for joint selection and uses a learnable main router to\ndetermine the weights of the sub-routers. The results show that MoR outperforms\nbaseline models on most tasks, achieving an average performance improvement of\n1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method\nsuitable for a wide range of applications. Our code is available here:\nhttps://anonymous.4open.science/r/MoR-DFC6.\n","authors":["Jia-Chen Zhang","Yu-Jie Xiong","Xi-He Qiu","Chun-Ming Xia","Fei Dai"],"pdf_url":"https://arxiv.org/pdf/2503.23362v2.pdf","comment":"10 pages,4 figures"},{"id":"http://arxiv.org/abs/2501.11493v2","updated":"2025-05-16T14:14:55Z","published":"2025-01-20T13:59:41Z","title":"Communication-Efficient Federated Learning Based on Explanation-Guided\n  Pruning for Remote Sensing Image Classification","summary":"  Federated learning (FL) is a decentralized machine learning paradigm in which\nmultiple clients collaboratively train a global model by exchanging only model\nupdates with the central server without sharing the local data of the clients.\nDue to the large volume of model updates required to be transmitted between\nclients and the central server, most FL systems are associated with high\ntransfer costs (i.e., communication overhead). This issue is more critical for\noperational applications in remote sensing (RS), especially when large-scale RS\ndata is processed and analyzed through FL systems with restricted communication\nbandwidth. To address this issue, we introduce an explanation-guided pruning\nstrategy for communication-efficient FL in the context of RS image\nclassification. Our pruning strategy is defined based on the layer-wise\nrelevance propagation (LRP) driven explanations to: 1) efficiently and\neffectively identify the most relevant and informative model parameters (to be\nexchanged between clients and the central server); and 2) eliminate the\nnon-informative ones to minimize the volume of model updates. The experimental\nresults on the BigEarthNet-S2 dataset demonstrate that our strategy effectively\nreduces the number of shared model updates, while increasing the generalization\nability of the global model. The code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/FL-LRP.\n","authors":["Jonas Klotz","Barış Büyüktaş","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2501.11493v2.pdf","comment":"Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2025"},{"id":"http://arxiv.org/abs/2412.15529v3","updated":"2025-05-16T14:13:36Z","published":"2024-12-20T03:37:07Z","title":"XRAG: eXamining the Core -- Benchmarking Foundational Components in\n  Advanced Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points.\n","authors":["Qianren Mao","Yangyifei Luo","Qili Zhang","Yashuo Luo","Zhilong Cao","Jinlong Zhang","HanWen Hao","Zhijun Chen","Weifeng Jiang","Junnan Liu","Xiaolong Wang","Zhenting Huang","Zhixing Tan","Sun Jie","Bo Li","Xudong Liu","Richong Zhang","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2412.15529v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04973v2","updated":"2025-05-16T14:13:34Z","published":"2025-04-07T11:58:19Z","title":"Ensuring Safety in an Uncertain Environment: Constrained MDPs via\n  Stochastic Thresholds","summary":"  This paper studies constrained Markov decision processes (CMDPs) with\nconstraints against stochastic thresholds, aiming at the safety of\nreinforcement learning in unknown and uncertain environments. We leverage a\nGrowing-Window estimator sampling from interactions with the uncertain and\ndynamic environment to estimate the thresholds, based on which we design\nStochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based\nprimal-dual algorithm for multiple constraints against stochastic thresholds.\nSPOT enables reinforcement learning under both pessimistic and optimistic\nthreshold settings. We prove that our algorithm achieves sublinear regret and\nconstraint violation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$\nwhile allowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$\nepisodes. The theoretical guarantees show that our algorithm achieves\nperformance comparable to that of an approach relying on fixed and clear\nthresholds. To the best of our knowledge, SPOT is the first reinforcement\nlearning algorithm that realises theoretical guaranteed performance in an\nuncertain environment where even thresholds are unknown.\n","authors":["Qian Zuo","Fengxiang He"],"pdf_url":"https://arxiv.org/pdf/2504.04973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13955v4","updated":"2025-05-16T14:12:03Z","published":"2025-04-16T17:29:05Z","title":"Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling\n  Prolonged Exposure Therapy Conversations","summary":"  The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools.\n","authors":["Suhas BN","Andrew M. Sherrill","Rosa I. Arriaga","Chris W. Wiese","Saeed Abdullah"],"pdf_url":"https://arxiv.org/pdf/2504.13955v4.pdf","comment":"22 pages, 6 figures Updated Appendix with example model responses"},{"id":"http://arxiv.org/abs/2505.11277v1","updated":"2025-05-16T14:11:29Z","published":"2025-05-16T14:11:29Z","title":"Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs","summary":"  Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.\n","authors":["Yaorui Shi","Shihan Li","Chang Wu","Zhiyuan Liu","Junfeng Fang","Hengxing Cai","An Zhang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11274v1","updated":"2025-05-16T14:08:04Z","published":"2025-05-16T14:08:04Z","title":"SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning","summary":"  Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.\n","authors":["Zheng Li","Qingxiu Dong","Jingyuan Ma","Di Zhang","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2505.11274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11271v1","updated":"2025-05-16T14:04:31Z","published":"2025-05-16T14:04:31Z","title":"Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models","summary":"  Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.\n","authors":["Camille Couturier","Spyros Mastorakis","Haiying Shen","Saravan Rajmohan","Victor Rühle"],"pdf_url":"https://arxiv.org/pdf/2505.11271v1.pdf","comment":"Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings"},{"id":"http://arxiv.org/abs/2505.11270v1","updated":"2025-05-16T14:03:30Z","published":"2025-05-16T14:03:30Z","title":"TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes","summary":"  The variety of data in data lakes presents significant challenges for data\nanalytics, as data scientists must simultaneously analyze multi-modal data,\nincluding structured, semi-structured, and unstructured data. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities, they still\nremain inadequate for multi-modal data analytics in terms of accuracy,\nefficiency, and freshness. First, current natural language (NL) or SQL-like\nquery languages may struggle to precisely and comprehensively capture users'\nanalytical intent. Second, relying on a single unified LLM to process diverse\ndata modalities often leads to substantial inference overhead. Third, data\nstored in data lakes may be incomplete or outdated, making it essential to\nintegrate external open-domain knowledge to generate timely and relevant\nanalytics results.\n  In this paper, we envision a new multi-modal data analytics system.\nSpecifically, we propose a novel architecture built upon the Model Context\nProtocol (MCP), an emerging paradigm that enables LLMs to collaborate with\nknowledgeable agents. First, we define a semantic operator hierarchy tailored\nfor querying multi-modal data in data lakes and develop an AI-agent-powered\nNL2Operator translator to bridge user intent and analytical execution. Next, we\nintroduce an MCP-based execution framework, in which each MCP server hosts\nspecialized foundation models optimized for specific data modalities. This\ndesign enhances both accuracy and efficiency, while supporting high scalability\nthrough modular deployment. Finally, we propose a updating mechanism by\nharnessing the deep research and machine unlearning techniques to refresh the\ndata lakes and LLM knowledges, with the goal of balancing the data freshness\nand inference efficiency.\n","authors":["Chao Zhang","Shaolei Zhang","Quehuan Liu","Sibei Chen","Tong Li","Ju Fan"],"pdf_url":"https://arxiv.org/pdf/2505.11270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11267v1","updated":"2025-05-16T14:00:11Z","published":"2025-05-16T14:00:11Z","title":"Equal is Not Always Fair: A New Perspective on Hyperspectral\n  Representation Non-Uniformity","summary":"  Hyperspectral image (HSI) representation is fundamentally challenged by\npervasive non-uniformity, where spectral dependencies, spatial continuity, and\nfeature efficiency exhibit complex and often conflicting behaviors. Most\nexisting models rely on a unified processing paradigm that assumes homogeneity\nacross dimensions, leading to suboptimal performance and biased\nrepresentations. To address this, we propose FairHyp, a fairness-directed\nframework that explicitly disentangles and resolves the threefold\nnon-uniformity through cooperative yet specialized modules. We introduce a\nRunge-Kutta-inspired spatial variability adapter to restore spatial coherence\nunder resolution discrepancies, a multi-receptive field convolution module with\nsparse-aware refinement to enhance discriminative features while respecting\ninherent sparsity, and a spectral-context state space model that captures\nstable and long-range spectral dependencies via bidirectional Mamba scanning\nand statistical aggregation. Unlike one-size-fits-all solutions, FairHyp\nachieves dimension-specific adaptation while preserving global consistency and\nmutual reinforcement. This design is grounded in the view that non-uniformity\narises from the intrinsic structure of HSI representations, rather than any\nparticular task setting. To validate this, we apply FairHyp across four\nrepresentative tasks including classification, denoising, super-resolution, and\ninpaintin, demonstrating its effectiveness in modeling a shared structural\nflaw. Extensive experiments show that FairHyp consistently outperforms\nstate-of-the-art methods under varied imaging conditions. Our findings redefine\nfairness as a structural necessity in HSI modeling and offer a new paradigm for\nbalancing adaptability, efficiency, and fidelity in high-dimensional vision\ntasks.\n","authors":["Wuzhou Quan","Mingqiang Wei","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2505.11267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04165v4","updated":"2025-05-16T13:56:30Z","published":"2025-05-07T06:34:34Z","title":"TS-SNN: Temporal Shift Module for Spiking Neural Networks","summary":"  Spiking Neural Networks (SNNs) are increasingly recognized for their\nbiological plausibility and energy efficiency, positioning them as strong\nalternatives to Artificial Neural Networks (ANNs) in neuromorphic computing\napplications. SNNs inherently process temporal information by leveraging the\nprecise timing of spikes, but balancing temporal feature utilization with low\nenergy consumption remains a challenge. In this work, we introduce Temporal\nShift module for Spiking Neural Networks (TS-SNN), which incorporates a novel\nTemporal Shift (TS) module to integrate past, present, and future spike\nfeatures within a single timestep via a simple yet effective shift operation. A\nresidual combination method prevents information loss by integrating shifted\nand original features. The TS module is lightweight, requiring only one\nadditional learnable parameter, and can be seamlessly integrated into existing\narchitectures with minimal additional computational cost. TS-SNN achieves\nstate-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100\n(80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low\nenergy consumption. This work marks a significant step forward in developing\nefficient and accurate SNN architectures.\n","authors":["Kairong Yu","Tianqing Zhang","Qi Xu","Gang Pan","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04165v4.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2505.11247v1","updated":"2025-05-16T13:41:05Z","published":"2025-05-16T13:41:05Z","title":"LD-Scene: LLM-Guided Diffusion for Controllable Generation of\n  Adversarial Safety-Critical Driving Scenarios","summary":"  Ensuring the safety and robustness of autonomous driving systems necessitates\na comprehensive evaluation in safety-critical scenarios. However, these\nsafety-critical scenarios are rare and difficult to collect from real-world\ndriving data, posing significant challenges to effectively assessing the\nperformance of autonomous vehicles. Typical existing methods often suffer from\nlimited controllability and lack user-friendliness, as extensive expert\nknowledge is essentially required. To address these challenges, we propose\nLD-Scene, a novel framework that integrates Large Language Models (LLMs) with\nLatent Diffusion Models (LDMs) for user-controllable adversarial scenario\ngeneration through natural language. Our approach comprises an LDM that\ncaptures realistic driving trajectory distributions and an LLM-based guidance\nmodule that translates user queries into adversarial loss functions,\nfacilitating the generation of scenarios aligned with user queries. The\nguidance module integrates an LLM-based Chain-of-Thought (CoT) code generator\nand an LLM-based code debugger, enhancing the controllability and robustness in\ngenerating guidance functions. Extensive experiments conducted on the nuScenes\ndataset demonstrate that LD-Scene achieves state-of-the-art performance in\ngenerating realistic, diverse, and effective adversarial scenarios.\nFurthermore, our framework provides fine-grained control over adversarial\nbehaviors, thereby facilitating more effective testing tailored to specific\ndriving scenarios.\n","authors":["Mingxing Peng","Yuting Xie","Xusen Guo","Ruoyu Yao","Hai Yang","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2505.11247v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.11243v1","updated":"2025-05-16T13:36:07Z","published":"2025-05-16T13:36:07Z","title":"A Set-Sequence Model for Time Series","summary":"  In many financial prediction problems, the behavior of individual units (such\nas loans, bonds, or stocks) is influenced by observable unit-level factors and\nmacroeconomic variables, as well as by latent cross-sectional effects.\nTraditional approaches attempt to capture these latent effects via handcrafted\nsummary features. We propose a Set-Sequence model that eliminates the need for\nhandcrafted features. The Set model first learns a shared cross-sectional\nsummary at each period. The Sequence model then ingests the summary-augmented\ntime series for each unit independently to predict its outcome. Both components\nare learned jointly over arbitrary sets sampled during training. Our approach\nharnesses the set nature of the cross-section and is computationally efficient,\ngenerating set summaries in linear time relative to the number of units. It is\nalso flexible, allowing the use of existing sequence models and accommodating a\nvariable number of units at inference. Empirical evaluations demonstrate that\nour Set-Sequence model significantly outperforms benchmarks on stock return\nprediction and mortgage behavior tasks. Code will be released.\n","authors":["Elliot L. Epstein","Apaar Sadhwani","Kay Giesecke"],"pdf_url":"https://arxiv.org/pdf/2505.11243v1.pdf","comment":"Presented at the Workshop on Financial AI at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.16529v2","updated":"2025-05-16T13:29:19Z","published":"2025-03-18T08:38:10Z","title":"Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts","summary":"  DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for the entire DeepSeek-R1 model series. Evaluation results\nindicate that the enhanced models achieve significant improvements in safety\nwhile maintaining reasoning capabilities without notable degradation. We\nopen-source the safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource\nfor future research and optimization of DeepSeek models.\n","authors":["Wenjing Zhang","Xuejiao Lei","Zhaoxiang Liu","Limin Han","Jiaojiao Zhao","Junting Guo","Zhenhong Long","Shu Yang","Meijuan An","Beibei Huang","Rongjia Du","Ning Wang","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2503.16529v2.pdf","comment":"21 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.13977v2","updated":"2025-05-16T13:25:28Z","published":"2025-01-23T00:26:32Z","title":"Re-ranking Using Large Language Models for Mitigating Exposure to\n  Harmful Content on Social Media Platforms","summary":"  Social media platforms utilize Machine Learning (ML) and Artificial\nIntelligence (AI) powered recommendation algorithms to maximize user\nengagement, which can result in inadvertent exposure to harmful content.\nCurrent moderation efforts, reliant on classifiers trained with extensive\nhuman-annotated data, struggle with scalability and adapting to new forms of\nharm. To address these challenges, we propose a novel re-ranking approach using\nLarge Language Models (LLMs) in zero-shot and few-shot settings. Our method\ndynamically assesses and re-ranks content sequences, effectively mitigating\nharmful content exposure without requiring extensive labeled data. Alongside\ntraditional ranking metrics, we also introduce two new metrics to evaluate the\neffectiveness of re-ranking in reducing exposure to harmful content. Through\nexperiments on three datasets, three models and across three configurations, we\ndemonstrate that our LLM-based approach significantly outperforms existing\nproprietary moderation approaches, offering a scalable and adaptable solution\nfor harm mitigation.\n","authors":["Rajvardhan Oak","Muhammad Haroon","Claire Jo","Magdalena Wojcieszak","Anshuman Chhabra"],"pdf_url":"https://arxiv.org/pdf/2501.13977v2.pdf","comment":"Accepted to ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2505.11227v1","updated":"2025-05-16T13:23:26Z","published":"2025-05-16T13:23:26Z","title":"Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability\n  in LLMs","summary":"  The development of reasoning capabilities represents a critical frontier in\nlarge language models (LLMs) research, where reinforcement learning (RL) and\nprocess reward models (PRMs) have emerged as predominant methodological\nframeworks. Contrary to conventional wisdom, empirical evidence from\nDeepSeek-R1 demonstrates that pure RL training focused on mathematical\nproblem-solving can progressively enhance reasoning abilities without PRM\nintegration, challenging the perceived necessity of process supervision. In\nthis study, we conduct a systematic investigation of the relationship between\nRL training and PRM capabilities. Our findings demonstrate that problem-solving\nproficiency and process supervision capabilities represent complementary\ndimensions of reasoning that co-evolve synergistically during pure RL training.\nIn particular, current PRMs underperform simple baselines like majority voting\nwhen applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To\naddress this limitation, we propose Self-PRM, an introspective framework in\nwhich models autonomously evaluate and rerank their generated solutions through\nself-reward mechanisms. Although Self-PRM consistently improves the accuracy of\nthe benchmark (particularly with larger sample sizes), analysis exposes\npersistent challenges: The approach exhibits low precision (<10\\%) on difficult\nproblems, frequently misclassifying flawed solutions as valid. These analyses\nunderscore the need for continued RL scaling to improve reward alignment and\nintrospective accuracy. Overall, our findings suggest that PRM may not be\nessential for enhancing complex reasoning, as pure RL not only improves\nproblem-solving skills but also inherently fosters robust PRM capabilities. We\nhope these findings provide actionable insights for building more reliable and\nself-aware complex reasoning models.\n","authors":["Zhangying Feng","Qianglong Chen","Ning Lu","Yongqian Li","Siqi Cheng","Shuangmu Peng","Duyu Tang","Shengcai Liu","Zhirui Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00715v5","updated":"2025-05-16T13:16:11Z","published":"2024-04-25T15:34:53Z","title":"Towards Adapting Open-Source Large Language Models for Expert-Level\n  Clinical Note Generation","summary":"  Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across all three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than\nphysician-authored notes (4.1/5). We highlight key considerations for future\nclinical note-generation tasks, emphasizing the importance of pre-defining a\nbest-practice note format, rather than relying on LLMs to determine this for\nclinical practice.\n","authors":["Hanyin Wang","Chufan Gao","Bolun Liu","Qiping Xu","Guleid Hussein","Mohamad El Labban","Kingsley Iheasirim","Hariprasad Korsapati","Chuck Outcalt","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2405.00715v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11217v1","updated":"2025-05-16T13:13:25Z","published":"2025-05-16T13:13:25Z","title":"Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI\n  models in Sound Localization","summary":"  Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we finetune a\nstate-of-the-art model using a stereo audio-image dataset generated via 3D\nsimulations. Even with limited training data, the refined model surpasses\nexisting benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy.\n","authors":["Yanhao Jia","Ji Xie","S Jivaganesh","Hao Li","Xu Wu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11217v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.04207v2","updated":"2025-05-16T13:12:38Z","published":"2025-05-07T07:58:57Z","title":"An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection\n  and Measurement","summary":"  Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired.\n","authors":["Mustafa Yurdakul","Şakir Tasdemir"],"pdf_url":"https://arxiv.org/pdf/2505.04207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04969v2","updated":"2025-05-16T13:08:40Z","published":"2024-05-08T11:15:20Z","title":"A Review on Discriminative Self-supervised Learning Methods in Computer\n  Vision","summary":"  Self-supervised learning (SSL) has rapidly emerged as a transformative\napproach in computer vision, enabling the extraction of rich feature\nrepresentations from vast amounts of unlabeled data and reducing reliance on\ncostly manual annotations. This review presents a comprehensive analysis of\ndiscriminative SSL methods, which focus on learning representations by solving\npretext tasks that do not require human labels. The paper systematically\ncategorizes discriminative SSL approaches into five main groups: contrastive\nmethods, clustering methods, self-distillation methods, knowledge distillation\nmethods, and feature decorrelation methods. For each category, the review\ndetails the underlying principles, architectural components, loss functions,\nand representative algorithms, highlighting their unique mechanisms and\ncontributions to the field. Extensive comparative evaluations are provided,\nincluding linear and semi-supervised protocols on standard benchmarks such as\nImageNet, as well as transfer learning performance across diverse downstream\ntasks. The review also discusses theoretical foundations, scalability,\nefficiency, and practical challenges, such as computational demands and\naccessibility. By synthesizing recent advancements and identifying key trends,\nopen challenges, and future research directions, this work serves as a valuable\nresource for researchers and practitioners aiming to leverage discriminative\nSSL for robust and generalizable computer vision models.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki","Athanasios Gkelias"],"pdf_url":"https://arxiv.org/pdf/2405.04969v2.pdf","comment":"Preprint. 97 pages, 12 figures, 16 tables"},{"id":"http://arxiv.org/abs/2505.11211v1","updated":"2025-05-16T13:06:25Z","published":"2025-05-16T13:06:25Z","title":"Bayesian Hierarchical Invariant Prediction","summary":"  We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP.\n","authors":["Francisco Madaleno","Pernille Julie Viuff Sand","Francisco C. Pereira","Sergio Hernan Garrido Mejia"],"pdf_url":"https://arxiv.org/pdf/2505.11211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11208v1","updated":"2025-05-16T13:05:45Z","published":"2025-05-16T13:05:45Z","title":"GLOVA: Global and Local Variation-Aware Analog Circuit Design with\n  Risk-Sensitive Reinforcement Learning","summary":"  Analog/mixed-signal circuit design encounters significant challenges due to\nperformance degradation from process, voltage, and temperature (PVT)\nvariations. To achieve commercial-grade reliability, iterative manual design\nrevisions and extensive statistical simulations are required. While several\nstudies have aimed to automate variation aware analog design to reduce\ntime-to-market, the substantial mismatches in real-world wafers have not been\nthoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing\nframework that effectively manages the impact of diverse random mismatches to\nimprove robustness against PVT variations. In the proposed approach,\nrisk-sensitive reinforcement learning is leveraged to account for the\nreliability bound affected by PVT variations, and ensemble-based critic is\nintroduced to achieve sample-efficient learning. For design verification, we\nalso propose $\\mu$-$\\sigma$ evaluation and simulation reordering method to\nreduce simulation costs of identifying failed designs. GLOVA supports\nverification through industrial-level PVT variation evaluation methods,\nincluding corner simulation as well as global and local Monte Carlo (MC)\nsimulations. Compared to previous state-of-the-art variation-aware analog\nsizing frameworks, GLOVA achieves up to 80.5$\\times$ improvement in sample\nefficiency and 76.0$\\times$ reduction in time.\n","authors":["Dongjun Kim","Junwoo Park","Chaehyeon Shin","Jaeheon Jung","Kyungho Shin","Seungheon Baek","Sanghyuk Heo","Woongrae Kim","Inchul Jeong","Joohwan Cho","Jongsun Park"],"pdf_url":"https://arxiv.org/pdf/2505.11208v1.pdf","comment":"Accepted for DAC 2025"},{"id":"http://arxiv.org/abs/2411.08881v2","updated":"2025-05-16T13:05:27Z","published":"2024-10-25T20:17:59Z","title":"Can We Trust AI Agents? A Case Study of an LLM-Based Multi-Agent System\n  for Ethical AI","summary":"  AI-based systems, including Large Language Models (LLM), impact millions by\nsupporting diverse tasks but face issues like misinformation, bias, and misuse.\nAI ethics is crucial as new technologies and concerns emerge, but objective,\npractical guidance remains debated. This study examines the use of LLMs for AI\nethics in practice, assessing how LLM trustworthiness-enhancing techniques\naffect software development in this context. Using the Design Science Research\n(DSR) method, we identify techniques for LLM trustworthiness: multi-agents,\ndistinct roles, structured communication, and multiple rounds of debate. We\ndesign a multi-agent prototype LLM-MAS, where agents engage in structured\ndiscussions on real-world AI ethics issues from the AI Incident Database. We\nevaluate the prototype across three case scenarios using thematic analysis,\nhierarchical clustering, comparative (baseline) studies, and running source\ncode. The system generates approximately 2,000 lines of code per case, compared\nto only 80 lines in baseline trials. Discussions reveal terms like bias\ndetection, transparency, accountability, user consent, GDPR compliance,\nfairness evaluation, and EU AI Act compliance, showing this prototype ability\nto generate extensive source code and documentation addressing often overlooked\nAI ethics issues. However, practical challenges in source code integration and\ndependency management may limit its use by practitioners.\n","authors":["José Antonio Siqueira de Cerqueira","Mamia Agbese","Rebekah Rousi","Nannan Xi","Juho Hamari","Pekka Abrahamsson"],"pdf_url":"https://arxiv.org/pdf/2411.08881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11204v1","updated":"2025-05-16T13:02:12Z","published":"2025-05-16T13:02:12Z","title":"RanDeS: Randomized Delta Superposition for Multi-Model Compression","summary":"  From a multi-model compression perspective, model merging enables\nmemory-efficient serving of multiple models fine-tuned from the same base, but\nsuffers from degraded performance due to interference among their task-specific\nparameter adjustments (i.e., deltas). In this paper, we reformulate model\nmerging as a compress-and-retrieve scheme, revealing that the task interference\narises from the summation of irrelevant deltas during model retrieval. To\naddress this issue, we use random orthogonal transformations to decorrelate\nthese vectors into self-cancellation. We show that this approach drastically\nreduces interference, improving performance across both vision and language\ntasks. Since these transformations are fully defined by random seeds, adding\nnew models requires no extra memory. Further, their data- and model-agnostic\nnature enables easy addition or removal of models with minimal compute\noverhead, supporting efficient and flexible multi-model serving.\n","authors":["Hangyu Zhou","Aaron Gokaslan","Volodymyr Kuleshov","Bharath Hariharan"],"pdf_url":"https://arxiv.org/pdf/2505.11204v1.pdf","comment":"https://github.com/Zhou-Hangyu/randes"},{"id":"http://arxiv.org/abs/2505.11200v1","updated":"2025-05-16T12:57:23Z","published":"2025-05-16T12:57:23Z","title":"Audio Turing Test: Benchmarking the Human-likeness of Large Language\n  Model-based Text-to-Speech Systems in Chinese","summary":"  Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).\n","authors":["Xihuai Wang","Ziyi Zhao","Siyu Ren","Shao Zhang","Song Li","Xiaoyu Li","Ziwen Wang","Lin Qiu","Guanglu Wan","Xuezhi Cao","Xunliang Cai","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11200v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.11198v1","updated":"2025-05-16T12:56:40Z","published":"2025-05-16T12:56:40Z","title":"User-centric Music Recommendations","summary":"  This work presents a user-centric recommendation framework, designed as a\npipeline with four distinct, connected, and customizable phases. These phases\nare intended to improve explainability and boost user engagement.\n  We have collected the historical Last.fm track playback records of a single\nuser over approximately 15 years. The collected dataset includes more than\n90,000 playbacks and approximately 14,000 unique tracks.\n  From track playback records, we have created a dataset of user temporal\ncontexts (each row is a specific moment when the user listened to certain music\ndescriptors). As music descriptors, we have used community-contributed Last.fm\ntags and Spotify audio features. They represent the music that, throughout\nyears, the user has been listening to.\n  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the\nday), we predict the Spotify audio features that best fit the user preferences\nin that particular moment. Finally, we use the predicted audio features to find\ntracks similar to these features. The final aim is to recommend (and discover)\ntracks that the user may feel like listening to at a particular moment.\n  For our initial study case, we have chosen to predict only a single audio\nfeature target: danceability. The framework, however, allows to include more\ntarget variables.\n  The ability to learn the musical habits from a single user can be quite\npowerful, and this framework could be extended to other users.\n","authors":["Jaime Ramirez Castillo","M. Julia Flores","Ann E. Nicholson"],"pdf_url":"https://arxiv.org/pdf/2505.11198v1.pdf","comment":"Accepted for the 16th Bayesian Modelling Applications Workshop\n  (@UAI2022) (BMAW 2022)"},{"id":"http://arxiv.org/abs/2405.01053v5","updated":"2025-05-16T12:49:48Z","published":"2024-05-02T07:15:23Z","title":"On the Universality of Self-Supervised Learning","summary":"  In this paper, we investigate what constitutes a good representation or model\nin self-supervised learning (SSL). We argue that a good representation should\nexhibit universality, characterized by three essential properties:\ndiscriminability, generalizability, and transferability. While these\ncapabilities are implicitly desired in most SSL frameworks, existing methods\nlack an explicit modeling of universality, and its theoretical foundations\nremain underexplored. To address these gaps, we propose General SSL (GeSSL), a\nnovel framework that explicitly models universality from three complementary\ndimensions: the optimization objective, the parameter update mechanism, and the\nlearning paradigm. GeSSL integrates a bi-level optimization structure that\njointly models task-specific adaptation and cross-task consistency, thereby\ncapturing all three aspects of universality within a unified SSL objective.\nFurthermore, we derive a theoretical generalization bound, ensuring that the\noptimization process of GeSSL consistently leads to representations that\ngeneralize well to unseen tasks. Empirical results on multiple benchmark\ndatasets demonstrate that GeSSL consistently achieves superior performance\nacross diverse downstream tasks, validating its effectiveness in modeling\nuniversal representations.\n","authors":["Wenwen Qiang","Jingyao Wang","Changwen Zheng","Hui Xiong","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2405.01053v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11191v1","updated":"2025-05-16T12:49:36Z","published":"2025-05-16T12:49:36Z","title":"Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied\n  AI: Potentials and Challenges for Edge Integration","summary":"  As embodied AI systems become increasingly multi-modal, personalized, and\ninteractive, they must learn effectively from diverse sensory inputs, adapt\ncontinually to user preferences, and operate safely under resource and privacy\nconstraints. These challenges expose a pressing need for machine learning\nmodels capable of swift, context-aware adaptation while balancing model\ngeneralization and personalization. Here, two methods emerge as suitable\ncandidates, each offering parts of these capabilities: Foundation Models (FMs)\nprovide a pathway toward generalization across tasks and modalities, whereas\nFederated Learning (FL) offers the infrastructure for distributed,\nprivacy-preserving model updates and user-level model personalization. However,\nwhen used in isolation, each of these approaches falls short of meeting the\ncomplex and diverse capability requirements of real-world embodied\nenvironments. In this vision paper, we introduce Federated Foundation Models\n(FFMs) for embodied AI, a new paradigm that unifies the strengths of\nmulti-modal multi-task (M3T) FMs with the privacy-preserving distributed nature\nof FL, enabling intelligent systems at the wireless edge. We collect critical\ndeployment dimensions of FFMs in embodied AI ecosystems under a unified\nframework, which we name \"EMBODY\": Embodiment heterogeneity, Modality richness\nand imbalance, Bandwidth and compute constraints, On-device continual learning,\nDistributed control and autonomy, and Yielding safety, privacy, and\npersonalization. For each, we identify concrete challenges and envision\nactionable research directions. We also present an evaluation framework for\ndeploying FFMs in embodied AI systems, along with the associated trade-offs.\n","authors":["Kasra Borazjani","Payam Abdisarabshali","Fardis Nadimi","Naji Khosravan","Minghui Liwang","Xianbin Wang","Yiguang Hong","Seyyedali Hosseinalipour"],"pdf_url":"https://arxiv.org/pdf/2505.11191v1.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.11189v1","updated":"2025-05-16T12:48:44Z","published":"2025-05-16T12:48:44Z","title":"Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule\n  Extraction vs RuleSHAP","summary":"  Generative AI systems can help spread information but also misinformation and\nbiases, potentially undermining the UN Sustainable Development Goals (SDGs).\nExplainable AI (XAI) aims to reveal the inner workings of AI systems and expose\nmisbehaviours or biases. However, current XAI tools, built for simpler models,\nstruggle to handle the non-numerical nature of large language models (LLMs).\nThis paper examines the effectiveness of global XAI methods, such as\nrule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we\nfirst show a text-to-ordinal mapping strategy to convert non-numerical\ninputs/outputs into numerical features, enabling these tools to identify (some)\nmisinformation-related biases in LLM-generated content. Then, we inject\nnon-linear biases of varying complexity (univariate, conjunctive, and\nnon-convex) into widespread LLMs like ChatGPT and Llama via system\ninstructions, using global XAI methods to detect them. This way, we found that\nRuleFit struggles with conjunctive and non-convex biases, while SHAP can\napproximate conjunctive biases but cannot express them as actionable rules.\nHence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP\nand RuleFit to detect more non-univariate biases, improving injected bias\ndetection over RuleFit by +94% (MRR@1) on average.\n","authors":["Francesco Sovrano"],"pdf_url":"https://arxiv.org/pdf/2505.11189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02433v2","updated":"2025-05-16T12:47:32Z","published":"2025-05-05T07:58:54Z","title":"FairPO: Robust Preference Optimization for Fair Multi-Label Learning","summary":"  We propose FairPO, a novel framework designed to promote fairness in\nmulti-label classification by directly optimizing preference signals with a\ngroup robustness perspective. In our framework, the set of labels is\npartitioned into privileged and non-privileged groups, and a preference-based\nloss inspired by Direct Preference Optimization (DPO) is employed to more\neffectively differentiate true positive labels from confusing negatives within\nthe privileged group, while preserving baseline classification performance for\nnon-privileged labels. By framing the learning problem as a robust optimization\nover groups, our approach dynamically adjusts the training emphasis toward\ngroups with poorer performance, thereby mitigating bias and ensuring a fairer\ntreatment across diverse label categories. In addition, we outline plans to\nextend this approach by investigating alternative loss formulations such as\nSimple Preference Optimisation (SimPO) and Contrastive Preference Optimization\n(CPO) to exploit reference-free reward formulations and contrastive training\nsignals. Furthermore, we plan to extend FairPO with multilabel generation\ncapabilities, enabling the model to dynamically generate diverse and coherent\nlabel sets for ambiguous inputs.\n","authors":["Soumen Kumar Mondal","Akshit Varmora","Prateek Chanda","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2505.02433v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16525v2","updated":"2025-05-16T12:42:48Z","published":"2025-03-17T16:43:35Z","title":"KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse","summary":"  Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.\n","authors":["Huan Yang","Renji Zhang","Mingzhe Huang","Weijun Wang","Yin Tang","Yuanchun Li","Yunxin Liu","Deyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11182v1","updated":"2025-05-16T12:37:10Z","published":"2025-05-16T12:37:10Z","title":"Imputation-free and Alignment-free: Incomplete Multi-view Clustering\n  Driven by Consensus Semantic Learning","summary":"  In incomplete multi-view clustering (IMVC), missing data induce prototype\nshifts within views and semantic inconsistencies across views. A feasible\nsolution is to explore cross-view consistency in paired complete observations,\nfurther imputing and aligning the similarity relationships inherently shared\nacross views. Nevertheless, existing methods are constrained by two-tiered\nlimitations: (1) Neither instance- nor cluster-level consistency learning\nconstruct a semantic space shared across views to learn consensus semantics.\nThe former enforces cross-view instances alignment, and wrongly regards\nunpaired observations with semantic consistency as negative pairs; the latter\nfocuses on cross-view cluster counterparts while coarsely handling fine-grained\nintra-cluster relationships within views. (2) Excessive reliance on consistency\nresults in unreliable imputation and alignment without incorporating\nview-specific cluster information. Thus, we propose an IMVC framework,\nimputation- and alignment-free for consensus semantics learning (FreeCSL). To\nbridge semantic gaps across all observations, we learn consensus prototypes\nfrom available data to discover a shared space, where semantically similar\nobservations are pulled closer for consensus semantics learning. To capture\nsemantic relationships within specific views, we design a heuristic graph\nclustering based on modularity to recover cluster structure with intra-cluster\ncompactness and inter-cluster separation for cluster semantics enhancement.\nExtensive experiments demonstrate, compared to state-of-the-art competitors,\nFreeCSL achieves more confident and robust assignments on IMVC task.\n","authors":["Yuzhuo Dai","Jiaqi Jin","Zhibin Dong","Siwei Wang","Xinwang Liu","En Zhu","Xihong Yang","Xinbiao Gan","Yu Feng"],"pdf_url":"https://arxiv.org/pdf/2505.11182v1.pdf","comment":"The paper has been accepted by the 42nd CVPR 2025. The main text has\n  9 pages, including 8 figures and 4 tables. The appendix has 8 pages, with 10\n  figures and 6 tables. The reference list has 3 pages"},{"id":"http://arxiv.org/abs/2505.11181v1","updated":"2025-05-16T12:37:08Z","published":"2025-05-16T12:37:08Z","title":"Feasibility with Language Models for Open-World Compositional Zero-Shot\n  Learning","summary":"  Humans can easily tell if an attribute (also called state) is realistic,\ni.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In\nOpen-World Compositional Zero-Shot Learning, when all possible state-object\ncombinations are considered as unseen classes, zero-shot predictors tend to\nperform poorly. Our work focuses on using external auxiliary knowledge to\ndetermine the feasibility of state-object combinations. Our Feasibility with\nLanguage Model (FLM) is a simple and effective approach that leverages Large\nLanguage Models (LLMs) to better comprehend the semantic relationships between\nstates and objects. FLM involves querying an LLM about the feasibility of a\ngiven pair and retrieving the output logit for the positive answer. To mitigate\npotential misguidance of the LLM given that many of the state-object\ncompositions are rare or completely infeasible, we observe that the in-context\nlearning ability of LLMs is essential. We present an extensive study\nidentifying Vicuna and ChatGPT as best performing, and we demonstrate that our\nFLM consistently improves OW-CZSL performance across all three benchmarks.\n","authors":["Jae Myung Kim","Stephan Alaniz","Cordelia Schmid","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2505.11181v1.pdf","comment":"ECCV Workshop in OOD-CV, 2024"},{"id":"http://arxiv.org/abs/2404.13274v5","updated":"2025-05-16T12:29:57Z","published":"2024-04-20T05:14:52Z","title":"Augmented Object Intelligence with XR-Objects","summary":"  Seamless integration of physical objects as interactive digital entities\nremains a challenge for spatial computing. This paper explores Augmented Object\nIntelligence (AOI) in the context of XR, an interaction paradigm that aims to\nblur the lines between digital and physical by equipping real-world objects\nwith the ability to interact as if they were digital, where every object has\nthe potential to serve as a portal to digital functionalities. Our approach\nutilizes real-time object segmentation and classification, combined with the\npower of Multimodal Large Language Models (MLLMs), to facilitate these\ninteractions without the need for object pre-registration. We implement the AOI\nconcept in the form of XR-Objects, an open-source prototype system that\nprovides a platform for users to engage with their physical environment in\ncontextually relevant ways using object-based context menus. This system\nenables analog objects to not only convey information but also to initiate\ndigital actions, such as querying for details or executing tasks. Our\ncontributions are threefold: (1) we define the AOI concept and detail its\nadvantages over traditional AI assistants, (2) detail the XR-Objects system's\nopen-source design and implementation, and (3) show its versatility through\nvarious use cases and a user study.\n","authors":["Mustafa Doga Dogan","Eric J. Gonzalez","Karan Ahuja","Ruofei Du","Andrea Colaço","Johnny Lee","Mar Gonzalez-Franco","David Kim"],"pdf_url":"https://arxiv.org/pdf/2404.13274v5.pdf","comment":"15 pages, 15 figures, 2024 ACM Symposium on User Interface Software\n  and Technology (UIST)"},{"id":"http://arxiv.org/abs/2410.06883v4","updated":"2025-05-16T12:24:56Z","published":"2024-10-09T13:45:54Z","title":"Degree-Conscious Spiking Graph for Cross-Domain Adaptation","summary":"  Spiking Graph Networks (SGNs) have demonstrated significant potential in\ngraph classification by emulating brain-inspired neural dynamics to achieve\nenergy-efficient computation. However, existing SGNs are generally constrained\nto in-distribution scenarios and struggle with distribution shifts. In this\npaper, we first propose the domain adaptation problem in SGNs, and introduce a\nnovel framework named Degree-Consicious Spiking Graph for Cross-Domain\nAdaptation. DeSGraDA enhances generalization across domains with three key\ncomponents. First, we introduce the degree-conscious spiking representation\nmodule by adapting spike thresholds based on node degrees, enabling more\nexpressive and structure-aware signal encoding. Then, we perform temporal\ndistribution alignment by adversarially matching membrane potentials between\ndomains, ensuring effective performance under domain shift while preserving\nenergy efficiency. Additionally, we extract consistent predictions across two\nspaces to create reliable pseudo-labels, effectively leveraging unlabeled data\nto enhance graph classification performance. Furthermore, we establish the\nfirst generalization bound for SGDA, providing theoretical insights into its\nadaptation performance. Extensive experiments on benchmark datasets validate\nthat DeSGraDA consistently outperforms state-of-the-art methods in both\nclassification accuracy and energy efficiency.\n","authors":["Yingxu Wang","Mengzhu Wang","Siwei Liu","Houcheng Su","Nan Yin","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2410.06883v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11178v1","updated":"2025-05-16T12:23:58Z","published":"2025-05-16T12:23:58Z","title":"CompAlign: Improving Compositional Text-to-Image Generation with a\n  Complex Benchmark and Fine-Grained Feedback","summary":"  State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.\n","authors":["Yixin Wan","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2505.11178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15371v9","updated":"2025-05-16T12:21:57Z","published":"2024-09-19T10:26:42Z","title":"Balancing LoRA Performance and Efficiency with Simple Shard Sharing","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), effectively reduce the number of trainable parameters in\nLarge Language Models (LLMs). However, as model scales continue to grow, the\ndemand for computational resources remains a significant challenge. Existing\nLoRA variants often struggle to strike an optimal balance between adaptability\n(model performance and convergence speed) and efficiency (computational\noverhead, memory usage, and initialization time). This paper introduces\nFOSSIL(\\textbf{F}ramework for \\textbf{O}ptimal \\textbf{S}hard \\textbf{S}haring\n\\textbf{I}ntegration in \\textbf{L}oRA), a novel PEFT approach that addresses\nthis trade-off through a simple shard-sharing mechanism. FOSSIL leverages the\ninsight that a low-rank adaptation can be achieved by decomposing the weight\nmatrix into multiple fragment matrices and utilizing a shared, trainable common\nfragment. This method constructs the low-rank update matrix through the\nreplication of these shared, partitioned shards. We also propose a\nhardware-efficient and broadly applicable implementation for FOSSIL. Extensive\nexperiments conducted on a range of tasks, alongside a systematic analysis of\ncomputational performance, demonstrate FOSSIL's superiority. The results show\nthat FOSSIL significantly outperforms standard LoRA and its prominent variants\nin both model performance metrics and computational efficiency, including\ninitialization speed and training throughput. By effectively balancing\nexpressive power and resource utilization, FOSSIL offers a compelling solution\nfor efficiently adapting large-scale models.\n","authors":["Jiale Kang","Qingyu Yin"],"pdf_url":"https://arxiv.org/pdf/2409.15371v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11177v1","updated":"2025-05-16T12:20:37Z","published":"2025-05-16T12:20:37Z","title":"Low-Resource Language Processing: An OCR-Driven Summarization and\n  Translation Pipeline","summary":"  This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments\n","authors":["Hrishit Madhavi","Jacob Cherian","Yuvraj Khamkar","Dhananjay Bhagat"],"pdf_url":"https://arxiv.org/pdf/2505.11177v1.pdf","comment":"8 pages, 7 figures, direct arXiv submission"},{"id":"http://arxiv.org/abs/2505.11176v1","updated":"2025-05-16T12:20:31Z","published":"2025-05-16T12:20:31Z","title":"From Intent Discovery to Recognition with Topic Modeling and Synthetic\n  Data","summary":"  Understanding and recognizing customer intents in AI systems is crucial,\nparticularly in domains characterized by short utterances and the cold start\nproblem, where recommender systems must include new products or services\nwithout sufficient real user data. Customer utterances are characterized by\ninfrequent word co-occurences and high term variability, which poses\nsignificant challenges for traditional methods in specifying distinct user\nneeds and preparing synthetic queries. To address this, we propose an agentic\nLLM framework for topic modeling and synthetic query generation, which\naccelerates the discovery and recognition of customer intents. We first apply\nhierarchical topic modeling and intent discovery to expand a human-curated\ntaxonomy from 36 generic user intents to 278 granular intents, demonstrating\nthe potential of LLMs to significantly enhance topic specificity and diversity.\nNext, to support newly discovered intents and address the cold start problem,\nwe generate synthetic user query data, which augments real utterances and\nreduces dependency on human annotation, especially in low-resource settings.\nTopic model experiments show substantial improvements in coherence and\nrelevance after topic expansion, while synthetic data experiments indicate that\nin-class few-shot prompting significantly improves the quality and utility of\nsynthetic queries without compromising diversity. We also show that\nLLM-generated intent descriptions and keywords can effectively substitute for\nhuman-curated versions when used as context for synthetic query generation. Our\nresearch underscores the scalability and utility of LLM agents in topic\nmodeling and highlights the strategic use of synthetic utterances to enhance\ndataset variability and coverage for intent recognition. We present a\ncomprehensive and robust framework for online discovery and recognition of new\ncustomer intents in dynamic domains.\n","authors":["Aaron Rodrigues","Mahmood Hegazy","Azzam Naeem"],"pdf_url":"https://arxiv.org/pdf/2505.11176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00213v2","updated":"2025-05-16T12:15:17Z","published":"2025-01-31T23:05:52Z","title":"Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in\n  Transformers","summary":"  Transformers are challenging to optimize with SGD and typically require\nadaptive optimizers such as Adam. However, the reasons behind the superior\nperformance of Adam over SGD remain unclear. In this study, we investigate the\noptimization of transformers by focusing on gradient heterogeneity, defined as\nthe disparity in gradient norms among parameters. Our analysis shows that\ngradient heterogeneity hinders gradient-based optimization, including SGD,\nwhile sign-based optimization, a simplified variant of Adam, is less affected.\nWe further examine gradient heterogeneity in transformers and show that it is\ninfluenced by the placement of layer normalization. Experimental results from\nfine-tuning transformers in both NLP and vision domains validate our\ntheoretical analyses. This study provides insights into the optimization\nchallenges of transformers and offers guidance for designing future\noptimization algorithms. Code is available at\nhttps://github.com/tom4649/gradient-heterogeneity.\n","authors":["Akiyoshi Tomihari","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2502.00213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07776v2","updated":"2025-05-16T12:11:05Z","published":"2025-04-10T14:15:18Z","title":"SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified\n  Flow","summary":"  Recently, flow matching based speech synthesis has significantly enhanced the\nquality of synthesized speech while reducing the number of inference steps. In\nthis paper, we introduce SlimSpeech, a lightweight and efficient speech\nsynthesis system based on rectified flow. We have built upon the existing\nspeech synthesis method utilizing the rectified flow model, modifying its\nstructure to reduce parameters and serve as a teacher model. By refining the\nreflow operation, we directly derive a smaller model with a more straight\nsampling trajectory from the larger model, while utilizing distillation\ntechniques to further enhance the model performance. Experimental results\ndemonstrate that our proposed method, with significantly reduced model\nparameters, achieves comparable performance to larger models through one-step\nsampling.\n","authors":["Kaidi Wang","Wenhao Guan","Shenghui Lu","Jianglong Yao","Lin Li","Qingyang Hong"],"pdf_url":"https://arxiv.org/pdf/2504.07776v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11168v1","updated":"2025-05-16T12:10:01Z","published":"2025-05-16T12:10:01Z","title":"CheX-DS: Improving Chest X-ray Image Classification with Ensemble\n  Learning Based on DenseNet and Swin Transformer","summary":"  The automatic diagnosis of chest diseases is a popular and challenging task.\nMost current methods are based on convolutional neural networks (CNNs), which\nfocus on local features while neglecting global features. Recently,\nself-attention mechanisms have been introduced into the field of computer\nvision, demonstrating superior performance. Therefore, this paper proposes an\neffective model, CheX-DS, for classifying long-tail multi-label data in the\nmedical field of chest X-rays. The model is based on the excellent CNN model\nDenseNet for medical imaging and the newly popular Swin Transformer model,\nutilizing ensemble deep learning techniques to combine the two models and\nleverage the advantages of both CNNs and Transformers. The loss function of\nCheX-DS combines weighted binary cross-entropy loss with asymmetric loss,\neffectively addressing the issue of data imbalance. The NIH ChestX-ray14\ndataset is selected to evaluate the model's effectiveness. The model\noutperforms previous studies with an excellent average AUC score of 83.76\\%,\ndemonstrating its superior performance.\n","authors":["Xinran Li","Yu Liu","Xiujuan Xu","Xiaowei Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.11168v1.pdf","comment":"BIBM"},{"id":"http://arxiv.org/abs/2505.11166v1","updated":"2025-05-16T12:08:48Z","published":"2025-05-16T12:08:48Z","title":"SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long\n  Preference Optimization","summary":"  Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.\n","authors":["Huashan Sun","Shengyi Liao","Yansen Han","Yu Bai","Yang Gao","Cheng Fu","Weizhou Shen","Fanqi Wan","Ming Yan","Ji Zhang","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2505.11166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11165v1","updated":"2025-05-16T12:07:50Z","published":"2025-05-16T12:07:50Z","title":"Maximizing Asynchronicity in Event-based Neural Networks","summary":"  Event cameras deliver visual data with high temporal resolution, low latency,\nand minimal redundancy, yet their asynchronous, sparse sequential nature\nchallenges standard tensor-based machine learning (ML). While the recent\nasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by\nasynchronously encoding events into learned representations for ML pipelines,\nexisting A2S approaches often sacrifice representation expressivity and\ngeneralizability compared to dense, synchronous methods. This paper introduces\nEVA (EVent Asynchronous representation learning), a novel A2S framework to\ngenerate highly expressive and generalizable event-by-event representations.\nInspired by the analogy between events and language, EVA uniquely adapts\nadvances from language modeling in linear attention and self-supervised\nlearning for its construction. In demonstration, EVA outperforms prior A2S\nmethods on recognition tasks (DVS128-Gesture and N-Cars), and represents the\nfirst A2S framework to successfully master demanding detection tasks, achieving\na remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's\ntransformative potential for advancing real-time event-based vision\napplications.\n","authors":["Haiqing Hao","Nikola Zubić","Weihua He","Zhipeng Sui","Davide Scaramuzza","Wenhui Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11165v1.pdf","comment":"18 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2504.13986v2","updated":"2025-05-16T12:04:13Z","published":"2025-04-18T10:12:04Z","title":"Forgetting in short and heterogeneous sequences of belief revisions","summary":"  Forgetting a specific belief revision episode may not erase information\nbecause the other revisions may provide or entail the same information. Whether\nit does was proved coNP-hard for sequences of two arbitrary lexicographic\nrevisions or arbitrarily long lexicographic Horn revisions. A polynomial\nalgorithm is presented for the case of two lexicographic Horn revision.\nHeterogeneous sequences, including revisions other than lexicographic, were\nproved to belong in Delta2. Their previously proved coNP-hardness is enhanced\nto Dp-hardness.\n","authors":["Paolo Liberatore"],"pdf_url":"https://arxiv.org/pdf/2504.13986v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.15445,\n  arXiv:2305.09200"},{"id":"http://arxiv.org/abs/2406.11061v2","updated":"2025-05-16T12:01:46Z","published":"2024-06-16T20:26:38Z","title":"A-I-RAVEN and I-RAVEN-Mesh: Two New Benchmarks for Abstract Visual\n  Reasoning","summary":"  We study generalization and knowledge reuse capabilities of deep neural\nnetworks in the domain of abstract visual reasoning (AVR), employing Raven's\nProgressive Matrices (RPMs), a recognized benchmark task for assessing AVR\nabilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset\nare investigated. Firstly, inspired by generalization assessment capabilities\nof the PGM dataset and popularity of I-RAVEN, we introduce\nAttributeless-I-RAVEN (A-I-RAVEN), a benchmark with 10 generalization regimes\nthat allow to systematically test generalization of abstract rules applied to\nheld-out attributes at various levels of complexity (primary and extended\nregimes). In contrast to PGM, A-I-RAVEN features compositionality, a variety of\nfigure configurations, and does not require substantial computational\nresources. Secondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs\nwith a novel component structure comprising line-based patterns, facilitating\nassessment of progressive knowledge acquisition in transfer learning setting.\nWe evaluate 13 strong models from the AVR literature on the introduced\ndatasets, revealing their specific shortcomings in generalization and knowledge\ntransfer.\n","authors":["Mikołaj Małkiński","Jacek Mańdziuk"],"pdf_url":"https://arxiv.org/pdf/2406.11061v2.pdf","comment":"Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2504.16116v2","updated":"2025-05-16T12:00:59Z","published":"2025-04-18T16:40:39Z","title":"DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across\n  the Web3 Domain","summary":"  Large Language Models (LLMs) have achieved impressive performance in diverse\nnatural language processing tasks, but specialized domains such as Web3 present\nnew challenges and require more tailored evaluation. Despite the significant\nuser base and capital flows in Web3, encompassing smart contracts,\ndecentralized finance (DeFi), non-fungible tokens (NFTs), decentralized\nautonomous organizations (DAOs), on-chain governance, and novel\ntoken-economics, no comprehensive benchmark has systematically assessed LLM\nperformance in this domain. To address this gap, we introduce the DMind\nBenchmark, a holistic Web3-oriented evaluation suite covering nine critical\nsubfields: fundamental blockchain concepts, blockchain infrastructure, smart\ncontract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and\nsecurity vulnerabilities. Beyond multiple-choice questions, DMind Benchmark\nfeatures domain-specific tasks such as contract debugging and on-chain numeric\nreasoning, mirroring real-world scenarios. We evaluated 26 models, including\nChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable\nperformance gaps in specialized areas like token economics and\nsecurity-critical contract analysis. While some models excel in blockchain\ninfrastructure tasks, advanced subfields remain challenging. Our benchmark\ndataset and evaluation pipeline are open-sourced on\nhttps://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in\nHugging Face's trending dataset charts within a week of release.\n","authors":["Enhao Huang","Pengyu Sun","Zixin Lin","Alex Chen","Joey Ouyang","Hobert Wang","Dong Dong","Gang Zhao","James Yi","Frank Li","Ziang Ling","Lowes Yang"],"pdf_url":"https://arxiv.org/pdf/2504.16116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11157v1","updated":"2025-05-16T11:59:30Z","published":"2025-05-16T11:59:30Z","title":"Attention on the Sphere","summary":"  We introduce a generalized attention mechanism for spherical domains,\nenabling Transformer architectures to natively process data defined on the\ntwo-dimensional sphere - a critical need in fields such as atmospheric physics,\ncosmology, and robotics, where preserving spherical symmetries and topology is\nessential for physical accuracy. By integrating numerical quadrature weights\ninto the attention mechanism, we obtain a geometrically faithful spherical\nattention that is approximately rotationally equivariant, providing strong\ninductive biases and leading to better performance than Cartesian approaches.\nTo further enhance both scalability and model performance, we propose\nneighborhood attention on the sphere, which confines interactions to geodesic\nneighborhoods. This approach reduces computational complexity and introduces\nthe additional inductive bias for locality, while retaining the symmetry\nproperties of our method. We provide optimized CUDA kernels and\nmemory-efficient implementations to ensure practical applicability. The method\nis validated on three diverse tasks: simulating shallow water equations on the\nrotating sphere, spherical image segmentation, and spherical depth estimation.\nAcross all tasks, our spherical Transformers consistently outperform their\nplanar counterparts, highlighting the advantage of geometric priors for\nlearning on spherical domains.\n","authors":["Boris Bonev","Max Rietmann","Andrea Paris","Alberto Carpentieri","Thorsten Kurth"],"pdf_url":"https://arxiv.org/pdf/2505.11157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04893v4","updated":"2025-05-16T11:54:09Z","published":"2025-04-07T10:01:38Z","title":"SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models","summary":"  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper along with the code for evaluations at\nwww.bliss.berlin/research/scam.\n","authors":["Justus Westerhoff","Erblina Purelku","Jakob Hackstein","Jonas Loos","Leo Pinetzki","Lorenz Hufe"],"pdf_url":"https://arxiv.org/pdf/2504.04893v4.pdf","comment":"Accepted at CVPR 2025 Workshop EVAL-FoMo-2"},{"id":"http://arxiv.org/abs/2403.01840v2","updated":"2025-05-16T11:52:04Z","published":"2024-03-04T08:38:15Z","title":"FreeA: Human-object Interaction Detection using Free Annotation Labels","summary":"  Recent human-object interaction (HOI) detection methods depend on extensively\nannotated image datasets, which require a significant amount of manpower. In\nthis paper, we propose a novel self-adaptive, language-driven HOI detection\nmethod, termed FreeA. This method leverages the adaptability of the text-image\nmodel to generate latent HOI labels without requiring manual annotation.\nSpecifically, FreeA aligns image features of human-object pairs with HOI text\ntemplates and employs a knowledge-based masking technique to decrease\nimprobable interactions. Furthermore, FreeA implements a proposed method for\nmatching interaction correlations to increase the probability of actions\nassociated with a particular action, thereby improving the generated HOI\nlabels. Experiments on two benchmark datasets showcase that FreeA achieves\nstate-of-the-art performance among weakly supervised HOI competitors. Our\nproposal gets +\\textbf{13.29} (\\textbf{159\\%$\\uparrow$}) mAP and\n+\\textbf{17.30} (\\textbf{98\\%$\\uparrow$}) mAP than the newest ``Weakly''\nsupervised model, and +\\textbf{7.19} (\\textbf{28\\%$\\uparrow$}) mAP and\n+\\textbf{14.69} (\\textbf{34\\%$\\uparrow$}) mAP than the latest ``Weakly+''\nsupervised model, respectively, on HICO-DET and V-COCO datasets, more accurate\nin localizing and classifying the interactive actions. The source code will be\nmade public.\n","authors":["Qi Liu","Yuxiao Wang","Xinyu Jiang","Wolin Liang","Zhenao Wei","Yu Lei","Nan Zhuang","Weiying Xue"],"pdf_url":"https://arxiv.org/pdf/2403.01840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15191v2","updated":"2025-05-16T11:50:36Z","published":"2024-11-19T09:17:13Z","title":"Finding One's Bearings in the Hyperparameter Landscape of a Wide-Kernel\n  Convolutional Fault Detector","summary":"  State-of-the-art algorithms are reported to be almost perfect at\ndistinguishing the vibrations arising from healthy and damaged machine\nbearings, according to benchmark datasets at least. However, what about their\napplication to new data? In this paper, we confirm that neural networks for\nbearing fault detection can be crippled by incorrect hyperparameterisation, and\nalso that the correct hyperparameter settings can change when transitioning to\nnew data. The paper combines multiple methods to explain the behaviour of the\nhyperparameters of a wide-kernel convolutional neural network and how to set\nthem. Since guidance already exists for generic hyperparameters like minibatch\nsize, we focus on how to set architecture-specific hyperparameters such as the\nwidth of the convolutional kernels, a topic which might otherwise be obscure.\nWe reflect different data properties by fusing information from seven different\nbenchmark datasets, and our results show that the kernel size in the first\nlayer in particular is sensitive to changes in the data. Looking deeper, we use\nmanipulated copies of one dataset in an attempt to spot why the kernel size\nsometimes needs to change. The relevance of sampling rate is studied by using\ndifferent levels of resampling, and spectral content is studied by increasingly\nfiltering out high frequencies. We find that, contrary to speculation in\nearlier work, high-frequency noise is not the main reason why a wide kernel is\npreferable to a narrow kernel. Finally, we conclude by stating clear guidance\non how to set the hyperparameters of our neural network architecture to work\neffectively on new data.\n","authors":["Dan Hudson","Jurgen van den Hoogen","Martin Atzmueller"],"pdf_url":"https://arxiv.org/pdf/2411.15191v2.pdf","comment":"24 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2505.11146v1","updated":"2025-05-16T11:48:19Z","published":"2025-05-16T11:48:19Z","title":"X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic\n  Humanoid Imitation","summary":"  The ability to imitate realistic facial expressions is essential for humanoid\nrobots engaged in affective human-robot communication. However, the lack of\ndatasets containing diverse humanoid facial expressions with proper annotations\nhinders progress in realistic humanoid facial expression imitation. To address\nthese challenges, we introduce X2C (Anything to Control), a dataset featuring\nnuanced facial expressions for realistic humanoid imitation. With X2C, we\ncontribute: 1) a high-quality, high-diversity, large-scale dataset comprising\n100,000 (image, control value) pairs. Each image depicts a humanoid robot\ndisplaying a diverse range of facial expressions, annotated with 30 control\nvalues representing the ground-truth expression configuration; 2) X2CNet, a\nnovel human-to-humanoid facial expression imitation framework that learns the\ncorrespondence between nuanced humanoid expressions and their underlying\ncontrol values from X2C. It enables facial expression imitation in the wild for\ndifferent human performers, providing a baseline for the imitation task,\nshowcasing the potential value of our dataset; 3) real-world demonstrations on\na physical humanoid robot, highlighting its capability to advance realistic\nhumanoid facial expression imitation. Code and Data:\nhttps://lipzh5.github.io/X2CNet/\n","authors":["Peizhen Li","Longbing Cao","Xiao-Ming Wu","Runze Yang","Xiaohan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.11146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09724v2","updated":"2025-05-16T11:47:10Z","published":"2025-05-14T18:32:18Z","title":"An AI-Powered Research Assistant in the Lab: A Practical Guide for Text\n  Analysis Through Iterative Collaboration with LLMs","summary":"  Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.\n","authors":["Gino Carmona-Díaz","William Jiménez-Leal","María Alejandra Grisales","Chandra Sripada","Santiago Amaya","Michael Inzlicht","Juan Pablo Bermúdez"],"pdf_url":"https://arxiv.org/pdf/2505.09724v2.pdf","comment":"31 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.11141v1","updated":"2025-05-16T11:41:19Z","published":"2025-05-16T11:41:19Z","title":"Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in\n  MLLMs vs. Humans","summary":"  The goal of achieving Artificial General Intelligence (AGI) is to imitate\nhumans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have\ndemonstrated that large language models (LLMs) with human-like reasoning\ncapabilities exhibit exceptional performance and are being gradually integrated\ninto multimodal large language models (MLLMs). However, whether these models\npossess capabilities comparable to humans in handling reasoning tasks remains\nunclear at present. In this paper, we propose Human-Aligned Bench, a benchmark\nfor fine-grained alignment of multimodal reasoning with human performance.\nSpecifically, we collected 9,794 multimodal questions that solely rely on\ncontextual reasoning, including bilingual (Chinese and English) multimodal\nquestions and pure text-based questions, encompassing four question types:\nvisual reasoning, definition judgment, analogical reasoning, and logical\njudgment. More importantly, each question is accompanied by human success rates\nand options that humans are prone to choosing incorrectly. Extensive\nexperiments on the Human-Aligned Bench reveal notable differences between the\nperformance of current MLLMs in multimodal reasoning and human performance. The\nfindings on our benchmark provide insights into the development of the\nnext-generation models.\n","authors":["Yansheng Qiu","Li Xiao","Zhaopan Xu","Pengfei Zhou","Zheng Wang","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11140v1","updated":"2025-05-16T11:39:33Z","published":"2025-05-16T11:39:33Z","title":"Scaling Reasoning can Improve Factuality in Large Language Models","summary":"  Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.\n","authors":["Mike Zhang","Johannes Bjerva","Russa Biswas"],"pdf_url":"https://arxiv.org/pdf/2505.11140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09184v2","updated":"2025-05-16T11:38:19Z","published":"2025-04-12T11:44:47Z","title":"Parameterized Synthetic Text Generation with SimpleStories","summary":"  We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million samples each in English and Japanese. Through\nparameterizing prompts at multiple levels of abstraction, we achieve control\nover story characteristics at scale, inducing syntactic and semantic diversity.\nAblations on a newly trained model suite show improved sample efficiency and\nmodel interpretability compared to the TinyStories dataset. We open-source all\nconstituent parts of model creation, hoping to enable novel ways to study the\nend-to-end training process. As a byproduct, we move the frontier regarding the\nfewest-parameter language model that outputs grammatical natural language.\n","authors":["Lennart Finke","Chandan Sreedhara","Thomas Dooms","Mat Allen","Emerald Zhang","Juan Diego Rodriguez","Noa Nabeshima","Thomas Marshall","Dan Braun"],"pdf_url":"https://arxiv.org/pdf/2504.09184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14191v2","updated":"2025-05-16T11:37:51Z","published":"2025-04-19T05:35:45Z","title":"AI Idea Bench 2025: AI Research Idea Generation Benchmark","summary":"  Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.\n","authors":["Yansheng Qiu","Haoquan Zhang","Zhaopan Xu","Ming Li","Diping Song","Zheng Wang","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11136v1","updated":"2025-05-16T11:33:29Z","published":"2025-05-16T11:33:29Z","title":"Reinforcement Learning for AMR Charging Decisions: The Impact of Reward\n  and Action Space Design","summary":"  We propose a novel reinforcement learning (RL) design to optimize the\ncharging strategy for autonomous mobile robots in large-scale block stacking\nwarehouses. RL design involves a wide array of choices that can mostly only be\nevaluated through lengthy experimentation. Our study focuses on how different\nreward and action space configurations, ranging from flexible setups to more\nguided, domain-informed design configurations, affect the agent performance.\nUsing heuristic charging strategies as a baseline, we demonstrate the\nsuperiority of flexible, RL-based approaches in terms of service times.\nFurthermore, our findings highlight a trade-off: While more open-ended designs\nare able to discover well-performing strategies on their own, they may require\nlonger convergence times and are less stable, whereas guided configurations\nlead to a more stable learning process but display a more limited\ngeneralization potential. Our contributions are threefold. First, we extend\nSLAPStack, an open-source, RL-compatible simulation-framework to accommodate\ncharging strategies. Second, we introduce a novel RL design for tackling the\ncharging strategy problem. Finally, we introduce several novel adaptive\nbaseline heuristics and reproducibly evaluate the design using a Proximal\nPolicy Optimization agent and varying different design configurations, with a\nfocus on reward.\n","authors":["Janik Bischoff","Alexandru Rinciog","Anne Meyer"],"pdf_url":"https://arxiv.org/pdf/2505.11136v1.pdf","comment":"Under review LION19: The 19th Learning and Intelligent OptimizatioN\n  Conference"},{"id":"http://arxiv.org/abs/2505.11135v1","updated":"2025-05-16T11:32:29Z","published":"2025-05-16T11:32:29Z","title":"Scalability of Reinforcement Learning Methods for Dispatching in\n  Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real\n  Industry Datasets","summary":"  Benchmark datasets are crucial for evaluating approaches to scheduling or\ndispatching in the semiconductor industry during the development and deployment\nphases. However, commonly used benchmark datasets like the Minifab or SMT2020\nlack the complex details and constraints found in real-world scenarios. To\nmitigate this shortcoming, we compare open-source simulation models with a real\nindustry dataset to evaluate how optimization methods scale with different\nlevels of complexity. Specifically, we focus on Reinforcement Learning methods,\nperforming optimization based on policy-gradient and Evolution Strategies. Our\nresearch provides insights into the effectiveness of these optimization methods\nand their applicability to realistic semiconductor frontend fab simulations. We\nshow that our proposed Evolution Strategies-based method scales much better\nthan a comparable policy-gradient-based approach. Moreover, we identify the\nselection and combination of relevant bottleneck tools to control by the agent\nas crucial for an efficient optimization. For the generalization across\ndifferent loading scenarios and stochastic tool failure patterns, we achieve\nadvantages when utilizing a diverse training dataset. While the overall\napproach is computationally expensive, it manages to scale well with the number\nof CPU cores used for training. For the real industry dataset, we achieve an\nimprovement of up to 4% regarding tardiness and up to 1% regarding throughput.\nFor the less complex open-source models Minifab and SMT2020, we observe\ndouble-digit percentage improvement in tardiness and single digit percentage\nimprovement in throughput by use of Evolution Strategies.\n","authors":["Patrick Stöckermann","Henning Südfeld","Alessandro Immordino","Thomas Altenmüller","Marc Wegmann","Martin Gebser","Konstantin Schekotihin","Georg Seidel","Chew Wye Chan","Fei Fei Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11131v1","updated":"2025-05-16T11:25:50Z","published":"2025-05-16T11:25:50Z","title":"One Image is Worth a Thousand Words: A Usability Preservable Text-Image\n  Collaborative Erasing Framework","summary":"  Concept erasing has recently emerged as an effective paradigm to prevent\ntext-to-image diffusion models from generating visually undesirable or even\nharmful content. However, current removal methods heavily rely on manually\ncrafted text prompts, making it challenging to achieve a high erasure\n(efficacy) while minimizing the impact on other benign concepts (usability). In\nthis paper, we attribute the limitations to the inherent gap between the text\nand image modalities, which makes it hard to transfer the intricately entangled\nconcept knowledge from text prompts to the image generation process. To address\nthis, we propose a novel solution by directly integrating visual supervision\ninto the erasure process, introducing the first text-image Collaborative\nConcept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the\nconcept jointly by text prompts and the corresponding undesirable images\ninduced by the prompts, and then reduces the generating probability of the\ntarget concept through negative guidance. This approach effectively bypasses\nthe knowledge gap between text and image, significantly enhancing erasure\nefficacy. Additionally, we design a text-guided image concept refinement\nstrategy that directs the model to focus on visual features most relevant to\nthe specified text concept, minimizing disruption to other benign concepts.\nFinally, comprehensive experiments suggest that Co-Erasing outperforms\nstate-of-the-art erasure approaches significantly with a better trade-off\nbetween efficacy and usability. Codes are available at\nhttps://github.com/Ferry-Li/Co-Erasing.\n","authors":["Feiran Li","Qianqian Xu","Shilong Bao","Zhiyong Yang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2505.11131v1.pdf","comment":"This paper has been accepeted to ICML 2025. Not Final Version"},{"id":"http://arxiv.org/abs/2505.11129v1","updated":"2025-05-16T11:23:30Z","published":"2025-05-16T11:23:30Z","title":"PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video","summary":"  Recent advances in self-supervised learning (SSL) have revolutionized\ncomputer vision through innovative architectures and learning objectives, yet\nthey have not fully leveraged insights from biological visual processing\nsystems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is\nbased on a ResNet backbone and operates on static image inputs with strong\naugmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based\narchitecture that processes temporal visual input (that is, sequences of\nimages) without relying on strong augmentation. Our model leverages variational\ninference to learn robust visual representations from continuous input streams,\nsimilar to human visual processing. Through extensive experimentation, we\ndemonstrate that PhiNet v2 achieves competitive performance compared to\nstate-of-the-art vision foundation models, while maintaining the ability to\nlearn from sequential input without strong data augmentation. This work\nrepresents a significant step toward more biologically plausible computer\nvision systems that process visual information in a manner more closely aligned\nwith human cognitive processes.\n","authors":["Makoto Yamada","Kian Ming A. Chai","Ayoub Rhim","Satoki Ishikawa","Mohammad Sabokrou","Yao-Hung Hubert Tsai"],"pdf_url":"https://arxiv.org/pdf/2505.11129v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.14650"},{"id":"http://arxiv.org/abs/2505.11123v1","updated":"2025-05-16T11:14:22Z","published":"2025-05-16T11:14:22Z","title":"Conditioning Matters: Training Diffusion Policies is Faster Than You\n  Think","summary":"  Diffusion policies have emerged as a mainstream paradigm for building\nvision-language-action (VLA) models. Although they demonstrate strong robot\ncontrol capabilities, their training efficiency remains suboptimal. In this\nwork, we identify a fundamental challenge in conditional diffusion policy\ntraining: when generative conditions are hard to distinguish, the training\nobjective degenerates into modeling the marginal action distribution, a\nphenomenon we term loss collapse. To overcome this, we propose Cocos, a simple\nyet general solution that modifies the source distribution in the conditional\nflow matching to be condition-dependent. By anchoring the source distribution\naround semantics extracted from condition inputs, Cocos encourages stronger\ncondition integration and prevents the loss collapse. We provide theoretical\njustification and extensive empirical results across simulation and real-world\nbenchmarks. Our method achieves faster convergence and higher success rates\nthan existing approaches, matching the performance of large-scale pre-trained\nVLAs using significantly fewer gradient steps and parameters. Cocos is\nlightweight, easy to implement, and compatible with diverse policy\narchitectures, offering a general-purpose improvement to diffusion policy\ntraining.\n","authors":["Zibin Dong","Yicheng Liu","Yinchuan Li","Hang Zhao","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2505.11123v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2505.10105"},{"id":"http://arxiv.org/abs/2505.11122v1","updated":"2025-05-16T11:14:17Z","published":"2025-05-16T11:14:17Z","title":"Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic\n  Factor Mining","summary":"  Alpha factor mining is pivotal in quantitative investment for identifying\npredictive signals from complex financial data. While traditional formulaic\nalpha mining relies on human expertise, contemporary automated methods, such as\nthose based on genetic programming or reinforcement learning, often suffer from\nsearch inefficiency or yield poorly interpretable alpha factors. This paper\nintroduces a novel framework that integrates Large Language Models (LLMs) with\nMonte Carlo Tree Search (MCTS) to overcome these limitations. Our approach\nleverages the LLM's instruction-following and reasoning capability to\niteratively generate and refine symbolic alpha formulas within an MCTS-driven\nexploration. A key innovation is the guidance of MCTS exploration by rich,\nquantitative feedback from financial backtesting of each candidate factor,\nenabling efficient navigation of the vast search space. Furthermore, a frequent\nsubtree avoidance mechanism is introduced to bolster search efficiency and\nalpha factor performance. Experimental results on real-world stock market data\ndemonstrate that our LLM-based framework outperforms existing methods by mining\nalphas with superior predictive accuracy, trading performance, and improved\ninterpretability, while offering a more efficient solution for formulaic alpha\nmining.\n","authors":["Yu Shi","Yitong Duan","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2505.11122v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2411.06581v2","updated":"2025-05-16T11:03:52Z","published":"2024-11-10T19:59:54Z","title":"HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with\n  Quantization","summary":"  Federated fine-tuning of pre-trained Large Language Models (LLMs) enables\ntask-specific adaptation across diverse datasets while preserving privacy.\nHowever, challenges such as high computational and memory demands,\nheterogeneous client resources, bandwidth constraints, and ineffective global\naggregation hinder its efficiency. To address these issues, we propose HAFLQ\n(Heterogeneous Adaptive Federated Low-Rank Adaptation Fine-tuned LLM with\nQuantization), a novel framework for efficient and scalable federated\nfine-tuning of LLMs in heterogeneous environments. To reduce memory and\ncomputation demands, we propose a salience-driven adaptive LLM quantization\nframework that evaluates the importance of transformer blocks using a salience\nmetric and applies adaptive block-wise quantization accordingly. To handle\nheterogeneous computational capabilities, we propose an importance-based\nparameter truncation and freezing scheme. To address communication bottlenecks,\nwe propose an importance-aware bandwidth-adaptive quantization method, which\ndynamically adjusts parameter precision based on importance and bandwidth\nconstraints. To improve global model aggregation, we propose an adaptive rank-1\nmatrix-level aggregation strategy, which prevents information dilution and\naccelerates convergence by aggregating only updated rank-1 matrices from\nclients. Experimental results on the text classification task demonstrate that\nHAFLQ reduces memory usage by 31%, lowers communication cost by 49%, improves\naccuracy by 50%, and achieves faster convergence compared to the baseline\nmethod.\n","authors":["Yang Su","Na Yan","Yansha Deng","Mischa Dohler","Robert Schober"],"pdf_url":"https://arxiv.org/pdf/2411.06581v2.pdf","comment":"This is an extended journal version based on our previous conference\n  paper accepted at the 2025 IEEE International Conference on Communications\n  (ICC), with additional sections and new results"},{"id":"http://arxiv.org/abs/2505.11119v1","updated":"2025-05-16T11:02:55Z","published":"2025-05-16T11:02:55Z","title":"Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral\n  Changes Approach","summary":"  Timely prediction of students at high risk of dropout is critical for early\nintervention and improving educational outcomes. However, in offline\neducational settings, poor data quality, limited scale, and high heterogeneity\noften hinder the application of advanced machine learning models. Furthermore,\nwhile educational theories provide valuable insights into dropout phenomena,\nthe lack of quantifiable metrics for key indicators limits their use in\ndata-driven modeling. Through data analysis and a review of educational\nliterature, we identified abrupt changes in student behavior as key early\nsignals of dropout risk. To address this, we propose the Dual-Modal Multiscale\nSliding Window (DMSW) Model, which integrates academic performance and\nbehavioral data to dynamically capture behavior patterns using minimal data.\nThe DMSW model improves prediction accuracy by 15% compared to traditional\nmethods, enabling educators to identify high-risk students earlier, provide\ntimely support, and foster a more inclusive learning environment. Our analysis\nhighlights key behavior patterns, offering practical insights for preventive\nstrategies and tailored support. These findings bridge the gap between theory\nand practice in dropout prediction, giving educators an innovative tool to\nenhance student retention and outcomes.\n","authors":["Jiabei Cheng","Zhen-Qun Yang","Jiannong Cao","Yu Yang","Xinzhe Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.11119v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.16594v2","updated":"2025-05-16T10:53:01Z","published":"2024-07-23T15:53:17Z","title":"Flexible Generation of Preference Data for Recommendation Analysis","summary":"  Simulating a recommendation system in a controlled environment, to identify\nspecific behaviors and user preferences, requires highly flexible synthetic\ndata generation models capable of mimicking the patterns and trends of real\ndatasets. In this context, we propose HYDRA, a novel preferences data\ngeneration model driven by three main factors: user-item interaction level,\nitem popularity, and user engagement level. The key innovations of the proposed\nprocess include the ability to generate user communities characterized by\nsimilar item adoptions, reflecting real-world social influences and trends.\nAdditionally, HYDRA considers item popularity and user engagement as mixtures\nof different probability distributions, allowing for a more realistic\nsimulation of diverse scenarios. This approach enhances the model's capacity to\nsimulate a wide range of real-world cases, capturing the complexity and\nvariability found in actual user behavior. We demonstrate the effectiveness of\nHYDRA through extensive experiments on well-known benchmark datasets. The\nresults highlight its capability to replicate real-world data patterns,\noffering valuable insights for developing and testing recommendation systems in\na controlled and realistic manner. The code used to perform the experiments is\npublicly available at https://github.com/SimoneMungari/HYDRA.\n","authors":["Simone Mungari","Erica Coppolillo","Ettore Ritacco","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2407.16594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11111v1","updated":"2025-05-16T10:48:19Z","published":"2025-05-16T10:48:19Z","title":"FairSHAP: Preprocessing for Fairness Through Attribution-Based Data\n  Augmentation","summary":"  Ensuring fairness in machine learning models is critical, particularly in\nhigh-stakes domains where biased decisions can lead to serious societal\nconsequences. Existing preprocessing approaches generally lack transparent\nmechanisms for identifying which features or instances are responsible for\nunfairness. This obscures the rationale behind data modifications. We introduce\nFairSHAP, a novel pre-processing framework that leverages Shapley value\nattribution to improve both individual and group fairness. FairSHAP identifies\nfairness-critical instances in the training data using an interpretable measure\nof feature importance, and systematically modifies them through instance-level\nmatching across sensitive groups. This process reduces discriminative risk - an\nindividual fairness metric - while preserving data integrity and model\naccuracy. We demonstrate that FairSHAP significantly improves demographic\nparity and equality of opportunity across diverse tabular datasets, achieving\nfairness gains with minimal data perturbation and, in some cases, improved\npredictive performance. As a model-agnostic and transparent method, FairSHAP\nintegrates seamlessly into existing machine learning pipelines and provides\nactionable insights into the sources of bias.Our code is on\nhttps://github.com/youlei202/FairSHAP.\n","authors":["Lin Zhu","Yijun Bian","Lei You"],"pdf_url":"https://arxiv.org/pdf/2505.11111v1.pdf","comment":"3 figures, 15 pages"},{"id":"http://arxiv.org/abs/2505.11109v1","updated":"2025-05-16T10:42:30Z","published":"2025-05-16T10:42:30Z","title":"MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark","summary":"  We present the first large-scale open-set benchmark for multilingual\naudio-video deepfake detection. Our dataset comprises over 250 hours of real\nand fake videos across eight languages, with 60% of data being generated. For\neach language, the fake videos are generated with seven distinct deepfake\ngeneration models, selected based on the quality of the generated content. We\norganize the training, validation and test splits such that only a subset of\nthe chosen generative models and languages are available during training, thus\ncreating several challenging open-set evaluation setups. We perform experiments\nwith various pre-trained and fine-tuned deepfake detectors proposed in recent\nliterature. Our results show that state-of-the-art detectors are not currently\nable to maintain their performance levels when tested in our open-set\nscenarios. We publicly release our data and code at:\nhttps://huggingface.co/datasets/unibuc-cs/MAVOS-DD.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Marius Popescu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2505.11109v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2505.11108v1","updated":"2025-05-16T10:40:44Z","published":"2025-05-16T10:40:44Z","title":"PARSEC: Preference Adaptation for Robotic Object Rearrangement from\n  Scene Context","summary":"  Object rearrangement is a key task for household robots requiring\npersonalization without explicit instructions, meaningful object placement in\nenvironments occupied with objects, and generalization to unseen objects and\nnew environments. To facilitate research addressing these challenges, we\nintroduce PARSEC, an object rearrangement benchmark for learning user\norganizational preferences from observed scene context to place objects in a\npartially arranged environment. PARSEC is built upon a novel dataset of 110K\nrearrangement examples crowdsourced from 72 users, featuring 93 object\ncategories and 15 environments. We also propose ContextSortLM, an LLM-based\nrearrangement model that places objects in partially arranged environments by\nadapting to user preferences from prior and current scene context while\naccounting for multiple valid placements. We evaluate ContextSortLM and\nexisting personalized rearrangement approaches on the PARSEC benchmark and\ncomplement these findings with a crowdsourced evaluation of 108 online raters\nranking model predictions based on alignment with user preferences. Our results\nindicate that personalized rearrangement models leveraging multiple scene\ncontext sources perform better than models relying on a single context source.\nMoreover, ContextSortLM outperforms other models in placing objects to\nreplicate the target user's arrangement and ranks among the top two in all\nthree environment categories, as rated by online evaluators. Importantly, our\nevaluation highlights challenges associated with modeling environment semantics\nacross different environment categories and provides recommendations for future\nwork.\n","authors":["Kartik Ramachandruni","Sonia Chernova"],"pdf_url":"https://arxiv.org/pdf/2505.11108v1.pdf","comment":"Under review at ROMAN 2025"},{"id":"http://arxiv.org/abs/2505.11107v1","updated":"2025-05-16T10:40:35Z","published":"2025-05-16T10:40:35Z","title":"Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity","summary":"  Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.\n","authors":["Chan-Jan Hsu","Davide Buffelli","Jamie McGowan","Feng-Ting Liao","Yi-Chang Chen","Sattar Vakili","Da-shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2505.11107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11106v1","updated":"2025-05-16T10:39:46Z","published":"2025-05-16T10:39:46Z","title":"Inferring the Most Similar Variable-length Subsequences between\n  Multidimensional Time Series","summary":"  Finding the most similar subsequences between two multidimensional time\nseries has many applications: e.g. capturing dependency in stock market or\ndiscovering coordinated movement of baboons. Considering one pattern occurring\nin one time series, we might be wondering whether the same pattern occurs in\nanother time series with some distortion that might have a different length.\nNevertheless, to the best of our knowledge, there is no efficient framework\nthat deals with this problem yet. In this work, we propose an algorithm that\nprovides the exact solution of finding the most similar multidimensional\nsubsequences between time series where there is a difference in length both\nbetween time series and between subsequences. The algorithm is built based on\ntheoretical guarantee of correctness and efficiency. The result in simulation\ndatasets illustrated that our approach not just only provided correct solution,\nbut it also utilized running time only quarter of time compared against the\nbaseline approaches. In real-world datasets, it extracted the most similar\nsubsequences even faster (up to 20 times faster against baseline methods) and\nprovided insights regarding the situation in stock market and following\nrelations of multidimensional time series of baboon movement. Our approach can\nbe used for any time series. The code and datasets of this work are provided\nfor the public use.\n","authors":["Thanadej Rattanakornphan","Piyanon Charoenpoonpanich","Chainarong Amornbunchornvej"],"pdf_url":"https://arxiv.org/pdf/2505.11106v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.06576v2","updated":"2025-05-16T10:39:40Z","published":"2025-05-10T09:26:22Z","title":"Two-Stage Random Alternation Framework for One-Shot Pansharpening","summary":"  Deep learning has substantially advanced pansharpening, achieving impressive\nfusion quality. However, a prevalent limitation is that conventional deep\nlearning models, which typically rely on training datasets, often exhibit\nsuboptimal generalization to unseen real-world image pairs. This restricts\ntheir practical utility when faced with real-world scenarios not included in\nthe training datasets. To overcome this, we introduce a two-stage random\nalternating framework (TRA-PAN) that performs instance-specific optimization\nfor any given Multispectral(MS)/Panchromatic(PAN) pair, ensuring robust and\nhigh-quality fusion. TRA-PAN effectively integrates strong supervision\nconstraints from reduced-resolution images with the physical characteristics of\nthe full-resolution images. The first stage introduces a pre-training\nprocedure, which includes Degradation-Aware Modeling (DAM) to capture spectral\ndegradation mappings, alongside a warm-up procedure designed to reduce training\ntime and mitigate the adverse effects of reduced-resolution data. The second\nstage employs Random Alternation Optimization (RAO), randomly alternating\nbetween reduced- and full-resolution images to refine the fusion model\nprogressively. This adaptive, per-instance optimization strategy, operating in\na one-shot manner for each MS/PAN pair, yields superior high-resolution\nmultispectral images. Experimental results demonstrate that TRA-PAN outperforms\nstate-of-the-art (SOTA) methods in quantitative metrics and visual quality in\nreal-world scenarios, underscoring its enhanced practical applicability and\nrobustness.\n","authors":["Haorui Chen","Zeyu Ren","Jiaxuan Ren","Ran Ran","Jinliang Shao","Jie Huang","Liangjian Deng"],"pdf_url":"https://arxiv.org/pdf/2505.06576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06830v2","updated":"2025-05-16T10:37:28Z","published":"2025-02-05T15:37:21Z","title":"OrderFusion: Encoding Orderbook for End-to-End Probabilistic Intraday\n  Electricity Price Prediction","summary":"  Accurate and reliable probabilistic prediction of intraday electricity prices\nis essential to manage market uncertainties and support robust trading\nstrategies. However, current methods rely heavily on domain feature extraction\nand fail to capture the dynamics between buy and sell orders, limiting the\nability to form rich representations of the orderbook. Furthermore, these\nmethods often require training separate models for different quantiles and\nintroduce additional procedures-such as post-hoc quantile sorting or loss-based\npenalties-to address the quantile crossing issue, where predicted upper\nquantiles fall below lower ones. These steps are either decoupled from model\ntraining or introduce extra tuning complexity. To address these challenges, we\npropose an encoding method called OrderFusion and design a hierarchical\nmulti-quantile head. OrderFusion encodes the orderbook into a 2.5D\nrepresentation and employs a tailored jump cross-attention to model buy-sell\ndynamics without the need for domain feature extraction. The multi-quantile\nhead anchors on the median quantile and hierarchically estimates other\nquantiles through constrained residuals, ensuring monotonicity without\npost-processing or additional tuning. We conduct extensive experiments and\nablation studies on three key price indices (ID1, ID2, and ID3) using three\nyears of orderbook data from the German and Austrian markets. The results\ndemonstrate that our approach provides an accurate, reliable, and unified\nend-to-end framework for probabilistic intraday price prediction.\n","authors":["Runyao Yu","Yuchen Tao","Fabian Leimgruber","Tara Esterl","Jochen L. Cremer"],"pdf_url":"https://arxiv.org/pdf/2502.06830v2.pdf","comment":"20 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.11100v1","updated":"2025-05-16T10:31:10Z","published":"2025-05-16T10:31:10Z","title":"Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent\n  Generalizable Behaviors","summary":"  Population-population generalization is a challenging problem in multi-agent\nreinforcement learning (MARL), particularly when agents encounter unseen\nco-players. However, existing self-play-based methods are constrained by the\nlimitation of inside-space generalization. In this study, we propose\nBidirectional Distillation (BiDist), a novel mixed-play framework, to overcome\nthis limitation in MARL. BiDist leverages knowledge distillation in two\nalternating directions: forward distillation, which emulates the historical\npolicies' space and creates an implicit self-play, and reverse distillation,\nwhich systematically drives agents towards novel distributions outside the\nknown policy space in a non-self-play manner. In addition, BiDist operates as a\nconcise and efficient solution without the need for the complex and costly\nstorage of past policies. We provide both theoretical analysis and empirical\nevidence to support BiDist's effectiveness. Our results highlight its\nremarkable generalization ability across a variety of cooperative, competitive,\nand social dilemma tasks, and reveal that BiDist significantly diversifies the\npolicy distribution space. We also present comprehensive ablation studies to\nreinforce BiDist's effectiveness and key success factors. Source codes are\navailable in the supplementary material.\n","authors":["Lang Feng","Jiahao Lin","Dong Xing","Li Zhang","De Ma","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2505.11100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11086v1","updated":"2025-05-16T10:17:53Z","published":"2025-05-16T10:17:53Z","title":"Analysis of Customer Journeys Using Prototype Detection and\n  Counterfactual Explanations for Sequential Data","summary":"  Recently, the proliferation of omni-channel platforms has attracted interest\nin customer journeys, particularly regarding their role in developing marketing\nstrategies. However, few efforts have been taken to quantitatively study or\ncomprehensively analyze them owing to the sequential nature of their data and\nthe complexity involved in analysis. In this study, we propose a novel approach\ncomprising three steps for analyzing customer journeys. First, the distance\nbetween sequential data is defined and used to identify and visualize\nrepresentative sequences. Second, the likelihood of purchase is predicted based\non this distance. Third, if a sequence suggests no purchase, counterfactual\nsequences are recommended to increase the probability of a purchase using a\nproposed method, which extracts counterfactual explanations for sequential\ndata. A survey was conducted, and the data were analyzed; the results revealed\nthat typical sequences could be extracted, and the parts of those sequences\nimportant for purchase could be detected. We believe that the proposed approach\ncan support improvements in various marketing activities.\n","authors":["Keita Kinjo"],"pdf_url":"https://arxiv.org/pdf/2505.11086v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.09521v3","updated":"2025-05-16T10:17:33Z","published":"2024-12-12T18:07:23Z","title":"Efficient and Comprehensive Feature Extraction in Large Vision-Language\n  Model for Pathology Analysis","summary":"  Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\nexisting large vision-language models (LVLMs) are limited by input resolution\nconstraints, hindering their efficiency and accuracy in pathology image\nanalysis. To overcome these issues, we propose two innovative strategies: the\nmixed task-guided feature enhancement, which directs feature extraction toward\nlesion-related details across scales, and the prompt-guided detail feature\ncompletion, which integrates coarse- and fine-grained features from WSI based\non specific prompts without compromising inference speed. Leveraging a\ncomprehensive dataset of 490K samples from diverse pathology tasks, we trained\nthe pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate\nthat this model significantly outperforms existing methods in diagnostic\naccuracy and efficiency, providing an interactive, clinically aligned approach\nfor auxiliary diagnosis in a wide range of pathology applications.\n","authors":["Shengxuming Zhang","Weihan Li","Tianhong Gao","Jiacong Hu","Haoming Luo","Xiuming Zhang","Jing Zhang","Mingli Song","Zunlei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.09521v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11085v1","updated":"2025-05-16T10:14:57Z","published":"2025-05-16T10:14:57Z","title":"A Fast Kernel-based Conditional Independence test with Application to\n  Causal Discovery","summary":"  Kernel-based conditional independence (KCI) testing is a powerful\nnonparametric method commonly employed in causal discovery tasks. Despite its\nflexibility and statistical reliability, cubic computational complexity limits\nits application to large datasets. To address this computational bottleneck, we\npropose \\textit{FastKCI}, a scalable and parallelizable kernel-based\nconditional independence test that utilizes a mixture-of-experts approach\ninspired by embarrassingly parallel inference techniques for Gaussian\nprocesses. By partitioning the dataset based on a Gaussian mixture model over\nthe conditioning variables, FastKCI conducts local KCI tests in parallel,\naggregating the results using an importance-weighted sampling scheme.\nExperiments on synthetic datasets and benchmarks on real-world production data\nvalidate that FastKCI maintains the statistical power of the original KCI test\nwhile achieving substantial computational speedups. FastKCI thus represents a\npractical and efficient solution for conditional independence testing in causal\ninference on large-scale data.\n","authors":["Oliver Schacht","Biwei Huang"],"pdf_url":"https://arxiv.org/pdf/2505.11085v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.04072v2","updated":"2025-05-16T10:14:51Z","published":"2025-04-05T06:09:32Z","title":"Among Us: A Sandbox for Measuring and Detecting Agentic Deception","summary":"  Prior studies on deception in language-based AI agents typically assess\nwhether the agent produces a false statement about a topic, or makes a binary\nchoice prompted by a goal, rather than allowing open-ended deceptive behavior\nto emerge in pursuit of a longer-term goal. To fix this, we introduce\n$\\textit{Among Us}$, a sandbox social deception game where LLM-agents exhibit\nlong-term, open-ended deception as a consequence of the game objectives. While\nmost benchmarks saturate quickly, $\\textit{Among Us}$ can be expected to last\nmuch longer, because it is a multi-player game far from equilibrium. Using the\nsandbox, we evaluate $18$ proprietary and open-weight LLMs and uncover a\ngeneral trend: models trained with RL are comparatively much better at\nproducing deception than detecting it. We evaluate the effectiveness of methods\nto detect lying and deception: logistic regression on the activations and\nsparse autoencoders (SAEs). We find that probes trained on a dataset of\n``pretend you're a dishonest model: $\\dots$'' generalize extremely well\nout-of-distribution, consistently obtaining AUROCs over 95% even when evaluated\njust on the deceptive statement, without the chain of thought. We also find two\nSAE features that work well at deception detection but are unable to steer the\nmodel to lie less. We hope our open-sourced sandbox, game logs, and probes\nserve to anticipate and mitigate deceptive behavior and capabilities in\nlanguage-based agents.\n","authors":["Satvik Golechha","Adrià Garriga-Alonso"],"pdf_url":"https://arxiv.org/pdf/2504.04072v2.pdf","comment":"21 pages, preprint"},{"id":"http://arxiv.org/abs/2505.11083v1","updated":"2025-05-16T10:14:10Z","published":"2025-05-16T10:14:10Z","title":"Fault Diagnosis across Heterogeneous Domains via Self-Adaptive\n  Temporal-Spatial Attention and Sample Generation","summary":"  Deep learning methods have shown promising performance in fault diagnosis for\nmultimode process. Most existing studies assume that the collected health state\ncategories from different operating modes are identical. However, in real\nindustrial scenarios, these categories typically exhibit only partial overlap.\nThe incompleteness of the available data and the large distributional\ndifferences between the operating modes pose a significant challenge to\nexisting fault diagnosis methods. To address this problem, a novel fault\ndiagnosis model named self-adaptive temporal-spatial attention network\n(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy\ncategory data to generate multimode samples. To enrich the diversity of the\nfault data, interpolation is performed between healthy and fault samples.\nSubsequently, the fault diagnosis model is trained using real and generated\ndata. The self-adaptive instance normalization is established to suppress\nirrelevant information while retaining essential statistical features for\ndiagnosis. In addition, a temporal-spatial attention mechanism is constructed\nto focus on the key features, thus enhancing the generalization ability of the\nmodel. The extensive experiments demonstrate that the proposed model\nsignificantly outperforms the state-of-the-art methods. The code will be\navailable on Github at https://github.com/GuangqiangLi/TSA-SAN.\n","authors":["Guangqiang Li","M. Amine Atoui","Xiangshun Li"],"pdf_url":"https://arxiv.org/pdf/2505.11083v1.pdf","comment":"31 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.11080v1","updated":"2025-05-16T10:11:43Z","published":"2025-05-16T10:11:43Z","title":"BLEUBERI: BLEU is a surprisingly effective reward for instruction\n  following","summary":"  Reward models are central to aligning LLMs with human preferences, but they\nare costly to train, requiring large-scale human-labeled preference data and\npowerful pretrained LLM backbones. Meanwhile, the increasing availability of\nhigh-quality synthetic instruction-following datasets raises the question: can\nsimpler, reference-based metrics serve as viable alternatives to reward models\nduring RL-based alignment? In this paper, we show first that BLEU, a basic\nstring-matching metric, surprisingly matches strong reward models in agreement\nwith human preferences on general instruction-following datasets. Based on this\ninsight, we develop BLEUBERI, a method that first identifies challenging\ninstructions and then applies Group Relative Policy Optimization (GRPO) using\nBLEU directly as the reward function. We demonstrate that BLEUBERI-trained\nmodels are competitive with models trained via reward model-guided RL across\nfour challenging instruction-following benchmarks and three different base\nlanguage models. A human evaluation further supports that the quality of\nBLEUBERI model outputs is on par with those from reward model-aligned models.\nMoreover, BLEUBERI models generate outputs that are more factually grounded\nthan competing methods. Overall, we show that given access to high-quality\nreference outputs (easily obtained via existing instruction-following datasets\nor synthetic data generation), string matching-based metrics are cheap yet\neffective proxies for reward models during alignment. We release our code and\ndata at https://github.com/lilakk/BLEUBERI.\n","authors":["Yapei Chang","Yekyung Kim","Michael Krumdick","Amir Zadeh","Chuan Li","Chris Tanner","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2505.11080v1.pdf","comment":"28 pages, 11 figures, 15 tables"},{"id":"http://arxiv.org/abs/2410.00031v2","updated":"2025-05-16T10:05:18Z","published":"2024-09-19T20:10:40Z","title":"Strategic Collusion of LLM Agents: Market Division in Multi-Commodity\n  Competitions","summary":"  Machine-learning technologies are seeing increased deployment in real-world\nmarket scenarios. In this work, we explore the strategic behaviors of large\nlanguage models (LLMs) when deployed as autonomous agents in multi-commodity\nmarkets, specifically within Cournot competition frameworks. We examine whether\nLLMs can independently engage in anti-competitive practices such as collusion\nor, more specifically, market division. Our findings demonstrate that LLMs can\neffectively monopolize specific commodities by dynamically adjusting their\npricing and resource allocation strategies, thereby maximizing profitability\nwithout direct human input or explicit collusion commands. These results pose\nunique challenges and opportunities for businesses looking to integrate AI into\nstrategic roles and for regulatory bodies tasked with maintaining fair and\ncompetitive markets. The study provides a foundation for further exploration\ninto the ramifications of deferring high-stakes decisions to LLM-based agents.\n","authors":["Ryan Y. Lin","Siddhartha Ojha","Kevin Cai","Maxwell F. Chen"],"pdf_url":"https://arxiv.org/pdf/2410.00031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11070v1","updated":"2025-05-16T10:04:57Z","published":"2025-05-16T10:04:57Z","title":"Towards Self-Improvement of Diffusion Models via Group Preference\n  Optimization","summary":"  Aligning text-to-image (T2I) diffusion models with Direct Preference\nOptimization (DPO) has shown notable improvements in generation quality.\nHowever, applying DPO to T2I faces two challenges: the sensitivity of DPO to\npreference pairs and the labor-intensive process of collecting and annotating\nhigh-quality data. In this work, we demonstrate that preference pairs with\nmarginal differences can degrade DPO performance. Since DPO relies exclusively\non relative ranking while disregarding the absolute difference of pairs, it may\nmisclassify losing samples as wins, or vice versa. We empirically show that\nextending the DPO from pairwise to groupwise and incorporating reward\nstandardization for reweighting leads to performance gains without explicit\ndata selection. Furthermore, we propose Group Preference Optimization (GPO), an\neffective self-improvement method that enhances performance by leveraging the\nmodel's own capabilities without requiring external data. Extensive experiments\ndemonstrate that GPO is effective across various diffusion models and tasks.\nSpecifically, combining with widely used computer vision models, such as YOLO\nand OCR, the GPO improves the accurate counting and text rendering capabilities\nof the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a\nplug-and-play method, no extra overhead is introduced during inference.\n","authors":["Renjie Chen","Wenfeng Lin","Yichen Zhang","Jiangchuan Wei","Boyuan Liu","Chao Feng","Jiao Ran","Mingyu Guo"],"pdf_url":"https://arxiv.org/pdf/2505.11070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11067v1","updated":"2025-05-16T10:02:32Z","published":"2025-05-16T10:02:32Z","title":"Assessing the Performance of Analog Training for Transfer Learning","summary":"  Analog in-memory computing is a next-generation computing paradigm that\npromises fast, parallel, and energy-efficient deep learning training and\ntransfer learning (TL). However, achieving this promise has remained elusive\ndue to a lack of suitable training algorithms. Analog memory devices exhibit\nasymmetric and non-linear switching behavior in addition to device-to-device\nvariation, meaning that most, if not all, of the current off-the-shelf training\nalgorithms cannot achieve good training outcomes. Also, recently introduced\nalgorithms have enjoyed limited attention, as they require bi-directionally\nswitching devices of unrealistically high symmetry and precision and are highly\nsensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which\nleverages the chopped technique to address many of the challenges mentioned\nabove. In this paper, we assess the performance of the c-TTv2 algorithm for\nanalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also\ninvestigate the robustness of our algorithm to changes in some device\nspecifications, including weight transfer noise, symmetry point skew, and\nsymmetry point variability\n","authors":["Omobayode Fagbohungbe","Corey Lammie","Malte J. Rasch","Takashi Ando","Tayfun Gokmen","Vijay Narayanan"],"pdf_url":"https://arxiv.org/pdf/2505.11067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11066v1","updated":"2025-05-16T10:02:22Z","published":"2025-05-16T10:02:22Z","title":"A Multi-modal Fusion Network for Terrain Perception Based on\n  Illumination Aware","summary":"  Road terrains play a crucial role in ensuring the driving safety of\nautonomous vehicles (AVs). However, existing sensors of AVs, including cameras\nand Lidars, are susceptible to variations in lighting and weather conditions,\nmaking it challenging to achieve real-time perception of road conditions. In\nthis paper, we propose an illumination-aware multi-modal fusion network (IMF),\nwhich leverages both exteroceptive and proprioceptive perception and optimizes\nthe fusion process based on illumination features. We introduce an\nillumination-perception sub-network to accurately estimate illumination\nfeatures. Moreover, we design a multi-modal fusion network which is able to\ndynamically adjust weights of different modalities according to illumination\nfeatures. We enhance the optimization process by pre-training of the\nillumination-perception sub-network and incorporating illumination loss as one\nof the training constraints. Extensive experiments demonstrate that the IMF\nshows a superior performance compared to state-of-the-art methods. The\ncomparison results with single modality perception methods highlight the\ncomprehensive advantages of multi-modal fusion in accurately perceiving road\nterrains under varying lighting conditions. Our dataset is available at:\nhttps://github.com/lindawang2016/IMF.\n","authors":["Rui Wang","Shichun Yang","Yuyi Chen","Zhuoyang Li","Zexiang Tong","Jianyi Xu","Jiayi Lu","Xinjie Feng","Yaoguang Cao"],"pdf_url":"https://arxiv.org/pdf/2505.11066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11065v1","updated":"2025-05-16T10:00:56Z","published":"2025-05-16T10:00:56Z","title":"Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund\n  Investment Benchmarking","summary":"  Large Language Models (LLMs) have demonstrated notable capabilities across\nfinancial tasks, including financial report summarization, earnings call\ntranscript analysis, and asset classification. However, their real-world\neffectiveness in managing complex fund investment remains inadequately\nassessed. A fundamental limitation of existing benchmarks for evaluating\nLLM-driven trading strategies is their reliance on historical back-testing,\ninadvertently enabling LLMs to \"time travel\"-leveraging future information\nembedded in their training corpora, thus resulting in possible information\nleakage and overly optimistic performance estimates. To address this issue, we\nintroduce DeepFund, a live fund benchmark tool designed to rigorously evaluate\nLLM in real-time market conditions. Utilizing a multi-agent architecture,\nDeepFund connects directly with real-time stock market data-specifically data\npublished after each model pretraining cutoff-to ensure fair and leakage-free\nevaluations. Empirical tests on nine flagship LLMs from leading global\ninstitutions across multiple investment dimensions-including ticker-level\nanalysis, investment decision-making, portfolio management, and risk\ncontrol-reveal significant practical challenges. Notably, even cutting-edge\nmodels such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses\nwithin DeepFund real-time evaluation environment, underscoring the present\nlimitations of LLMs for active fund management. Our code is available at\nhttps://github.com/HKUSTDial/DeepFund.\n","authors":["Changlun Li","Yao Shi","Chen Wang","Qiqi Duan","Runke Ruan","Weijie Huang","Haonan Long","Lijun Huang","Yuyu Luo","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2505.11065v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.00254v3","updated":"2025-05-16T10:00:32Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVAS, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVAS incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an\naccuracy of 75.8%.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v3.pdf","comment":"15 pages, AVAS, add latency breakdown"},{"id":"http://arxiv.org/abs/2502.16936v3","updated":"2025-05-16T09:57:33Z","published":"2025-02-24T08:01:40Z","title":"Supervised contrastive learning from weakly-labeled audio segments for\n  musical version matching","summary":"  Detecting musical versions (different renditions of the same piece) is a\nchallenging task with important applications. Because of the ground truth\nnature, existing approaches match musical versions at the track level (e.g.,\nwhole song). However, most applications require to match them at the segment\nlevel (e.g., 20s chunks). In addition, existing approaches resort to\nclassification and triplet losses, disregarding more recent losses that could\nbring meaningful improvements. In this paper, we propose a method to learn from\nweakly annotated segments, together with a contrastive loss variant that\noutperforms well-studied alternatives. The former is based on pairwise segment\ndistance reductions, while the latter modifies an existing loss following\ndecoupling, hyper-parameter, and geometric considerations. With these two\nelements, we do not only achieve state-of-the-art results in the standard\ntrack-level evaluation, but we also obtain a breakthrough performance in a\nsegment-level evaluation. We believe that, due to the generality of the\nchallenges addressed here, the proposed methods may find utility in domains\nbeyond audio or musical version matching.\n","authors":["Joan Serrà","R. Oguz Araz","Dmitry Bogdanov","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.16936v3.pdf","comment":"17 pages, 6 figures, 8 tables (includes Appendix); accepted at ICML25"},{"id":"http://arxiv.org/abs/2505.11060v1","updated":"2025-05-16T09:57:15Z","published":"2025-05-16T09:57:15Z","title":"CUBIC: Concept Embeddings for Unsupervised Bias Identification using\n  VLMs","summary":"  Deep vision models often rely on biases learned from spurious correlations in\ndatasets. To identify these biases, methods that interpret high-level,\nhuman-understandable concepts are more effective than those relying primarily\non low-level features like heatmaps. A major challenge for these concept-based\nmethods is the lack of image annotations indicating potentially bias-inducing\nconcepts, since creating such annotations requires detailed labeling for each\ndataset and concept, which is highly labor-intensive. We present CUBIC (Concept\nembeddings for Unsupervised Bias IdentifiCation), a novel method that\nautomatically discovers interpretable concepts that may bias classifier\nbehavior. Unlike existing approaches, CUBIC does not rely on predefined bias\ncandidates or examples of model failures tied to specific biases, as such\ninformation is not always available. Instead, it leverages image-text latent\nspace and linear classifier probes to examine how the latent representation of\na superclass label$\\unicode{x2014}$shared by all instances in the\ndataset$\\unicode{x2014}$is influenced by the presence of a given concept. By\nmeasuring these shifts against the normal vector to the classifier's decision\nboundary, CUBIC identifies concepts that significantly influence model\npredictions. Our experiments demonstrate that CUBIC effectively uncovers\npreviously unknown biases using Vision-Language Models (VLMs) without requiring\nthe samples in the dataset where the classifier underperforms or prior\nknowledge of potential biases.\n","authors":["David Méndez","Gianpaolo Bontempo","Elisa Ficarra","Roberto Confalonieri","Natalia Díaz-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2505.11060v1.pdf","comment":"8 pages, 3 figures, 5 tables. Accepted at IJCNN 2025; to appear in\n  IEEE Xplore"},{"id":"http://arxiv.org/abs/2411.00401v2","updated":"2025-05-16T09:52:52Z","published":"2024-11-01T07:01:28Z","title":"Statistical Guarantees for Lifelong Reinforcement Learning using\n  PAC-Bayes Theory","summary":"  Lifelong reinforcement learning (RL) has been developed as a paradigm for\nextending single-task RL to more realistic, dynamic settings. In lifelong RL,\nthe \"life\" of an RL agent is modeled as a stream of tasks drawn from a task\ndistribution. We propose EPIC (Empirical PAC-Bayes that Improves Continuously),\na novel algorithm designed for lifelong RL using PAC-Bayes theory. EPIC learns\na shared policy distribution, referred to as the world policy, which enables\nrapid adaptation to new tasks while retaining valuable knowledge from previous\nexperiences. Our theoretical analysis establishes a relationship between the\nalgorithm's generalization performance and the number of prior tasks preserved\nin memory. We also derive the sample complexity of EPIC in terms of RL regret.\nExtensive experiments on a variety of environments demonstrate that EPIC\nsignificantly outperforms existing methods in lifelong RL, offering both\ntheoretical guarantees and practical efficacy through the use of the world\npolicy.\n","authors":["Zhi Zhang","Chris Chow","Yasi Zhang","Yanchao Sun","Haochen Zhang","Eric Hanchen Jiang","Han Liu","Furong Huang","Yuchen Cui","Oscar Hernan Madrid Padilla"],"pdf_url":"https://arxiv.org/pdf/2411.00401v2.pdf","comment":"9 pages, 4 figures, accepted at AISTATS 2025 (PMLR Vol 258), paper ID\n  9417"},{"id":"http://arxiv.org/abs/2505.11050v1","updated":"2025-05-16T09:46:36Z","published":"2025-05-16T09:46:36Z","title":"Halting Recurrent GNNs and the Graded $μ$-Calculus","summary":"  Graph Neural Networks (GNNs) are a class of machine-learning models that\noperate on graph-structured data. Their expressive power is intimately related\nto logics that are invariant under graded bisimilarity. Current proposals for\nrecurrent GNNs either assume that the graph size is given to the model, or\nsuffer from a lack of termination guarantees. In this paper, we propose a\nhalting mechanism for recurrent GNNs. We prove that our halting model can\nexpress all node classifiers definable in graded modal mu-calculus, even for\nthe standard GNN variant that is oblivious to the graph size. A recent\nbreakthrough in the study of the expressivity of graded modal mu-calculus in\nthe finite suggests that conversely, restricted to node classifiers definable\nin monadic second-order logic, recurrent GNNs can express only node classifiers\ndefinable in graded modal mu-calculus. To prove our main result, we develop a\nnew approximate semantics for graded mu-calculus, which we believe to be of\nindependent interest. We leverage this new semantics into a new model-checking\nalgorithm, called the counting algorithm, which is oblivious to the graph size.\nIn a final step we show that the counting algorithm can be implemented on a\nhalting recurrent GNN.\n","authors":["Jeroen Bollen","Jan Van den Bussche","Stijn Vansummeren","Jonni Virtema"],"pdf_url":"https://arxiv.org/pdf/2505.11050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11049v1","updated":"2025-05-16T09:46:10Z","published":"2025-05-16T09:46:10Z","title":"GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning","summary":"  To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/\n","authors":["Yue Liu","Shengfang Zhai","Mingzhe Du","Yulin Chen","Tri Cao","Hongcheng Gao","Cheng Wang","Xinfeng Li","Kun Wang","Junfeng Fang","Jiaheng Zhang","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2505.11049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07313v2","updated":"2025-05-16T09:41:23Z","published":"2025-05-12T07:59:13Z","title":"Towards Multi-Agent Reasoning Systems for Collaborative Expertise\n  Delegation: An Exploratory Design Study","summary":"  Designing effective collaboration structure for multi-agent LLM systems to\nenhance collective reasoning is crucial yet remains under-explored. In this\npaper, we systematically investigate how collaborative reasoning performance is\naffected by three key design dimensions: (1) Expertise-Domain Alignment, (2)\nCollaboration Paradigm (structured workflow vs. diversity-driven integration),\nand (3) System Scale. Our findings reveal that expertise alignment benefits are\nhighly domain-contingent, proving most effective for contextual reasoning\ntasks. Furthermore, collaboration focused on integrating diverse knowledge\nconsistently outperforms rigid task decomposition. Finally, we empirically\nexplore the impact of scaling the multi-agent system with expertise\nspecialization and study the computational trade off, highlighting the need for\nmore efficient communication protocol design. This work provides concrete\nguidelines for configuring specialized multi-agent system and identifies\ncritical architectural trade-offs and bottlenecks for scalable multi-agent\nreasoning. The code will be made available upon acceptance.\n","authors":["Baixuan Xu","Chunyang Li","Weiqi Wang","Wei Fan","Tianshi Zheng","Haochen Shi","Tao Fan","Yangqiu Song","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2505.07313v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2409.15963v4","updated":"2025-05-16T09:40:19Z","published":"2024-09-24T10:48:13Z","title":"Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning","summary":"  Optimizing objective functions subject to constraints is fundamental in many\nreal-world applications. However, these constraints are often not readily\ndefined and must be inferred from expert agent behaviors, a problem known as\nInverse Constraint Inference. Inverse Constrained Reinforcement Learning (ICRL)\nis a common solver for recovering feasible constraints in complex environments,\nrelying on training samples collected from interactive environments. However,\nthe efficacy and efficiency of current sampling strategies remain unclear. We\npropose a strategic exploration framework for sampling with guaranteed\nefficiency to bridge this gap. By defining the feasible cost set for ICRL\nproblems, we analyze how estimation errors in transition dynamics and the\nexpert policy influence the feasibility of inferred constraints. Based on this\nanalysis, we introduce two exploratory algorithms to achieve efficient\nconstraint inference via 1) dynamically reducing the bounded aggregate error of\ncost estimations or 2) strategically constraining the exploration policy around\nplausibly optimal ones. Both algorithms are theoretically grounded with\ntractable sample complexity, and their performance is validated empirically\nacross various environments.\n","authors":["Bo Yue","Jian Li","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.15963v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14731v2","updated":"2025-05-16T09:40:01Z","published":"2024-10-16T08:34:51Z","title":"MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection","summary":"  KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.\n","authors":["Bokai Lin","Zihao Zeng","Zipeng Xiao","Siqi Kou","Tianqi Hou","Xiaofeng Gao","Hao Zhang","Zhijie Deng"],"pdf_url":"https://arxiv.org/pdf/2410.14731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13522v3","updated":"2025-05-16T09:34:08Z","published":"2024-05-22T10:45:50Z","title":"Intervention-Aware Forecasting: Breaking Historical Limits from a System\n  Perspective","summary":"  Traditional time series forecasting methods predominantly rely on historical\ndata patterns, neglecting external interventions that significantly shape\nfuture dynamics. Through control-theoretic analysis, we show that the implicit\n\"self-stimulation\" assumption limits the accuracy of these forecasts. To\novercome this limitation, we propose an Intervention-Aware Time Series\nForecasting (IATSF) framework explicitly designed to incorporate external\ninterventions. We particularly emphasize textual interventions due to their\nunique capability to represent qualitative or uncertain influences inadequately\ncaptured by conventional exogenous variables. We propose a leak-free benchmark\ncomposed of temporally synchronized textual intervention data across synthetic\nand real-world scenarios. To rigorously evaluate IATSF, we develop FIATS, a\nlightweight forecasting model that integrates textual interventions through\nChannel-Aware Adaptive Sensitivity Modeling (CASM) and Channel-Aware Parameter\nSharing (CAPS) mechanisms, enabling the model to adjust its sensitivity to\ninterventions and historical data in a channel-specific manner. Extensive\nempirical evaluations confirm that FIATS surpasses state-of-the-art methods,\nhighlighting that forecasting improvements stem explicitly from modeling\nexternal interventions rather than increased model complexity alone.\n","authors":["Zhijian Xu","Hao Wang","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2405.13522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11034v1","updated":"2025-05-16T09:29:41Z","published":"2025-05-16T09:29:41Z","title":"CleanPatrick: A Benchmark for Image Data Cleaning","summary":"  Robust machine learning depends on clean data, yet current image data\ncleaning benchmarks rely on synthetic noise or narrow human studies, limiting\ncomparison and real-world relevance. We introduce CleanPatrick, the first\nlarge-scale benchmark for data cleaning in the image domain, built upon the\npublicly available Fitzpatrick17k dermatology dataset. We collect 496,377\nbinary annotations from 933 medical crowd workers, identify off-topic samples\n(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation\nmodel inspired by item-response theory followed by expert review to derive\nhigh-quality ground truth. CleanPatrick formalizes issue detection as a ranking\ntask and adopts typical ranking metrics mirroring real audit workflows.\nBenchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident\nLearning, NoiseRank, and SelfClean, we find that, on CleanPatrick,\nself-supervised representations excel at near-duplicate detection, classical\nmethods achieve competitive off-topic detection under constrained review\nbudgets, and label-error detection remains an open challenge for fine-grained\nmedical classification. By releasing both the dataset and the evaluation\nframework, CleanPatrick enables a systematic comparison of image-cleaning\nstrategies and paves the way for more reliable data-centric artificial\nintelligence.\n","authors":["Fabian Gröger","Simone Lionetti","Philippe Gottfrois","Alvaro Gonzalez-Jimenez","Ludovic Amruthalingam","Elisabeth Victoria Goessinger","Hanna Lindemann","Marie Bargiela","Marie Hofbauer","Omar Badri","Philipp Tschandl","Arash Koochek","Matthew Groh","Alexander A. Navarini","Marc Pouly"],"pdf_url":"https://arxiv.org/pdf/2505.11034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11030v1","updated":"2025-05-16T09:25:00Z","published":"2025-05-16T09:25:00Z","title":"The heteronomy of algorithms: Traditional knowledge and computational\n  knowledge","summary":"  If an active citizen should increasingly be a computationally enlightened\none, replacing the autonomy of reason with the heteronomy of algorithms, then I\nargue in this article that we must begin teaching the principles of critiquing\nthe computal through new notions of what we might call digital Bildung. Indeed,\nif civil society itself is mediated by computational systems and media, the\npublic use of reason must also be complemented by skills for negotiating and\nusing these computal forms to articulate such critique. Not only is there a\nneed to raise the intellectual tone regarding computation and its related\nsoftwarization processes, but there is an urgent need to attend to the likely\nepistemic challenges from computation which, as presently constituted, tends\ntowards justification through a philosophy of utility rather than through a\nphilosophy of care for the territory of the intellect. We therefore need to\ndevelop an approach to this field that uses concepts and methods drawn from\nphilosophy, politics, history, anthropology, sociology, media studies, computer\nscience, and the humanities more generally, to try to understand these issues -\nparticularly the way in which software and data increasingly penetrate our\neveryday life and the pressures and fissures that are created. We must, in\nother words, move to undertake a critical interdisciplinary research program to\nunderstand the way in which these systems are created, instantiated, and\nnormatively engendered in both specific and general contexts.\n","authors":["David M. Berry"],"pdf_url":"https://arxiv.org/pdf/2505.11030v1.pdf","comment":null}]},"2025-05-19T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2505.11365v2","updated":"2025-05-19T09:01:44Z","published":"2025-05-16T15:31:08Z","title":"Phare: A Safety Probe for Large Language Models","summary":"  Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.\n","authors":["Pierre Le Jeune","Benoît Malézieux","Weixuan Xiao","Matteo Dora"],"pdf_url":"https://arxiv.org/pdf/2505.11365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10111v3","updated":"2025-05-19T04:17:07Z","published":"2025-05-15T09:28:46Z","title":"When Mitigations Backfire: Timing Channel Attacks and Defense for\n  PRAC-Based RowHammer Mitigations","summary":"  Per Row Activation Counting (PRAC) has emerged as a robust framework for\nmitigating RowHammer (RH) vulnerabilities in modern DRAM systems. However, we\nuncover a critical vulnerability: a timing channel introduced by the Alert\nBack-Off (ABO) protocol and Refresh Management (RFM) commands. We present\nPRACLeak, a novel attack that exploits these timing differences to leak\nsensitive information, such as secret keys from vulnerable AES implementations,\nby monitoring memory access latencies.\n  To counter this, we propose Timing-Safe PRAC (TPRAC), a defense that\neliminates PRAC-induced timing channels without compromising RH mitigation\nefficacy. TPRAC uses Timing-Based RFMs, issued periodically and independent of\nmemory activity. It requires only a single-entry in-DRAM mitigation queue per\nDRAM bank and is compatible with existing DRAM standards. Our evaluations\ndemonstrate that TPRAC closes timing channels while incurring only 3.4%\nperformance overhead at the RH threshold of 1024.\n","authors":["Jeonghyun Woo","Joyce Qu","Gururaj Saileshwar","Prashant J. Nair"],"pdf_url":"https://arxiv.org/pdf/2505.10111v3.pdf","comment":"This paper was originally submitted to ISCA 2025\n  (https://iscaconf.org/isca2025/) on November 22, 2024, accepted on March 21,\n  2025, and will be presented on June 24, 2025. It is 19 pages long, including\n  references and an appendix. The artifact is available at:\n  https://github.com/STAR-Laboratory/PRAC_TC_ISCA25"},{"id":"http://arxiv.org/abs/2505.11063v2","updated":"2025-05-19T06:52:59Z","published":"2025-05-16T10:00:15Z","title":"Think Twice Before You Act: Enhancing Agent Behavioral Safety with\n  Thought Correction","summary":"  LLM-based autonomous agents possess capabilities such as reasoning, tool\ninvocation, and environment interaction, enabling the execution of complex\nmulti-step tasks. The internal reasoning process, i.e., thought, of behavioral\ntrajectory significantly influences tool usage and subsequent actions but can\nintroduce potential risks. Even minor deviations in the agent's thought may\ntrigger cascading effects leading to irreversible safety incidents. To address\nthe safety alignment challenges in long-horizon behavioral trajectories, we\npropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing\na lightweight and resource-efficient model, Thought-Aligner corrects each\nhigh-risk thought on the fly before each action execution. The corrected\nthought is then reintroduced to the agent, ensuring safer subsequent decisions\nand tool interactions. Importantly, Thought-Aligner modifies only the reasoning\nphase without altering the underlying agent framework, making it easy to deploy\nand widely applicable to various agent frameworks. To train the Thought-Aligner\nmodel, we construct an instruction dataset across ten representative scenarios\nand simulate ReAct execution trajectories, generating 5,000 diverse\ninstructions and more than 11,400 safe and unsafe thought pairs. The model is\nfine-tuned using contrastive learning techniques. Experiments across three\nagent safety benchmarks involving 12 different LLMs demonstrate that\nThought-Aligner raises agent behavioral safety from approximately 50% in the\nunprotected setting to 90% on average. Additionally, Thought-Aligner maintains\nresponse latency below 100ms with minimal resource usage, demonstrating its\ncapability for efficient deployment, broad applicability, and timely\nresponsiveness. This method thus provides a practical dynamic safety solution\nfor the LLM-based agents.\n","authors":["Changyue Jiang","Xudong Pan","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2505.11063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09928v2","updated":"2025-05-19T10:13:31Z","published":"2025-05-15T03:25:41Z","title":"DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles","summary":"  Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems.\n","authors":["Xingchen Sun","Runhua Xu","Wei Ni","Li Duan","Chao Li"],"pdf_url":"https://arxiv.org/pdf/2505.09928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13362v1","updated":"2025-05-19T17:07:00Z","published":"2025-05-19T17:07:00Z","title":"DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against\n  Membership Inference Attacks","summary":"  Membership Inference Attacks (MIAs) pose a significant risk to the privacy of\ntraining datasets by exploiting subtle differences in model outputs to\ndetermine whether a particular data sample was used during training. These\nattacks can compromise sensitive information, especially in domains such as\nhealthcare and finance, where data privacy is paramount. Traditional mitigation\ntechniques, such as static differential privacy, rely on injecting a fixed\namount of noise during training or inference. However, this approach often\nleads to a detrimental trade-off: the noise may be insufficient to counter\nsophisticated attacks or, when increased, may substantially degrade model\nperformance. In this paper, we present DynaNoise, an adaptive approach that\ndynamically modulates noise injection based on query sensitivity. Our approach\nperforms sensitivity analysis using measures such as Shannon entropy to\nevaluate the risk associated with each query and adjusts the noise variance\naccordingly. A probabilistic smoothing step is then applied to renormalize the\nperturbed outputs, ensuring that the model maintains high accuracy while\neffectively obfuscating membership signals. We further propose an empirical\nmetric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),\nwhich quantifies the balance between reducing attack success rates and\npreserving the target model's accuracy. Our extensive evaluation on several\nbenchmark datasets demonstrates that DynaNoise not only significantly reduces\nMIA success rates but also achieves up to a fourfold improvement in the MIDPUT\nmetric compared to the state-of-the-art. Moreover, DynaNoise maintains\ncompetitive model accuracy while imposing only marginal inference overhead,\nhighlighting its potential as an effective and efficient privacy defense\nagainst MIAs.\n","authors":["Javad Forough","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2505.13362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13329v1","updated":"2025-05-19T16:38:06Z","published":"2025-05-19T16:38:06Z","title":"Recommender Systems for Democracy: Toward Adversarial Robustness in\n  Voting Advice Applications","summary":"  Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.\n","authors":["Frédéric Berdoz","Dustin Brunner","Yann Vonlanthen","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2505.13329v1.pdf","comment":"This is the extended version of the paper, accepted at IJCAI 2025"},{"id":"http://arxiv.org/abs/2505.13292v1","updated":"2025-05-19T16:14:27Z","published":"2025-05-19T16:14:27Z","title":"Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs","summary":"  In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.\n","authors":["Huaiying Luo","Cheng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.13292v1.pdf","comment":"Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering"},{"id":"http://arxiv.org/abs/2505.13280v1","updated":"2025-05-19T16:04:43Z","published":"2025-05-19T16:04:43Z","title":"FlowPure: Continuous Normalizing Flows for Adversarial Purification","summary":"  Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.\n","authors":["Elias Collaert","Abel Rodríguez","Sander Joos","Lieven Desmet","Vera Rimmer"],"pdf_url":"https://arxiv.org/pdf/2505.13280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09493v2","updated":"2025-05-19T15:57:05Z","published":"2024-09-14T17:40:35Z","title":"Hacking, The Lazy Way: LLM Augmented Pentesting","summary":"  In our research, we introduce a new concept called \"LLM Augmented Pentesting\"\ndemonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field\nof ethical hacking by integrating Large Language Models (LLMs) into penetration\ntesting workflows, leveraging the advanced GPT-4-turbo model. Our approach\nfocuses on overcoming the traditional resistance to automation in penetration\ntesting by employing LLMs to automate specific sub-tasks while ensuring a\ncomprehensive understanding of the overall testing process.\n  Pentest Copilot showcases remarkable proficiency in tasks such as utilizing\ntesting tools, interpreting outputs, and suggesting follow-up actions,\nefficiently bridging the gap between automated systems and human expertise. By\nintegrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token\nusage and enhances decision-making processes, leading to more accurate and\ncontext-aware outputs. Additionally, our implementation of Retrieval-Augmented\nGeneration (RAG) minimizes hallucinations and ensures the tool remains aligned\nwith the latest cybersecurity techniques and knowledge. We also highlight a\nunique infrastructure system that supports in-browser penetration testing,\nproviding a robust platform for cybersecurity professionals. Our findings\ndemonstrate that LLM Augmented Pentesting can not only significantly enhance\ntask completion rates in penetration testing but also effectively addresses\nreal-world challenges, marking a substantial advancement in the cybersecurity\ndomain.\n","authors":["Dhruva Goyal","Sitaraman Subramanian","Aditya Peela","Nisha P. Shetty"],"pdf_url":"https://arxiv.org/pdf/2409.09493v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Nisha P. Shetty has been added as an author as the new version includes work\n  under her supervision, enhancing the research. Significant changes have been\n  made in the methodology, survey, and introduction sections"},{"id":"http://arxiv.org/abs/2505.13239v1","updated":"2025-05-19T15:21:11Z","published":"2025-05-19T15:21:11Z","title":"Network-wide Quantum Key Distribution with Onion Routing Relay","summary":"  The advancement of quantum computing threatens classical cryptographic\nmethods, necessitating the development of secure quantum key distribution (QKD)\nsolutions for QKD Networks (QKDN). In this paper, a novel key distribution\nprotocol, Onion Routing Relay (ORR), that integrates onion routing (OR) with\npost-quantum cryptography (PQC) in a key-relay (KR) model is evaluated for\nQKDNs. This approach increases the security by enhancing confidentiality,\nintegrity, authenticity, and anonymity in quantum-secure communications. By\nemploying PQC-based encapsulation, ORR pretends to avoid the security risks\nposed by intermediate malicious nodes and ensures end-to-end security. Results\nshow that the performance of the ORR model, against current key-relay (KR) and\ntrusted-node (TN) approaches, demonstrating its feasibility and applicability\nin high-security environments maintaining a consistent Quality of Service\n(QoS). The results show that while ORR incurs higher encryption overhead, it\nprovides substantial security improvements without significantly impacting the\noverall key distribution time.\n","authors":["Pedro Otero-García","David Pérez-Castro","Manuel Fernández-Veiga","Ana Fernández-Vilas"],"pdf_url":"https://arxiv.org/pdf/2505.13239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20565v2","updated":"2025-05-19T14:40:29Z","published":"2025-02-27T22:07:16Z","title":"DPZV: Elevating the Tradeoff between Privacy and Utility in Zeroth-Order\n  Vertical Federated Learning","summary":"  Vertical Federated Learning (VFL) enables collaborative training with\nfeature-partitioned data, yet remains vulnerable to privacy leakage through\ngradient transmissions. Standard differential privacy (DP) techniques such as\nDP-SGD are difficult to apply in this setting due to VFL's distributed nature\nand the high variance incurred by vector-valued noise. On the other hand,\nzeroth-order (ZO) optimization techniques can avoid explicit gradient exposure\nbut lack formal privacy guarantees. In this work, we propose DPZV, the first ZO\noptimization framework for VFL that achieves tunable DP with performance\nguarantees. DPZV overcomes these limitations by injecting low-variance scalar\nnoise at the server, enabling controllable privacy with reduced memory\noverhead. We conduct a comprehensive theoretical analysis showing that DPZV\nmatches the convergence rate of first-order optimization methods while\nsatisfying formal ($\\epsilon, \\delta$)-DP guarantees. Experiments on image and\nlanguage benchmarks demonstrate that DPZV outperforms several baselines in\nterms of accuracy under a wide range of privacy constraints ($\\epsilon \\le\n10$), thereby elevating the privacy-utility tradeoff in VFL.\n","authors":["Jianing Zhang","Evan Chen","Chaoyue Liu","Christopher G. Brinton"],"pdf_url":"https://arxiv.org/pdf/2502.20565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13158v1","updated":"2025-05-19T14:18:19Z","published":"2025-05-19T14:18:19Z","title":"Network-wide Quantum Key Distribution with Onion Routing Relay\n  (Conference Version)","summary":"  The advancement of quantum computing threatens classical cryptographic\nmethods, necessitating the development of secure quantum key distribution (QKD)\nsolutions for QKD Networks (QKDN). In this paper, a novel key distribution\nprotocol, Onion Routing Relay (ORR), that integrates onion routing (OR) with\npost-quantum cryptography (PQC) in a key-relay (KR) model is evaluated for\nQKDNs. This approach increases the security by enhancing confidentiality,\nintegrity, authenticity (CIA principles), and anonymity in quantum-secure\ncommunications. By employing PQC-based encapsulation, ORR aims to avoid the\nsecurity risks posed by intermediate malicious nodes and ensures end-to-end\nsecurity. Our results show a competitive performance of the basic ORR model,\nagainst current KR and trusted-node (TN) approaches, demonstrating its\nfeasibility and applicability in high-security environments maintaining a\nconsistent Quality of Service (QoS). The results also show that while basic ORR\nincurs higher encryption overhead, it provides substantial security\nimprovements without significantly impacting the overall key distribution time.\nNevertheless, the introduction of an end-to-end authentication extension\n(ORR-Ext) has a significant impact on the Quality of Service (QoS), thereby\nlimiting its suitability to applications with stringent security requirements.\n","authors":["Pedro Otero-García","David Pérez-Castro","Manuel Fernández-Veiga","Ana Fernández-Vilas"],"pdf_url":"https://arxiv.org/pdf/2505.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13153v1","updated":"2025-05-19T14:13:30Z","published":"2025-05-19T14:13:30Z","title":"Prink: $k_s$-Anonymization for Streaming Data in Apache Flink","summary":"  In this paper, we present Prink, a novel and practically applicable concept\nand fully implemented prototype for ks-anonymizing data streams in real-world\napplication architectures. Building upon the pre-existing, yet rudimentary\nCASTLE scheme, Prink for the first time introduces semantics-aware\nks-anonymization of non-numerical (such as categorical or hierarchically\ngeneralizable) streaming data in a information loss-optimized manner. In\naddition, it provides native integration into Apache Flink, one of the\nprevailing frameworks for enterprise-grade stream data processing in numerous\napplication domains.\n  Our contributions excel the previously established state of the art for the\nprivacy guarantee-providing anonymization of streaming data in that they 1)\nallow to include non-numerical data in the anonymization process, 2) provide\ndiscrete datapoints instead of aggregates, thereby facilitating flexible data\nuse, 3) are applicable in real-world system contexts with minimal integration\nefforts, and 4) are experimentally proven to raise acceptable performance\noverheads and information loss in realistic settings. With these\ncharacteristics, Prink provides an anonymization approach which is practically\nfeasible for a broad variety of real-world, enterprise-grade stream processing\napplications and environments.\n","authors":["Philip Groneberg","Saskia Nuñez von Voigt","Thomas Janke","Louis Loechel","Karl Wolf","Elias Grünewald","Frank Pallas"],"pdf_url":"https://arxiv.org/pdf/2505.13153v1.pdf","comment":"accepted for ARES 2025"},{"id":"http://arxiv.org/abs/2505.13103v1","updated":"2025-05-19T13:32:51Z","published":"2025-05-19T13:32:51Z","title":"Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair","summary":"  The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.\n","authors":["Han Zheng","Ilia Shumailov","Tianqi Fan","Aiden Hall","Mathias Payer"],"pdf_url":"https://arxiv.org/pdf/2505.13103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15724v3","updated":"2025-05-19T13:10:56Z","published":"2024-10-21T07:43:20Z","title":"Cross-Chain Options: A Bridgeless, Universal, and Efficient Approach","summary":"  Options are fundamental to blockchain-based financial services, offering\nessential tools for risk management and price speculation, which enhance\nliquidity, flexibility, and market efficiency in decentralized finance (DeFi).\nDespite the growing interest in options for blockchain-resident assets, such as\ncryptocurrencies, current option mechanisms face significant challenges,\nincluding a high reliance on trusted third parties, limited asset support, high\ntrading delays, and the requirement for option holders to provide upfront\ncollateral.\n  In this paper, we present a protocol that addresses the aforementioned\nissues. Our protocol is the first to eliminate the need for holders to post\ncollateral when establishing options in trustless service environments (i.e.\nwithout a cross-chain bridge), which is achieved by introducing a guarantee\nfrom the option writer. Its universality allows for cross-chain options\ninvolving nearly \\textit{any} assets on \\textit{any} two different blockchains,\nprovided the chains' programming languages can enforce and execute the\nnecessary contract logic. Another key innovation is reducing option position\ntransfer latency, which uses Double-Authentication-Preventing Signatures\n(DAPS). Our evaluation demonstrates that the proposed scheme reduces option\ntransfer latency to less than half of that in existing methods. Rigorous\nsecurity analysis proves that our protocol achieves secure option trading, even\nwhen facing adversarial behaviors.\n","authors":["Zifan Peng","Yingjie Xue","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15724v3.pdf","comment":"Accepted by ICWS '25"},{"id":"http://arxiv.org/abs/2505.13076v1","updated":"2025-05-19T13:10:29Z","published":"2025-05-19T13:10:29Z","title":"The Hidden Dangers of Browsing AI Agents","summary":"  Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.\n","authors":["Mykyta Mudryi","Markiyan Chaklosh","Grzegorz Wójcik"],"pdf_url":"https://arxiv.org/pdf/2505.13076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16805v2","updated":"2025-05-19T12:00:57Z","published":"2024-10-22T08:32:17Z","title":"Test-time Adversarial Defense with Opposite Adversarial Path and High\n  Attack Time Cost","summary":"  Deep learning models are known to be vulnerable to adversarial attacks by\ninjecting sophisticated designed perturbations to input data. Training-time\ndefenses still exhibit a significant performance gap between natural accuracy\nand robust accuracy. In this paper, we investigate a new test-time adversarial\ndefense method via diffusion-based recovery along opposite adversarial paths\n(OAPs). We present a purifier that can be plugged into a pre-trained model to\nresist adversarial attacks. Different from prior arts, the key idea is\nexcessive denoising or purification by integrating the opposite adversarial\ndirection with reverse diffusion to push the input image further toward the\nopposite adversarial direction. For the first time, we also exemplify the\npitfall of conducting AutoAttack (Rand) for diffusion-based defense methods.\nThrough the lens of time complexity, we examine the trade-off between the\neffectiveness of adaptive attack and its computation complexity against our\ndefense. Experimental evaluation along with time cost analysis verifies the\neffectiveness of the proposed method.\n","authors":["Cheng-Han Yeh","Kuanchun Yu","Chun-Shien Lu"],"pdf_url":"https://arxiv.org/pdf/2410.16805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12995v1","updated":"2025-05-19T11:32:43Z","published":"2025-05-19T11:32:43Z","title":"ACE: Confidential Computing for Embedded RISC-V Systems","summary":"  Confidential computing plays an important role in isolating sensitive\napplications from the vast amount of untrusted code commonly found in the\nmodern cloud. We argue that it can also be leveraged to build safer and more\nsecure mission-critical embedded systems. In this paper, we introduce the\nAssured Confidential Execution (ACE), an open-source and royalty-free\nconfidential computing technology targeted for embedded RISC-V systems. We\npresent a set of principles and a methodology that we used to build \\ACE and\nthat might be applied for developing other embedded systems that require formal\nverification. An evaluation of our prototype on the first available RISC-V\nhardware supporting virtualization indicates that ACE is a viable candidate for\nour target systems.\n","authors":["Wojciech Ozga","Guerney D. H. Hunt","Michael V. Le","Lennard Gäher","Avraham Shinnar","Elaine R. Palmer","Hani Jamjoom","Silvio Dragone"],"pdf_url":"https://arxiv.org/pdf/2505.12995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12968v1","updated":"2025-05-19T11:03:09Z","published":"2025-05-19T11:03:09Z","title":"Lara: Lightweight Anonymous Authentication with Asynchronous Revocation\n  Auditability","summary":"  Anonymous authentication is a technique that allows to combine access control\nwith privacy preservation. Typically, clients use different pseudonyms for each\naccess, hindering providers from correlating their activities. To perform the\nrevocation of pseudonyms in a privacy preserving manner is notoriously\nchallenging. When multiple pseudonyms are revoked together, an adversary may\ninfer that these pseudonyms belong to the same client and perform privacy\nbreaking correlations, in particular if these pseudonyms have already been\nused. Backward unlinkability and revocation auditability are two properties\nthat address this problem. Most systems that offer these properties rely on\nsome sort of time slots, which assume a common reference of time that must be\nshared among clients and providers; for instance, the client must be aware that\nit should not use a pseudonym after a certain time or should be able to assess\nthe freshness of a revocation list prior to perform authentication. In this\npaper we propose Lara, a Lightweight Anonymous Authentication with Asynchronous\nRevocation Auditability that does not require parties to agree on the current\ntime slot and it is not affected by the clock skew. Prior to disclosing a\npseudonym, clients are provided with a revocation list (RL) and can check that\nthe pseudonym has not been revoked. Then, they provide a proof on\nnon-revocation that cannot be used against any other (past or future) RL,\navoiding any dependency of timing assumptions. Lara can be implemented using\nefficient public-key primitives and space-efficient data structures. We have\nimplemented a prototype of Lara and have assessed experimentally its\nefficiency.\n","authors":["Claudio Correia","Guilherme Santos","Luis Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2505.12968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15585v2","updated":"2025-05-19T09:31:40Z","published":"2025-04-22T05:02:49Z","title":"A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment","summary":"  The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.\n","authors":["Kun Wang","Guibin Zhang","Zhenhong Zhou","Jiahao Wu","Miao Yu","Shiqian Zhao","Chenlong Yin","Jinhu Fu","Yibo Yan","Hanjun Luo","Liang Lin","Zhihao Xu","Haolang Lu","Xinye Cao","Xinyun Zhou","Weifei Jin","Fanci Meng","Junyuan Mao","Yu Wang","Hao Wu","Minghe Wang","Fan Zhang","Junfeng Fang","Wenjie Qu","Yue Liu","Chengwei Liu","Yifan Zhang","Qiankun Li","Chongye Guo","Yalan Qin","Zhaoxin Fan","Yi Ding","Donghai Hong","Jiaming Ji","Yingxin Lai","Zitong Yu","Xinfeng Li","Yifan Jiang","Yanhui Li","Xinyu Deng","Junlin Wu","Dongxia Wang","Yihao Huang","Yufei Guo","Jen-tse Huang","Qiufeng Wang","Wenxuan Wang","Dongrui Liu","Yanwei Yue","Wenke Huang","Guancheng Wan","Heng Chang","Tianlin Li","Yi Yu","Chenghao Li","Jiawei Li","Lei Bai","Jie Zhang","Qing Guo","Jingyi Wang","Tianlong Chen","Joey Tianyi Zhou","Xiaojun Jia","Weisong Sun","Cong Wu","Jing Chen","Xuming Hu","Yiming Li","Xiao Wang","Ningyu Zhang","Luu Anh Tuan","Guowen Xu","Jiaheng Zhang","Tianwei Zhang","Xingjun Ma","Jindong Gu","Xiang Wang","Bo An","Jun Sun","Mohit Bansal","Shirui Pan","Lingjuan Lyu","Yuval Elovici","Bhavya Kailkhura","Yaodong Yang","Hongwei Li","Wenyuan Xu","Yizhou Sun","Wei Wang","Qing Li","Ke Tang","Yu-Gang Jiang","Felix Juefei-Xu","Hui Xiong","Xiaofeng Wang","Dacheng Tao","Philip S. Yu","Qingsong Wen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2504.15585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02718v3","updated":"2025-05-19T08:59:12Z","published":"2024-09-04T13:54:38Z","title":"\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality\n  Reinforced Distillation","summary":"  Model extraction attacks (MEAs) on large language models (LLMs) have received\nincreasing attention in recent research. However, existing attack methods\ntypically adapt the extraction strategies originally developed for deep neural\nnetworks (DNNs). They neglect the underlying inconsistency between the training\ntasks of MEA and LLM alignment, leading to suboptimal attack performance. To\ntackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel\nmodel extraction algorithm specifically designed for LLMs. In particular, LoRD\nemploys a newly defined policy-gradient-style training task that utilizes the\nresponses of victim model as the signal to guide the crafting of preference for\nthe local model. Theoretical analyses demonstrate that I) The convergence\nprocedure of LoRD in model extraction is consistent with the alignment\nprocedure of LLMs, and II) LoRD can reduce query complexity while mitigating\nwatermark protection through our exploration-based stealing. Extensive\nexperiments validate the superiority of our method in extracting various\nstate-of-the-art commercial LLMs. Our code is available at:\nhttps://github.com/liangzid/LoRD-MEA .\n","authors":["Zi Liang","Qingqing Ye","Yanyun Wang","Sen Zhang","Yaxin Xiao","Ronghua Li","Jianliang Xu","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2409.02718v3.pdf","comment":"To appear at ACL 25 main conference"},{"id":"http://arxiv.org/abs/2505.12871v1","updated":"2025-05-19T08:57:08Z","published":"2025-05-19T08:57:08Z","title":"Does Low Rank Adaptation Lead to Lower Robustness against Training-Time\n  Attacks?","summary":"  Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.\n","authors":["Zi Liang","Haibo Hu","Qingqing Ye","Yaxin Xiao","Ronghua Li"],"pdf_url":"https://arxiv.org/pdf/2505.12871v1.pdf","comment":"To appear at ICML 25"},{"id":"http://arxiv.org/abs/2505.12869v1","updated":"2025-05-19T08:55:56Z","published":"2025-05-19T08:55:56Z","title":"Outsourced Privacy-Preserving Feature Selection Based on Fully\n  Homomorphic Encryption","summary":"  Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets.\n","authors":["Koki Wakiyama","Tomohiro I","Hiroshi Sakamoto"],"pdf_url":"https://arxiv.org/pdf/2505.12869v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2504.12757v2","updated":"2025-05-19T08:48:07Z","published":"2025-04-17T08:49:10Z","title":"MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI\n  System","summary":"  As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments.\n","authors":["Sonu Kumar","Anubhav Girdhar","Ritesh Patil","Divyansh Tripathi"],"pdf_url":"https://arxiv.org/pdf/2504.12757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05999v3","updated":"2025-05-19T08:41:15Z","published":"2024-05-09T09:37:22Z","title":"LLMPot: Dynamically Configured LLM-based Honeypot for Industrial\n  Protocol and Physical Process Emulation","summary":"  Industrial Control Systems (ICS) are extensively used in critical\ninfrastructures ensuring efficient, reliable, and continuous operations.\nHowever, their increasing connectivity and addition of advanced features make\nthem vulnerable to cyber threats, potentially leading to severe disruptions in\nessential services. In this context, honeypots play a vital role by acting as\ndecoy targets within ICS networks, or on the Internet, helping to detect, log,\nanalyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS\nhoneypots, however, is challenging due to the necessity of accurately\nreplicating industrial protocols and device characteristics, a crucial\nrequirement for effectively mimicking the unique operational behavior of\ndifferent industrial systems. Moreover, this challenge is compounded by the\nsignificant manual effort required in also mimicking the control logic the PLC\nwould execute, in order to capture attacker traffic aiming to disrupt critical\ninfrastructure operations. In this paper, we propose LLMPot, a novel approach\nfor designing honeypots in ICS networks harnessing the potency of Large\nLanguage Models (LLMs). LLMPot aims to automate and optimize the creation of\nrealistic honeypots with vendor-agnostic configurations, and for any control\nlogic, aiming to eliminate the manual effort and specialized knowledge\ntraditionally required in this domain. We conducted extensive experiments\nfocusing on a wide array of parameters, demonstrating that our LLM-based\napproach can effectively create honeypot devices implementing different\nindustrial protocols and diverse control logic.\n","authors":["Christoforos Vasilatos","Dunia J. Mahboobeh","Hithem Lamri","Manaar Alam","Michail Maniatakos"],"pdf_url":"https://arxiv.org/pdf/2405.05999v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12851v1","updated":"2025-05-19T08:39:07Z","published":"2025-05-19T08:39:07Z","title":"FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and\n  Non-IID-Aware Weighting","summary":"  Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients.\n","authors":["Yanhua Wen","Lu Ai","Gang Liu","Chuang Li","Jianhao Wei"],"pdf_url":"https://arxiv.org/pdf/2505.12851v1.pdf","comment":"14 pages, 5 figures, BlockSys2025"},{"id":"http://arxiv.org/abs/2505.12770v1","updated":"2025-05-19T06:50:28Z","published":"2025-05-19T06:50:28Z","title":"Testing Access-Control Configuration Changes for Web Applications","summary":"  Access-control misconfigurations are among the main causes of today's data\nbreaches in web applications. However, few techniques are available to support\nautomatic and systematic testing for access-control changes and detecting risky\nchanges to prevent severe consequences. As a result, those critical security\nconfigurations often lack testing, or are tested manually in an ad hoc way.\n  This paper advocates that tests should be made available for users to test\naccess-control configuration changes. The key challenges are such tests need to\nbe run with production environments (to reason end-to-end behavior) and need to\nbe performance-efficient. We present a new approach to create such tests, as a\nmini test environment incorporating production program and data, called\nACtests. ACtests report the impacts of access-control changes, namely the\nrequests that were denied but would be allowed after a change, and vice versa.\nUsers can validate if the changed requests are intended or not and identify\npotential security vulnerabilities.\n  We evaluate ACtests with 193 public configurations of widely-used web\napplications on Dockerhub. ACtests detect 168 new vulnerabilities from 72\nconfiguration images. We report them to the image maintainers: 54 of them have\nbeen confirmed and 44 have been fixed. We also conduct in-depth experiments\nwith five real-world deployed systems, including Wikipedia and a commercial\ncompany's web proxy. Our results show that ACtests effectively and efficiently\ndetect all the change impacts.\n","authors":["Chengcheng Xiang","Li Zhong","Eric Mugnier","Nathaniel Nguyen","Yuanyuan Zhou","Tianyin Xu"],"pdf_url":"https://arxiv.org/pdf/2505.12770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12750v1","updated":"2025-05-19T06:19:54Z","published":"2025-05-19T06:19:54Z","title":"Malware families discovery via Open-Set Recognition on Android manifest\n  permissions","summary":"  Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment.\n","authors":["Filippo Leveni","Matteo Mistura","Francesco Iubatti","Carmine Giangregorio","Nicolò Pastore","Cesare Alippi","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2505.12750v1.pdf","comment":"Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)"},{"id":"http://arxiv.org/abs/2505.12700v1","updated":"2025-05-19T04:41:49Z","published":"2025-05-19T04:41:49Z","title":"Writing a Good Security Paper for ISSCC (2025)","summary":"  Security is increasingly more important in designing chips and systems based\non them, and the International Solid-State Circuits Conference (ISSCC), the\nleading conference for presenting advances in solid-state circuits and\nsemiconductor technology, is committed to hardware security by establishing the\nsecurity subcommittee since 2024. In the past two years, the authors of this\npaper reviewed submissions as members of the Security Subcommittee, a part of\nInternational Technical Program Committee (ITPC). This paper aims to encourage\nhigh-quality submissions to grow this field in the overall scope of the ISSCC.\n","authors":["Utsav Banerjee","Chiraag Juvekar","Yong Ki Lee","Leibo Liu","Sanu Mathew","Thomas Poeppelmann","Shreyas Sen","Takeshi Sugawara","Ingrid Verbauwhede","Rabia Tugce Yazicigil"],"pdf_url":"https://arxiv.org/pdf/2505.12700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12690v1","updated":"2025-05-19T04:28:49Z","published":"2025-05-19T04:28:49Z","title":"An Automated Blackbox Noncompliance Checker for QUIC Server\n  Implementations","summary":"  We develop QUICtester, an automated approach for uncovering non-compliant\nbehaviors in the ratified QUIC protocol implementations (RFC 9000/9001).\nQUICtester leverages active automata learning to abstract the behavior of a\nQUIC implementation into a finite state machine (FSM) representation. Unlike\nprior noncompliance checking methods, to help uncover state dependencies on\nevent timing, QUICtester introduces the idea of state learning with event\ntiming variations, adopting both valid and invalid input configurations, and\ncombinations of security and transport layer parameters during learning. We use\npairwise differential analysis of learned behaviour models of tested QUIC\nimplementations to identify non-compliance instances as behaviour deviations in\na property-agnostic way. This exploits the existence of the many different QUIC\nimplementations, removing the need for validated, formal models. The diverse\nimplementations act as cross-checking test oracles to discover non-compliance.\nWe used QUICtester to analyze analyze 186 learned models from 19 QUIC\nimplementations under the five security settings and discovered 55\nimplementation errors. Significantly, the tool uncovered a QUIC specification\nambiguity resulting in an easily exploitable DoS vulnerability, led to 5 CVE\nassignments from developers, and two bug bounties thus far.\n","authors":["Kian Kai Ang","Guy Farrelly","Cheryl Pope","Damith C. Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2505.12690v1.pdf","comment":"Accepted to ASIA CCS 2025. For Code & PoCs, see\n  https://github.com/QUICTester"},{"id":"http://arxiv.org/abs/2505.12688v1","updated":"2025-05-19T04:23:16Z","published":"2025-05-19T04:23:16Z","title":"Shielding Latent Face Representations From Privacy Attacks","summary":"  In today's data-driven analytics landscape, deep learning has become a\npowerful tool, with latent representations, known as embeddings, playing a\ncentral role in several applications. In the face analytics domain, such\nembeddings are commonly used for biometric recognition (e.g., face\nidentification). However, these embeddings, or templates, can inadvertently\nexpose sensitive attributes such as age, gender, and ethnicity. Leaking such\ninformation can compromise personal privacy and affect civil liberty and human\nrights. To address these concerns, we introduce a multi-layer protection\nframework for embeddings. It consists of a sequence of operations: (a)\nencrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing\nthem using irreversible feature manifold hashing. Unlike conventional\nencryption methods, FHE enables computations directly on encrypted data,\nallowing downstream analytics while maintaining strong privacy guarantees. To\nreduce the overhead of encrypted processing, we employ embedding compression.\nOur proposed method shields latent representations of sensitive data from\nleaking private attributes (such as age and gender) while retaining essential\nfunctional capabilities (such as face identification). Extensive experiments on\ntwo datasets using two face encoders demonstrate that our approach outperforms\nseveral state-of-the-art privacy protection methods.\n","authors":["Arjun Ramesh Kaushik","Bharat Chandra Yalavarthi","Arun Ross","Vishnu Boddeti","Nalini Ratha"],"pdf_url":"https://arxiv.org/pdf/2505.12688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12655v1","updated":"2025-05-19T03:14:08Z","published":"2025-05-19T03:14:08Z","title":"Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large\n  Language Models","summary":"  Protecting cyber Intellectual Property (IP) such as web content is an\nincreasingly critical concern. The rise of large language models (LLMs) with\nonline retrieval capabilities presents a double-edged sword that enables\nconvenient access to information but often undermines the rights of original\ncontent creators. As users increasingly rely on LLM-generated responses, they\ngradually diminish direct engagement with original information sources,\nsignificantly reducing the incentives for IP creators to contribute, and\nleading to a saturating cyberspace with more AI-generated content. In response,\nwe propose a novel defense framework that empowers web content creators to\nsafeguard their web-based IP from unauthorized LLM real-time extraction by\nleveraging the semantic understanding capability of LLMs themselves. Our method\nfollows principled motivations and effectively addresses an intractable\nblack-box optimization problem. Real-world experiments demonstrated that our\nmethods improve defense success rates from 2.5% to 88.6% on different LLMs,\noutperforming traditional defenses such as configuration-based restrictions.\n","authors":["Yisheng Zhong","Yizhu Wen","Junfeng Guo","Mehran Kafai","Heng Huang","Hanqing Guo","Zhuangdi Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.12655v1.pdf","comment":"13 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.12640v1","updated":"2025-05-19T02:47:44Z","published":"2025-05-19T02:47:44Z","title":"GDPRShield: AI-Powered GDPR Support for Software Developers in Small and\n  Medium-Sized Enterprises","summary":"  With the rapid increase in privacy violations in modern software development,\nregulatory frameworks such as the General Data Protection Regulation (GDPR)\nhave been established to enforce strict data protection practices. However,\ninsufficient privacy awareness among SME software developers contributes to\nfailure in GDPR compliance. For instance, a developer unfamiliar with data\nminimization may build a system that collects excessive data, violating GDPR\nand risking fines. One reason for this lack of awareness is that developers in\nSMEs often take on multidisciplinary roles (e.g., front-end, back-end, database\nmanagement, and privacy compliance), which limits specialization in privacy.\nThis lack of awareness may lead to poor privacy attitudes, ultimately hindering\nthe development of a strong organizational privacy culture. However, SMEs that\nachieve GDPR compliance may gain competitive advantages, such as increased user\ntrust and marketing value, compared to others that do not.\n  Therefore, in this paper, we introduce a novel AI-powered framework called\n\"GDPRShield,\" specifically designed to enhance the GDPR awareness of SME\nsoftware developers and, through this, improve their privacy attitudes.\nSimultaneously, GDPRShield boosts developers' motivation to comply with GDPR\nfrom the early stages of software development. It leverages functional\nrequirements written as user stories to provide comprehensive GDPR-based\nprivacy descriptions tailored to each requirement. Alongside improving\nawareness, GDPRShield strengthens motivation by presenting real-world\nconsequences of noncompliance, such as heavy fines, reputational damage, and\nloss of user trust, aligned with each requirement. This dual focus on awareness\nand motivation leads developers to engage with GDPRShield, improving their GDPR\ncompliance and privacy attitudes, which will help SMEs build a stronger privacy\nculture over time.\n","authors":["Tharaka Wijesundara","Mathew Warren","Nalin Arachchilage"],"pdf_url":"https://arxiv.org/pdf/2505.12640v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.12625v1","updated":"2025-05-19T02:16:56Z","published":"2025-05-19T02:16:56Z","title":"R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model","summary":"  DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.\n","authors":["Ali Naseh","Harsh Chaudhari","Jaechul Roh","Mingshi Wu","Alina Oprea","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2505.12625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12613v1","updated":"2025-05-19T01:53:28Z","published":"2025-05-19T01:53:28Z","title":"Towards Centralized Orchestration of Cyber Protection Condition (CPCON)","summary":"  The United States Cyber Command (USCYBERCOM) Cyber Protection Condition\n(CPCON) framework mandates graduated security postures across Department of\nDefense (DoD) networks, but current implementation remains largely manual,\ninconsistent, and error-prone. This paper presents a prototype system for\ncentralized orchestration of CPCON directives, enabling automated policy\nenforcement and real-time threat response across heterogeneous network\nenvironments. Building on prior work in host-based intrusion response, our\nsystem leverages a policy-driven orchestrator to standardize security actions,\nisolate compromised subnets, and verify enforcement status. We validate the\nsystem through emulated attack scenarios, demonstrating improved speed,\naccuracy, and verifiability in CPCON transitions with human-in-the-loop\noversight.\n","authors":["Mark Timmons","Daniel Lukaszewski","Geoffrey Xie","Thomas Mayo","Donald McCanless"],"pdf_url":"https://arxiv.org/pdf/2505.12613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12612v1","updated":"2025-05-19T01:51:30Z","published":"2025-05-19T01:51:30Z","title":"EPSpatial: Achieving Efficient and Private Statistical Analytics of\n  Geospatial Data","summary":"  Geospatial data statistics involve the aggregation and analysis of location\ndata to derive the distribution of clients within geospatial. The need for\nprivacy protection in geospatial data analysis has become paramount due to\nconcerns over the misuse or unauthorized access of client location information.\nHowever, existing private geospatial data statistics mainly rely on privacy\ncomputing techniques such as cryptographic tools and differential privacy,\nwhich leads to significant overhead and inaccurate results. In practical\napplications, geospatial data is frequently generated by mobile devices such as\nsmartphones and IoT sensors. The continuous mobility of clients and the need\nfor real-time updates introduce additional complexity. To address these issues,\nwe first design \\textit{spatially distributed point functions (SDPF)}, which\ncombines a quad-tree structure with distributed point functions, allowing\nclients to succinctly secret-share values on the nodes of an exponentially\nlarge quad-tree. Then, we use Gray code to partition the region and combine\nSDPF with it to propose $\\mathtt{EPSpatial}$, a scheme for accurate, efficient,\nand private statistical analytics of geospatial data. Moreover, considering\nclients' frequent movement requires continuous location updates, we leverage\nthe region encoding property to present an efficient update algorithm.Security\nanalysis shows that $\\mathtt{EPSpatial}$ effectively protects client location\nprivacy. Theoretical analysis and experimental results on real datasets\ndemonstrate that $\\mathtt{EPSpatial}$ reduces computational and communication\noverhead by at least $50\\%$ compared to existing statistical schemes.\n","authors":["Chuan Zhang","Xuhao Ren","Zhangcheng Huang","Jinwen Liang","Jianzong Wang","Liehuang Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.12612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12610v1","updated":"2025-05-19T01:47:01Z","published":"2025-05-19T01:47:01Z","title":"hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced\n  Security and Privacy","summary":"  Concerns regarding privacy and data security in conventional healthcare\nprompted alternative technologies. In smart healthcare, blockchain technology\naddresses existing concerns with security, privacy, and electronic healthcare\ntransmission. Integration of Blockchain Technology with the Internet of Medical\nThings (IoMT) allows real-time monitoring of protected healthcare data.\nUtilizing edge devices with IoMT devices is very advantageous for addressing\nsecurity, computing, and storage challenges. Encryption using symmetric and\nasymmetric keys is used to conceal sensitive information from unauthorized\nparties. SHA256 is an algorithm for one-way hashing. It is used to verify that\nthe data has not been altered, since if it had, the hash value would have\nchanged. This article offers a blockchain-based smart healthcare system using\nIoMT devices for continuous patient monitoring. In addition, it employs edge\nresources in addition to IoMT devices to have extra computing power and storage\nto hash and encrypt incoming data before sending it to the blockchain.\nSymmetric key is utilized to keep the data private even in the blockchain,\nallowing the patient to safely communicate the data through smart contracts\nwhile preventing unauthorized physicians from seeing the data. Through the use\nof a verification node and blockchain, an asymmetric key is used for the\nsigning and validation of patient data in the healthcare provider system. In\naddition to other security measures, location-based authentication is\nrecommended to guarantee that data originates from the patient area. Through\nthe edge device, SHA256 is utilized to secure the data's integrity and a secret\nkey is used to maintain its secrecy. The hChain architecture improves the\ncomputing power of IoMT environments, the security of EHR sharing through smart\ncontracts, and the privacy and authentication procedures.\n","authors":["Musharraf Alruwaill","Saraju Mohanty","Elias Kougianos"],"pdf_url":"https://arxiv.org/pdf/2505.12610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15772v2","updated":"2025-05-19T01:40:25Z","published":"2025-03-20T01:11:35Z","title":"Detecting LLM-Generated Peer Reviews","summary":"  The integrity of peer review is fundamental to scientific progress, but the\nrise of large language models (LLMs) has introduced concerns that some\nreviewers may rely on these tools to generate reviews rather than writing them\nindependently. Although some venues have banned LLM-assisted reviewing,\nenforcement remains difficult as existing detection tools cannot reliably\ndistinguish between fully generated reviews and those merely polished with AI\nassistance. In this work, we address the challenge of detecting LLM-generated\nreviews. We consider the approach of performing indirect prompt injection via\nthe paper's PDF, prompting the LLM to embed a covert watermark in the generated\nreview, and subsequently testing for presence of the watermark in the review.\nWe identify and address several pitfalls in na\\\"ive implementations of this\napproach. Our primary contribution is a rigorous watermarking and detection\nframework that offers strong statistical guarantees. Specifically, we introduce\nwatermarking schemes and hypothesis tests that control the family-wise error\nrate across multiple reviews, achieving higher statistical power than standard\ncorrections such as Bonferroni, while making no assumptions about the nature of\nhuman-written reviews. We explore multiple indirect prompt injection\nstrategies--including font-based embedding and obfuscated prompts--and evaluate\ntheir effectiveness under various reviewer defense scenarios. Our experiments\nfind high success rates in watermark embedding across various LLMs. We also\nempirically find that our approach is resilient to common reviewer defenses,\nand that the bounds on error rates in our statistical tests hold in practice.\nIn contrast, we find that Bonferroni-style corrections are too conservative to\nbe useful in this setting.\n","authors":["Vishisht Rao","Aounon Kumar","Himabindu Lakkaraju","Nihar B. Shah"],"pdf_url":"https://arxiv.org/pdf/2503.15772v2.pdf","comment":"27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2501.14184v3","updated":"2025-05-19T00:29:31Z","published":"2025-01-24T02:23:51Z","title":"Sample Complexity Bounds for Scalar Parameter Estimation Under Quantum\n  Differential Privacy","summary":"  This paper presents tight upper and lower bounds for minimum number of\nsamples (copies of a quantum state) required to attain a prescribed accuracy\n(measured by error variance) for scalar parameters estimation using unbiased\nestimators under quantum local differential privacy for qubits. Particularly,\nthe best-case (optimal) scenario is considered by minimizing the sample\ncomplexity over all differentially-private channels; the worst-case channels\ncan be arbitrarily uninformative and render the problem ill-defined. In the\nsmall privacy budget $\\epsilon$ regime, i.e., $\\epsilon\\ll 1$, the sample\ncomplexity scales as $\\Theta(\\epsilon^{-2})$. This bound matches that of\nclassical parameter estimation under local differential privacy. The lower\nbound however loosens in the large privacy budget regime, i.e., $\\epsilon\\gg\n1$. The upper bound for the minimum number of samples is generalized to qudits\n(with dimension $d$) resulting in sample complexity of $O(d\\epsilon^{-2})$.\n","authors":["Farhad Farokhi"],"pdf_url":"https://arxiv.org/pdf/2501.14184v3.pdf","comment":"Accepted for publication in IEEE Control Systems Letters"},{"id":"http://arxiv.org/abs/2505.12582v1","updated":"2025-05-19T00:05:18Z","published":"2025-05-19T00:05:18Z","title":"Compile-Time Fully Homomorphic Encryption: Eliminating Online Encryption\n  via Algebraic Basis Synthesis","summary":"  We propose a new framework for compile-time ciphertext synthesis in fully\nhomomorphic encryption (FHE) systems. Instead of invoking encryption algorithms\nat runtime, our method synthesizes ciphertexts from precomputed encrypted basis\nvectors using only homomorphic additions, scalar multiplications, and\nrandomized encryptions of zero. This decouples ciphertext generation from\nencryption, and enables efficient batch encoding through algebraic reuse. We\nformalize this technique as a randomized module morphism and prove that it\nsatisfies IND-CPA security. Our proof uses a hybrid game framework that\ninterpolates between encrypted vector instances and reduces adversarial\nadvantage to the indistinguishability of the underlying FHE scheme. This\nreduction structure captures the security implications of ciphertext basis\nreuse and structured noise injection. The proposed synthesis primitive supports\nfast, encryption-free ingestion in outsourced database systems and other\nhigh-throughput FHE pipelines. It is compatible with standard FHE APIs and\npreserves layout semantics for downstream homomorphic operations.\n","authors":["Dongfang Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.12582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02742v2","updated":"2025-05-19T22:32:04Z","published":"2024-11-05T02:20:29Z","title":"Relating Quantum Tamper-Evident Encryption to Other Cryptographic\n  Notions","summary":"  A quantum tamper-evident encryption scheme is a non-interactive symmetric-key\nencryption scheme mapping classical messages to quantum ciphertexts such that\nan honest recipient of a ciphertext can detect with high probability any\nmeaningful eavesdropping. This quantum cryptographic primitive was first\nintroduced by Gottesman in 2003. Beyond formally defining this security notion,\nGottesman's work had three main contributions: showing that any quantum\nauthentication scheme is also a tamper-evident scheme, noting that a quantum\nkey distribution scheme can be constructed from any tamper-evident scheme, and\nconstructing a prepare-and-measure tamper-evident scheme using only Wiesner\nstates inspired by Shor and Preskill's proof of security for the BB84 quantum\nkey distribution scheme.\n  In this work, we further our understanding of tamper-evident encryption by\nformally relating it to other quantum cryptographic primitives in an\ninformation-theoretic setting. In particular, we show that tamper evidence\nimplies encryption, answering a question left open by Gottesman, we show that\nit can be constructed from any encryption scheme with revocation and\nvice-versa, and we formalize an existing sketch of a construction of quantum\nmoney from any tamper-evident encryption scheme. These results also yield as a\ncorollary that any scheme allowing the revocation of a message must be an\nencryption scheme. We also show separations between tamper evidence and other\nprimitives, notably showing that tamper evidence does not imply authentication\nand does not imply uncloneable encryption.\n","authors":["Sébastien Lord"],"pdf_url":"https://arxiv.org/pdf/2411.02742v2.pdf","comment":"79 pages, 5 figures. Corrected typos and minor tweaks to\n  presentation. Results unchanged"},{"id":"http://arxiv.org/abs/2505.13758v1","updated":"2025-05-19T22:14:22Z","published":"2025-05-19T22:14:22Z","title":"BeamClean: Language Aware Embedding Reconstruction","summary":"  In this work, we consider an inversion attack on the obfuscated input\nembeddings sent to a language model on a server, where the adversary has no\naccess to the language model or the obfuscation mechanism and sees only the\nobfuscated embeddings along with the model's embedding table. We propose\nBeamClean, an inversion attack that jointly estimates the noise parameters and\ndecodes token sequences by integrating a language-model prior. Against\nLaplacian and Gaussian obfuscation mechanisms, BeamClean always surpasses naive\ndistance-based attacks. This work highlights the necessity for and robustness\nof more advanced learned, input-dependent methods.\n","authors":["Kaan Kale","Kyle Mylonakis","Jay Roberts","Sidhartha Roy"],"pdf_url":"https://arxiv.org/pdf/2505.13758v1.pdf","comment":"9 pages, 5 figures, under review at NeurIPS, 2025"},{"id":"http://arxiv.org/abs/2505.13751v1","updated":"2025-05-19T21:53:58Z","published":"2025-05-19T21:53:58Z","title":"Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives\n  Against Censorship and Bribery","summary":"  Censorship resistance is one of the core value proposition of blockchains. A\nrecurring design pattern aimed at providing censorship resistance is enabling\nmultiple proposers to contribute inputs into block construction. Notably,\nFork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in\nEthereum. However, the current proposal relies on altruistic behavior, without\na Transaction Fee Mechanism (TFM). This study aims to address this gap by\nexploring how multiple proposers should be rewarded to incentivize censorship\nresistance. The main contribution of this work is the identification of TFMs\nthat ensure censorship resistance under bribery attacks, while also satisfying\nthe incentive compatibility properties of EIP-1559. We provide a concrete\npayment mechanism for FOCIL, along with generalizable contributions to the\nliterature by analyzing 1) incentive compatibility of TFMs in the presence of a\nbribing adversary, 2) TFMs in protocols with multiple phases of transaction\ninclusion, and 3) TFMs of protocols in which parties are uncertain about the\nbehavior and the possible bribe of others.\n","authors":["Aikaterini-Panagiota Stouka","Julian Ma","Thomas Thiery"],"pdf_url":"https://arxiv.org/pdf/2505.13751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13694v1","updated":"2025-05-19T19:52:21Z","published":"2025-05-19T19:52:21Z","title":"A Systematic Review and Taxonomy for Privacy Breach Classification:\n  Trends, Gaps, and Future Directions","summary":"  In response to the rising frequency and complexity of data breaches and\nevolving global privacy regulations, this study presents a comprehensive\nexamination of academic literature on the classification of privacy breaches\nand violations between 2010-2024. Through a systematic literature review, a\ncorpus of screened studies was assembled and analyzed to identify primary\nresearch themes, emerging trends, and gaps in the field. A novel taxonomy is\nintroduced to guide efforts by categorizing research efforts into seven\ndomains: breach classification, report classification, breach detection, threat\ndetection, breach prediction, risk analysis, and threat classification. An\nanalysis reveals that breach classification and detection dominate the\nliterature, while breach prediction and risk analysis have only recently\nemerged in the literature, suggesting opportunities for potential research\nimpacts. Keyword and phrase frequency analysis reveal potentially underexplored\nareas, including location privacy, prediction models, and healthcare data\nbreaches.\n","authors":["Clint Fuchs","John D. Hastings"],"pdf_url":"https://arxiv.org/pdf/2505.13694v1.pdf","comment":"7 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2505.13655v1","updated":"2025-05-19T18:55:34Z","published":"2025-05-19T18:55:34Z","title":"Optimal Client Sampling in Federated Learning with Client-Level\n  Heterogeneous Differential Privacy","summary":"  Federated Learning with client-level differential privacy (DP) provides a\npromising framework for collaboratively training models while rigorously\nprotecting clients' privacy. However, classic approaches like DP-FedAvg\nstruggle when clients have heterogeneous privacy requirements, as they must\nuniformly enforce the strictest privacy level across clients, leading to\nexcessive DP noise and significant model utility degradation. Existing methods\nto improve the model utility in such heterogeneous privacy settings often\nassume a trusted server and are largely heuristic, resulting in suboptimal\nperformance and lacking strong theoretical underpinnings. In this work, we\naddress these challenges under a practical attack model where both clients and\nthe server are honest-but-curious. We propose GDPFed, which partitions clients\ninto groups based on their privacy budgets and achieves client-level DP within\neach group to reduce the privacy budget waste and hence improve the model\nutility. Based on the privacy and convergence analysis of GDPFed, we find that\nthe magnitude of DP noise depends on both model dimensionality and the\nper-group client sampling ratios. To further improve the performance of GDPFed,\nwe introduce GDPFed$^+$, which integrates model sparsification to eliminate\nunnecessary noise and optimizes per-group client sampling ratios to minimize\nconvergence error. Extensive empirical evaluations on multiple benchmark\ndatasets demonstrate the effectiveness of GDPFed$^+$, showing substantial\nperformance gains compared with state-of-the-art methods.\n","authors":["Jiahao Xu","Rui Hu","Olivera Kotevska"],"pdf_url":"https://arxiv.org/pdf/2505.13655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13651v1","updated":"2025-05-19T18:49:31Z","published":"2025-05-19T18:49:31Z","title":"Traceable Black-box Watermarks for Federated Learning","summary":"  Due to the distributed nature of Federated Learning (FL) systems, each local\nclient has access to the global model, posing a critical risk of model leakage.\nExisting works have explored injecting watermarks into local models to enable\nintellectual property protection. However, these methods either focus on\nnon-traceable watermarks or traceable but white-box watermarks. We identify a\ngap in the literature regarding the formal definition of traceable black-box\nwatermarking and the formulation of the problem of injecting such watermarks\ninto FL systems. In this work, we first formalize the problem of injecting\ntraceable black-box watermarks into FL. Based on the problem, we propose a\nnovel server-side watermarking method, $\\mathbf{TraMark}$, which creates a\ntraceable watermarked model for each client, enabling verification of model\nleakage in black-box settings. To achieve this, $\\mathbf{TraMark}$ partitions\nthe model parameter space into two distinct regions: the main task region and\nthe watermarking region. Subsequently, a personalized global model is\nconstructed for each client by aggregating only the main task region while\npreserving the watermarking region. Each model then learns a unique watermark\nexclusively within the watermarking region using a distinct watermark dataset\nbefore being sent back to the local client. Extensive results across various FL\nsystems demonstrate that $\\mathbf{TraMark}$ ensures the traceability of all\nwatermarked models while preserving their main task performance.\n","authors":["Jiahao Xu","Rui Hu","Olivera Kotevska","Zikai Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13641v1","updated":"2025-05-19T18:29:00Z","published":"2025-05-19T18:29:00Z","title":"An Alignment Between the CRA's Essential Requirements and the ATT&CK's\n  Mitigations","summary":"  The paper presents an alignment evaluation between the mitigations present in\nthe MITRE's ATT&CK framework and the essential cyber security requirements of\nthe recently introduced Cyber Resilience Act (CRA) in the European Union. In\noverall, the two align well with each other. With respect to the CRA, there are\nnotable gaps only in terms of data minimization, data erasure, and\nvulnerability coordination. In terms of the ATT&CK framework, gaps are present\nonly in terms of threat intelligence, training, out-of-band communication\nchannels, and residual risks. The evaluation presented contributes to narrowing\nof a common disparity between law and technical frameworks.\n","authors":["Jukka Ruohonen","Eun-Young Kang","Qusai Ramadan"],"pdf_url":"https://arxiv.org/pdf/2505.13641v1.pdf","comment":"Submitted to ESPRE@RE"},{"id":"http://arxiv.org/abs/2502.17537v2","updated":"2025-05-19T18:05:08Z","published":"2025-02-24T17:26:01Z","title":"On the Vulnerability of Concept Erasure in Diffusion Models","summary":"  The proliferation of text-to-image diffusion models has raised significant\nprivacy and security concerns, particularly regarding the generation of\ncopyrighted or harmful images. In response, several concept erasure (defense)\nmethods have been developed to prevent the generation of unwanted content\nthrough post-hoc finetuning. On the other hand, concept restoration (attack)\nmethods seek to recover supposedly erased concepts via adversarially crafted\nprompts. However, all existing restoration methods only succeed in the highly\nrestrictive scenario of finding adversarial prompts tailed to some fixed seed.\nTo address this, we introduce RECORD, a novel coordinate-descent-based\nrestoration algorithm that finds adversarial prompts to recover erased concepts\nindependently of the seed. Our extensive experiments demonstrate RECORD\nconsistently outperforms the current restoration methods by up to 17.8 times in\nthis setting. Our findings further reveal the susceptibility of unlearned\nmodels to restoration attacks, providing crucial insights into the behavior of\nunlearned models under the influence of adversarial prompts.\n","authors":["Lucas Beerens","Alex D. Richardson","Kaicheng Zhang","Dongdong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.17537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13581v1","updated":"2025-05-19T15:41:39Z","published":"2025-05-19T15:41:39Z","title":"RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection","summary":"  Content moderation for large language models (LLMs) remains a significant\nchallenge, requiring flexible and adaptable solutions that can quickly respond\nto emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),\na novel approach that leverages a retrieval-augmented generation (RAG)\narchitecture to dynamically reject unsafe user queries without model\nretraining. By strategically inserting and marking malicious documents into the\nvector database, the system can identify and reject harmful requests when these\ndocuments are retrieved. Our preliminary results show that RAR achieves\ncomparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,\nwhile offering superior flexibility and real-time customization capabilities, a\nfundamental feature to timely address critical vulnerabilities. This approach\nintroduces no architectural changes to existing RAG systems, requiring only the\naddition of specially crafted documents and a simple rejection mechanism based\non retrieval results.\n","authors":["Tommaso Mario Buonocore","Enea Parimbelli"],"pdf_url":"https://arxiv.org/pdf/2505.13581v1.pdf","comment":"7 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2210.15745v2","updated":"2025-05-19T11:10:53Z","published":"2022-10-27T19:48:26Z","title":"DICTION:DynamIC robusT whIte bOx watermarkiNg scheme for deep neural\n  networks","summary":"  Deep neural network (DNN) watermarking is a suitable method for protecting\nthe ownership of deep learning (DL) models. It secretly embeds an identifier\n(watermark) within the model, which can be retrieved by the owner to prove\nownership. In this paper, we first provide a unified framework for white box\nDNN watermarking schemes. It includes current state-of-the-art methods\noutlining their theoretical inter-connections. Next, we introduce DICTION, a\nnew white-box Dynamic Robust watermarking scheme, we derived from this\nframework. Its main originality stands on a generative adversarial network\n(GAN) strategy where the watermark extraction function is a DNN trained as a\nGAN discriminator taking the target model to watermark as a GAN generator with\na latent space as the input of the GAN trigger set. DICTION can be seen as a\ngeneralization of DeepSigns which, to the best of our knowledge, is the only\nother Dynamic white-box watermarking scheme from the literature. Experiments\nconducted on the same model test set as Deepsigns demonstrate that our scheme\nachieves much better performance. Especially, with DICTION, one can increase\nthe watermark capacity while preserving the target model accuracy at best and\nsimultaneously ensuring strong watermark robustness against a wide range of\nwatermark removal and detection attacks.\n","authors":["Reda Bellafqira","Gouenou Coatrieux"],"pdf_url":"https://arxiv.org/pdf/2210.15745v2.pdf","comment":"24 pages, 4 figures, PrePrint"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.11365v2","updated":"2025-05-19T09:01:44Z","published":"2025-05-16T15:31:08Z","title":"Phare: A Safety Probe for Large Language Models","summary":"  Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.\n","authors":["Pierre Le Jeune","Benoît Malézieux","Weixuan Xiao","Matteo Dora"],"pdf_url":"https://arxiv.org/pdf/2505.11365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11275v2","updated":"2025-05-19T01:06:35Z","published":"2025-05-16T14:10:41Z","title":"TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding\n  Capabilities of MLLMs","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the ability of artificial intelligence systems to\nunderstand and generate multimodal content. However, these models often exhibit\nlimited effectiveness when applied to non-Western cultural contexts, which\nraises concerns about their wider applicability. To address this limitation, we\npropose the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a\nbilingual (i.e., Chinese and English) Visual Question Answering (VQA) benchmark\nspecifically designed for assessing the understanding of traditional Chinese\nculture by MLLMs. TCC-Bench comprises culturally rich and visually diverse\ndata, incorporating images from museum artifacts, everyday life scenes, comics,\nand other culturally significant contexts. We adopt a semi-automated pipeline\nthat utilizes GPT-4o in text-only mode to generate candidate questions,\nfollowed by human curation to ensure data quality and avoid potential data\nleakage. The benchmark also avoids language bias by preventing direct\ndisclosure of cultural concepts within question texts. Experimental evaluations\nacross a wide range of MLLMs demonstrate that current models still face\nsignificant challenges when reasoning about culturally grounded visual content.\nThe results highlight the need for further research in developing culturally\ninclusive and context-aware multimodal systems. The code and data can be found\nat: https://tcc-bench.github.io/.\n","authors":["Pengju Xu","Yan Wang","Shuyuan Zhang","Xuan Zhou","Xin Li","Yue Yuan","Fengzhao Li","Shunyuan Zhou","Xingyu Wang","Yi Zhang","Haiying Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.11275v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.11192v2","updated":"2025-05-19T01:33:42Z","published":"2025-05-16T12:50:05Z","title":"FALCON: False-Negative Aware Learning of Contrastive Negatives in\n  Vision-Language Pretraining","summary":"  False negatives pose a critical challenge in vision-language pretraining\n(VLP) due to the many-to-many correspondence between images and texts in\nlarge-scale datasets. These false negatives introduce conflicting supervision\nsignals that degrade the learned embedding space and diminish the effectiveness\nof hard negative sampling. In this paper, we propose FALCON (False-negative\nAware Learning of COntrastive Negatives), a learning-based mini-batch\nconstruction strategy that adaptively balances the trade-off between hard and\nfalse negatives during VLP. Rather than relying on fixed heuristics, FALCON\nemploys a negative mining scheduler that dynamically selects negative samples\nof appropriate hardness for each anchor instance during mini-batch\nconstruction, guided by a proxy for cross-modal alignment improvement.\nExperimental results demonstrate that FALCON significantly improves performance\nacross two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of\ndownstream tasks and evaluation settings, underscoring its effectiveness and\nrobustness in mitigating the impact of false negatives.\n","authors":["Myunsoo Kim","Seong-Woong Shim","Byung-Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2505.11192v2.pdf","comment":"The manuscript contains errors that require substantial revision"},{"id":"http://arxiv.org/abs/2505.11175v2","updated":"2025-05-19T05:14:55Z","published":"2025-05-16T12:19:13Z","title":"Real-Time Verification of Embodied Reasoning for Generative Skill\n  Acquisition","summary":"  Generative skill acquisition enables embodied agents to actively learn a\nscalable and evolving repertoire of control skills, crucial for the advancement\nof large decision models. While prior approaches often rely on supervision\nsignals from generalist agents (e.g., LLMs), their effectiveness in complex 3D\nenvironments remains unclear; exhaustive evaluation incurs substantial\ncomputational costs, significantly hindering the efficiency of skill learning.\nInspired by recent successes in verification models for mathematical reasoning,\nwe propose VERGSA (Verifying Embodied Reasoning in Generative Skill\nAcquisition), a framework that systematically integrates real-time verification\nprinciples into embodied skill learning. VERGSA establishes 1) a seamless\nextension from verification of mathematical reasoning into embodied learning by\ndynamically incorporating contextually relevant tasks into prompts and defining\nsuccess metrics for both subtasks and overall tasks, and 2) an automated,\nscalable reward labeling scheme that synthesizes dense reward signals by\niteratively finalizing the contribution of scene configuration and subtask\nlearning to overall skill acquisition. To the best of our knowledge, this\napproach constitutes the first comprehensive training dataset for\nverification-driven generative skill acquisition, eliminating arduous manual\nreward engineering. Experiments validate the efficacy of our approach: 1) the\nexemplar task pool improves the average task success rates by 21%, 2) our\nverification model boosts success rates by 24% for novel tasks and 36% for\nencountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification\nquality.\n","authors":["Bo Yue","Shuqi Guo","Kaiyu Hu","Chujiao Wang","Benyou Wang","Kui Jia","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.11175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11063v2","updated":"2025-05-19T06:52:59Z","published":"2025-05-16T10:00:15Z","title":"Think Twice Before You Act: Enhancing Agent Behavioral Safety with\n  Thought Correction","summary":"  LLM-based autonomous agents possess capabilities such as reasoning, tool\ninvocation, and environment interaction, enabling the execution of complex\nmulti-step tasks. The internal reasoning process, i.e., thought, of behavioral\ntrajectory significantly influences tool usage and subsequent actions but can\nintroduce potential risks. Even minor deviations in the agent's thought may\ntrigger cascading effects leading to irreversible safety incidents. To address\nthe safety alignment challenges in long-horizon behavioral trajectories, we\npropose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing\na lightweight and resource-efficient model, Thought-Aligner corrects each\nhigh-risk thought on the fly before each action execution. The corrected\nthought is then reintroduced to the agent, ensuring safer subsequent decisions\nand tool interactions. Importantly, Thought-Aligner modifies only the reasoning\nphase without altering the underlying agent framework, making it easy to deploy\nand widely applicable to various agent frameworks. To train the Thought-Aligner\nmodel, we construct an instruction dataset across ten representative scenarios\nand simulate ReAct execution trajectories, generating 5,000 diverse\ninstructions and more than 11,400 safe and unsafe thought pairs. The model is\nfine-tuned using contrastive learning techniques. Experiments across three\nagent safety benchmarks involving 12 different LLMs demonstrate that\nThought-Aligner raises agent behavioral safety from approximately 50% in the\nunprotected setting to 90% on average. Additionally, Thought-Aligner maintains\nresponse latency below 100ms with minimal resource usage, demonstrating its\ncapability for efficient deployment, broad applicability, and timely\nresponsiveness. This method thus provides a practical dynamic safety solution\nfor the LLM-based agents.\n","authors":["Changyue Jiang","Xudong Pan","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2505.11063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11032v2","updated":"2025-05-19T07:28:03Z","published":"2025-05-16T09:26:59Z","title":"DexGarmentLab: Dexterous Garment Manipulation Environment with\n  Generalizable Policy","summary":"  Garment manipulation is a critical challenge due to the diversity in garment\ncategories, geometries, and deformations. Despite this, humans can effortlessly\nhandle garments, thanks to the dexterity of our hands. However, existing\nresearch in the field has struggled to replicate this level of dexterity,\nprimarily hindered by the lack of realistic simulations of dexterous garment\nmanipulation. Therefore, we propose DexGarmentLab, the first environment\nspecifically designed for dexterous (especially bimanual) garment manipulation,\nwhich features large-scale high-quality 3D assets for 15 task scenarios, and\nrefines simulation techniques tailored for garment modeling to reduce the\nsim-to-real gap. Previous data collection typically relies on teleoperation or\ntraining expert reinforcement learning (RL) policies, which are labor-intensive\nand inefficient. In this paper, we leverage garment structural correspondence\nto automatically generate a dataset with diverse trajectories using only a\nsingle expert demonstration, significantly reducing manual intervention.\nHowever, even extensive demonstrations cannot cover the infinite states of\ngarments, which necessitates the exploration of new algorithms. To improve\ngeneralization across diverse garment shapes and deformations, we propose a\nHierarchical gArment-manipuLation pOlicy (HALO). It first identifies\ntransferable affordance points to accurately locate the manipulation area, then\ngenerates generalizable trajectories to complete the task. Through extensive\nexperiments and detailed analysis of our method and baseline, we demonstrate\nthat HALO consistently outperforms existing methods, successfully generalizing\nto previously unseen instances even with significant variations in shape and\ndeformation where others fail. Our project page is available at:\nhttps://wayrise.github.io/DexGarmentLab/.\n","authors":["Yuran Wang","Ruihai Wu","Yue Chen","Jiarui Wang","Jiaqi Liang","Ziyu Zhu","Haoran Geng","Jitendra Malik","Pieter Abbeel","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.11032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13448v1","updated":"2025-05-19T17:59:58Z","published":"2025-05-19T17:59:58Z","title":"CIE: Controlling Language Model Text Generations Using Continuous\n  Signals","summary":"  Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.\n","authors":["Vinay Samuel","Harshita Diddee","Yiming Zhang","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2505.13448v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.13445v1","updated":"2025-05-19T17:59:31Z","published":"2025-05-19T17:59:31Z","title":"Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards","summary":"  Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.\n","authors":["Xiaoyuan Liu","Tian Liang","Zhiwei He","Jiahao Xu","Wenxuan Wang","Pinjia He","Zhaopeng Tu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.13445v1.pdf","comment":"code available at https://github.com/xyliu-cs/RISE"},{"id":"http://arxiv.org/abs/2503.15558v3","updated":"2025-05-19T17:59:19Z","published":"2025-03-18T22:06:58Z","title":"Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning","summary":"  Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-7B and Cosmos-Reason1-56B. We curate data\nand train our models in two stages: Physical AI supervised fine-tuning (SFT)\nand Physical AI reinforcement learning (RL). To evaluate our models, we build\ncomprehensive benchmarks for physical common sense and embodied reasoning\naccording to our ontologies. Evaluation results show that Physical AI SFT and\nRL bring significant improvements. To facilitate the development of Physical\nAI, we make our code and pre-trained models available under the NVIDIA Open\nModel License at https://github.com/nvidia-cosmos/cosmos-reason1.\n","authors":[" NVIDIA"," :","Alisson Azzolini","Junjie Bai","Hannah Brandon","Jiaxin Cao","Prithvijit Chattopadhyay","Huayu Chen","Jinju Chu","Yin Cui","Jenna Diamond","Yifan Ding","Liang Feng","Francesco Ferroni","Rama Govindaraju","Jinwei Gu","Siddharth Gururani","Imad El Hanafi","Zekun Hao","Jacob Huffman","Jingyi Jin","Brendan Johnson","Rizwan Khan","George Kurian","Elena Lantz","Nayeon Lee","Zhaoshuo Li","Xuan Li","Maosheng Liao","Tsung-Yi Lin","Yen-Chen Lin","Ming-Yu Liu","Xiangyu Lu","Alice Luo","Andrew Mathau","Yun Ni","Lindsey Pavao","Wei Ping","David W. Romero","Misha Smelyanskiy","Shuran Song","Lyne Tchapmi","Andrew Z. Wang","Boxin Wang","Haoxiang Wang","Fangyin Wei","Jiashu Xu","Yao Xu","Dinghao Yang","Xiaodong Yang","Zhuolin Yang","Jingxu Zhang","Xiaohui Zeng","Zhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13439v1","updated":"2025-05-19T17:59:01Z","published":"2025-05-19T17:59:01Z","title":"VTBench: Evaluating Visual Tokenizers for Autoregressive Image\n  Generation","summary":"  Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.\n","authors":["Huawei Lin","Tong Geng","Zhaozhuo Xu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.13439v1.pdf","comment":"24 pages, 13 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.15175v3","updated":"2025-05-19T17:58:53Z","published":"2025-01-25T11:06:37Z","title":"Option-ID Based Elimination For Multiple Choice Questions","summary":"  Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies.\n","authors":["Zhenhao Zhu","Bulou Liu","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2501.15175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13438v1","updated":"2025-05-19T17:58:44Z","published":"2025-05-19T17:58:44Z","title":"Optimizing Anytime Reasoning via Budget Relative Policy Optimization","summary":"  Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.\n","authors":["Penghui Qi","Zichen Liu","Tianyu Pang","Chao Du","Wee Sun Lee","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2505.13438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13437v1","updated":"2025-05-19T17:58:11Z","published":"2025-05-19T17:58:11Z","title":"FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance","summary":"  Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.\n","authors":["Dian Shao","Mingfei Shi","Shengda Xu","Haodong Chen","Yongle Huang","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13437v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2502.08006v2","updated":"2025-05-19T17:57:30Z","published":"2025-02-11T23:05:16Z","title":"Greed is Good: A Unifying Perspective on Guided Generation","summary":"  Training-free guided generation is a widely used and powerful technique that\nallows the end user to exert further control over the generative process of\nflow/diffusion models. Generally speaking, two families of techniques have\nemerged for solving this problem for gradient-based guidance: namely, posterior\nguidance (i.e., guidance via projecting the current sample to the target\ndistribution via the target prediction model) and end-to-end guidance (i.e.,\nguidance by performing backpropagation throughout the entire ODE solve). In\nthis work, we show that these two seemingly separate families can actually be\nunified by looking at posterior guidance as a greedy strategy of end-to-end\nguidance. We explore the theoretical connections between these two families and\nprovide an in-depth theoretical of these two techniques relative to the\ncontinuous ideal gradients. Motivated by this analysis we then show a method\nfor interpolating between these two families enabling a trade-off between\ncompute and accuracy of the guidance gradients. We then validate this work on\nseveral inverse image problems and property-guided molecular generation.\n","authors":["Zander W. Blasingame","Chen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08006v2.pdf","comment":"Revised preprint with numerical experiments"},{"id":"http://arxiv.org/abs/2503.14234v3","updated":"2025-05-19T17:56:42Z","published":"2025-03-18T13:11:43Z","title":"Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative\n  Knowledge Retrieval","summary":"  Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.\n","authors":["Ruiyi Yang","Hao Xue","Imran Razzak","Hakim Hacid","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2503.14234v3.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.13427v1","updated":"2025-05-19T17:55:08Z","published":"2025-05-19T17:55:08Z","title":"MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision","summary":"  While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.\n","authors":["Lingxiao Du","Fanqing Meng","Zongkai Liu","Zhixiang Zhou","Ping Luo","Qiaosheng Zhang","Wenqi Shao"],"pdf_url":"https://arxiv.org/pdf/2505.13427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13425v1","updated":"2025-05-19T17:54:35Z","published":"2025-05-19T17:54:35Z","title":"Learnware of Language Models: Specialized Small Language Models Can Do\n  Big","summary":"  The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.\n","authors":["Zhi-Hao Tan","Zi-Chen Zhao","Hao-Yu Shi","Xin-Yu Zhang","Peng Tan","Yang Yu","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.13425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19460v3","updated":"2025-05-19T17:50:52Z","published":"2025-02-26T10:28:44Z","title":"Overcoming Dependent Censoring in the Evaluation of Survival Models","summary":"  Conventional survival metrics, such as Harrell's concordance index (CI) and\nthe Brier Score, rely on the independent censoring assumption for valid\ninference with right-censored data. However, in the presence of so-called\ndependent censoring, where the probability of censoring is related to the event\nof interest, these metrics can give biased estimates of the underlying model\nerror. In this paper, we introduce three new evaluation metrics for survival\nanalysis based on Archimedean copulas that can account for dependent censoring.\nWe also develop a framework to generate realistic, semi-synthetic datasets with\ndependent censoring to facilitate the evaluation of the metrics. Our\nexperiments in synthetic and semi-synthetic data demonstrate that the proposed\nmetrics can provide more accurate estimates of the model error than\nconventional metrics under dependent censoring.\n","authors":["Christian Marius Lillelund","Shi-ang Qi","Russell Greiner"],"pdf_url":"https://arxiv.org/pdf/2502.19460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13417v1","updated":"2025-05-19T17:50:52Z","published":"2025-05-19T17:50:52Z","title":"AdaptThink: Reasoning Models Can Learn When to Think","summary":"  Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.\n","authors":["Jiajie Zhang","Nianyi Lin","Lei Hou","Ling Feng","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2505.13417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13408v1","updated":"2025-05-19T17:44:26Z","published":"2025-05-19T17:44:26Z","title":"CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process","summary":"  Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.\n","authors":["Jinhe Bi","Danqi Yan","Yifan Wang","Wenke Huang","Haokun Chen","Guancheng Wan","Mang Ye","Xun Xiao","Hinrich Schuetze","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2505.13408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13406v1","updated":"2025-05-19T17:41:29Z","published":"2025-05-19T17:41:29Z","title":"AutoMathKG: The automated mathematical knowledge graph based on LLM and\n  vector database","summary":"  A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM.\n","authors":["Rong Bian","Yu Geng","Zijian Yang","Bing Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.13406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13400v1","updated":"2025-05-19T17:36:17Z","published":"2025-05-19T17:36:17Z","title":"Robin: A multi-agent system for automating scientific discovery","summary":"  Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.\n","authors":["Ali Essam Ghareeb","Benjamin Chang","Ludovico Mitchener","Angela Yiu","Caralyn J. Szostkiewicz","Jon M. Laurent","Muhammed T. Razzak","Andrew D. White","Michaela M. Hinks","Samuel G. Rodriques"],"pdf_url":"https://arxiv.org/pdf/2505.13400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13391v1","updated":"2025-05-19T17:32:07Z","published":"2025-05-19T17:32:07Z","title":"Advancing Generalization Across a Variety of Abstract Visual Reasoning\n  Tasks","summary":"  The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods.\n","authors":["Mikołaj Małkiński","Jacek Mańdziuk"],"pdf_url":"https://arxiv.org/pdf/2505.13391v1.pdf","comment":"Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.13388v1","updated":"2025-05-19T17:29:03Z","published":"2025-05-19T17:29:03Z","title":"R3: Robust Rubric-Agnostic Reward Models","summary":"  Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3\n","authors":["David Anugraha","Zilu Tang","Lester James V. Miranda","Hanyang Zhao","Mohammad Rifqi Farhansyah","Garry Kuwanto","Derry Wijaya","Genta Indra Winata"],"pdf_url":"https://arxiv.org/pdf/2505.13388v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.13381v1","updated":"2025-05-19T17:25:07Z","published":"2025-05-19T17:25:07Z","title":"How Adding Metacognitive Requirements in Support of AI Feedback in\n  Practice Exams Transforms Student Learning Behaviors","summary":"  Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.\n","authors":["Mak Ahmad","Prerna Ravi","David Karger","Marc Facciotti"],"pdf_url":"https://arxiv.org/pdf/2505.13381v1.pdf","comment":"10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy"},{"id":"http://arxiv.org/abs/2505.13380v1","updated":"2025-05-19T17:24:26Z","published":"2025-05-19T17:24:26Z","title":"CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition","summary":"  Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526\n","authors":["Nam V. Nguyen","Huy Nguyen","Quang Pham","Van Nguyen","Savitha Ramasamy","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2505.13380v1.pdf","comment":"52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526"},{"id":"http://arxiv.org/abs/2505.13379v1","updated":"2025-05-19T17:24:16Z","published":"2025-05-19T17:24:16Z","title":"Thinkless: LLM Learns When to Think","summary":"  Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless\n","authors":["Gongfan Fang","Xinyin Ma","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10872v2","updated":"2025-05-19T17:21:49Z","published":"2025-05-16T05:27:15Z","title":"REI-Bench: Can Embodied Agents Understand Vague Human Instructions in\n  Task Planning?","summary":"  Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children.\n","authors":["Chenxi Jiang","Chuhao Zhou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2505.10872v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.13372v1","updated":"2025-05-19T17:19:13Z","published":"2025-05-19T17:19:13Z","title":"Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific\n  Temporal Planning Guidance using Reinforcement Learning","summary":"  Recent work investigated the use of Reinforcement Learning (RL) for the\nsynthesis of heuristic guidance to improve the performance of temporal planners\nwhen a domain is fixed and a set of training problems (not plans) is given. The\nidea is to extract a heuristic from the value function of a particular\n(possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning\nframework that focuses on exploiting the information provided by symbolic\nheuristics during both the RL and planning phases. First, we formalize\ndifferent reward schemata for the synthesis and use symbolic heuristics to\nmitigate the problems caused by the truncation of episodes needed to deal with\nthe potentially infinite MDP. Second, we propose learning a residual of an\nexisting symbolic heuristic, which is a \"correction\" of the heuristic value,\ninstead of eagerly learning the whole heuristic from scratch. Finally, we use\nthe learned heuristic in combination with a symbolic heuristic using a\nmultiple-queue planning approach to balance systematic search with imperfect\nlearned information. We experimentally compare all the approaches, highlighting\ntheir strengths and weaknesses and significantly advancing the state of the art\nfor this planning and learning schema.\n","authors":["Irene Brugnara","Alessandro Valentini","Andrea Micheli"],"pdf_url":"https://arxiv.org/pdf/2505.13372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13355v1","updated":"2025-05-19T16:57:57Z","published":"2025-05-19T16:57:57Z","title":"Multi-Armed Bandits Meet Large Language Models","summary":"  Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.\n","authors":["Djallel Bouneffouf","Raphael Feraud"],"pdf_url":"https://arxiv.org/pdf/2505.13355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17666v3","updated":"2025-05-19T16:55:06Z","published":"2025-02-24T21:29:06Z","title":"Yes, Q-learning Helps Offline In-Context RL","summary":"  Existing offline in-context reinforcement learning (ICRL) methods have\npredominantly relied on supervised training objectives, which are known to have\nlimitations in offline RL settings. In this study, we explore the integration\nof RL objectives within an offline ICRL framework. Through experiments on more\nthan 150 GridWorld and MuJoCo environment-derived datasets, we demonstrate that\noptimizing RL objectives directly improves performance by approximately 30% on\naverage compared to widely adopted Algorithm Distillation (AD), across various\ndataset coverages, structures, expertise levels, and environmental\ncomplexities. Furthermore, in the challenging XLand-MiniGrid environment, RL\nobjectives doubled the performance of AD. Our results also reveal that the\naddition of conservatism during value learning brings additional improvements\nin almost all settings tested. Our findings emphasize the importance of\naligning ICRL learning objectives with the RL reward-maximization goal, and\ndemonstrate that offline RL is a promising direction for advancing ICRL.\n","authors":["Denis Tarasov","Alexander Nikulin","Ilya Zisman","Albina Klepach","Andrei Polubarov","Nikita Lyubaykin","Alexander Derevyagin","Igor Kiselev","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2502.17666v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13344v1","updated":"2025-05-19T16:50:26Z","published":"2025-05-19T16:50:26Z","title":"RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE\n  Optimization on Diffusion Transformers","summary":"  We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively.\n","authors":["Ahmet Berke Gokmen","Yigit Ekin","Bahri Batuhan Bilecen","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2505.13344v1.pdf","comment":"https://berkegokmen1.github.io/RoPECraft/"},{"id":"http://arxiv.org/abs/2505.13339v1","updated":"2025-05-19T16:48:14Z","published":"2025-05-19T16:48:14Z","title":"OPA-Pack: Object-Property-Aware Robotic Bin Packing","summary":"  Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios.\n","authors":["Jia-Hui Pan","Yeok Tatt Cheah","Zhengzhe Liu","Ka-Hei Hui","Xiaojie Gao","Pheng-Ann Heng","Yun-Hui Liu","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2505.13339v1.pdf","comment":"Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025"},{"id":"http://arxiv.org/abs/2505.13338v1","updated":"2025-05-19T16:47:46Z","published":"2025-05-19T16:47:46Z","title":"Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data\n  Condensation and Spoken QA Generation","summary":"  Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.\n","authors":["Qiongqiong Wang","Hardik B. Sailor","Tianchi Liu","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2505.13338v1.pdf","comment":"Accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.08140v2","updated":"2025-05-19T16:46:27Z","published":"2025-05-13T00:25:23Z","title":"Lost in Transmission: When and Why LLMs Fail to Reason Globally","summary":"  Despite their many successes, transformer-based large language models (LLMs)\ncontinue to struggle with tasks that require complex reasoning over large parts\nof their input. We argue that these failures arise due to capacity limits on\nthe accurate flow of information within LLMs. To formalize this issue, we\nintroduce the bounded attention prefix oracle (BAPO) model, a new computational\nframework that models bandwidth constraints on attention heads, the mechanism\nfor internal communication in LLMs. We show that several important reasoning\nproblems like graph reachability require high communication bandwidth for BAPOs\nto solve; we call these problems BAPO-hard. Our experiments corroborate our\ntheoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks\nand fail even on relatively small BAPO-hard tasks. BAPOs also reveal another\nbenefit of chain of thought (CoT): we prove that breaking down a task using CoT\ncan turn any BAPO-hard problem into a BAPO-easy one. Our results offer\nprincipled explanations for key LLM failures and suggest directions for\narchitectures and inference methods that mitigate bandwidth limits.\n","authors":["Tobias Schnabel","Kiran Tomlinson","Adith Swaminathan","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2505.08140v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2505.13329v1","updated":"2025-05-19T16:38:06Z","published":"2025-05-19T16:38:06Z","title":"Recommender Systems for Democracy: Toward Adversarial Robustness in\n  Voting Advice Applications","summary":"  Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.\n","authors":["Frédéric Berdoz","Dustin Brunner","Yann Vonlanthen","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2505.13329v1.pdf","comment":"This is the extended version of the paper, accepted at IJCAI 2025"},{"id":"http://arxiv.org/abs/2505.13324v1","updated":"2025-05-19T16:34:36Z","published":"2025-05-19T16:34:36Z","title":"From What Ifs to Insights: Counterfactuals in Causal Inference vs.\n  Explainable AI","summary":"  Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.\n","authors":["Galit Shmueli","David Martens","Jaewon Yoo","Travis Greene"],"pdf_url":"https://arxiv.org/pdf/2505.13324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01893v2","updated":"2025-05-19T16:32:11Z","published":"2024-09-03T13:30:00Z","title":"What are the Essential Factors in Crafting Effective Long Context\n  Multi-Hop Instruction Datasets? Insights and Best Practices","summary":"  Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT.\n","authors":["Zhi Chen","Qiguang Chen","Libo Qin","Qipeng Guo","Haijun Lv","Yicheng Zou","Wanxiang Che","Hang Yan","Kai Chen","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.01893v2.pdf","comment":"ACL 2025 Camera Ready. Code is available at:\n  https://github.com/WowCZ/LongMIT"},{"id":"http://arxiv.org/abs/2505.13316v1","updated":"2025-05-19T16:29:12Z","published":"2025-05-19T16:29:12Z","title":"Denoising Diffusion Probabilistic Model for Point Cloud Compression at\n  Low Bit-Rates","summary":"  Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC.\n","authors":["Gabriele Spadaro","Alberto Presta","Jhony H. Giraldo","Marco Grangetto","Wei Hu","Giuseppe Valenzise","Attilio Fiandrotti","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2505.13316v1.pdf","comment":"6 pages, 5 figures, accepted at ICME 2025"},{"id":"http://arxiv.org/abs/2505.13315v1","updated":"2025-05-19T16:29:07Z","published":"2025-05-19T16:29:07Z","title":"KHRONOS: a Kernel-Based Neural Architecture for Rapid,\n  Resource-Efficient Scientific Computation","summary":"  Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.\n","authors":["Reza T. Batley","Sourav Saha"],"pdf_url":"https://arxiv.org/pdf/2505.13315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13308v1","updated":"2025-05-19T16:26:02Z","published":"2025-05-19T16:26:02Z","title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space","summary":"  Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.\n","authors":["Hengli Li","Chenxi Li","Tong Wu","Xuekai Zhu","Yuxuan Wang","Zhaoxin Yu","Eric Hanchen Jiang","Song-Chun Zhu","Zixia Jia","Ying Nian Wu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13307v1","updated":"2025-05-19T16:25:55Z","published":"2025-05-19T16:25:55Z","title":"RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable\n  and Unmeasurable Capabilities for Chain-of-Thought Reasoning","summary":"  Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.\n","authors":["Qiguang Chen","Libo Qin","Jinhao Liu","Yue Liao","Jiaqi Wang","Jingxuan Zhou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2505.13307v1.pdf","comment":"Manuscript"},{"id":"http://arxiv.org/abs/2505.13292v1","updated":"2025-05-19T16:14:27Z","published":"2025-05-19T16:14:27Z","title":"Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms\n  of AI Systems by Integrating Federated Learning and LLMs","summary":"  In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.\n","authors":["Huaiying Luo","Cheng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.13292v1.pdf","comment":"Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering"},{"id":"http://arxiv.org/abs/2409.17140v2","updated":"2025-05-19T16:12:40Z","published":"2024-09-25T17:58:08Z","title":"AXIS: Efficient Human-Agent-Computer Interaction with API-First\n  LLM-Based Agents","summary":"  Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework that\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Microsoft\nWord demonstrate that AXIS reduces task completion time by 65%-70% and\ncognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared\nto humans. Our work contributes to a new human-agent-computer interaction\n(HACI) framework and explores a fresh UI design principle for application\nproviders to turn applications into agents in the era of LLMs, paving the way\ntowards an agent-centric operating system (Agent OS).\n","authors":["Junting Lu","Zhiyang Zhang","Fangkai Yang","Jue Zhang","Lu Wang","Chao Du","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.17140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13291v1","updated":"2025-05-19T16:11:23Z","published":"2025-05-19T16:11:23Z","title":"TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning\n  Engineering Agents","summary":"  We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.\n","authors":["Yifu Cai","Xinyu Li","Mononito Goswami","Michał Wiliński","Gus Welter","Artur Dubrawski"],"pdf_url":"https://arxiv.org/pdf/2505.13291v1.pdf","comment":"Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors"},{"id":"http://arxiv.org/abs/2505.13287v1","updated":"2025-05-19T16:09:30Z","published":"2025-05-19T16:09:30Z","title":"Level Generation with Quantum Reservoir Computing","summary":"  Reservoir computing is a form of machine learning particularly suited for\ntime series analysis, including forecasting predictions. We take an\nimplementation of \\emph{quantum} reservoir computing that was initially\ndesigned to generate variants of musical scores and adapt it to create levels\nof Super Mario Bros. Motivated by our analysis of these levels, we develop a\nnew Roblox \\textit{obby} where the courses can be generated in real time on\nsuperconducting qubit hardware, and investigate some of the constraints placed\nby such real-time generation.\n","authors":["João S. Ferreira","Pierre Fromholz","Hari Shaji","James R. Wootton"],"pdf_url":"https://arxiv.org/pdf/2505.13287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12613v2","updated":"2025-05-19T16:05:15Z","published":"2024-08-08T12:58:20Z","title":"Artificial Intelligence in Election Campaigns: Perceptions, Penalties,\n  and Implications","summary":"  As political parties around the world experiment with Artificial Intelligence\n(AI) in election campaigns, concerns about deception and manipulation are\nrising. This article examines how the public reacts to different uses of AI in\nelections and the potential consequences for party evaluations and regulatory\npreferences. Across three preregistered studies with over 7,600 American\nrespondents, we identify three categories of AI use -- campaign operations,\nvoter outreach, and deception. While people generally dislike AI in campaigns,\nthey are especially critical of deceptive uses, which they perceive as norm\nviolations. However, parties engaging in AI-enabled deception face no\nsignificant drop in favorability, neither with supporters nor opponents.\nInstead, deceptive AI use increases public support for stricter AI regulation,\nincluding calls for an outright ban on AI development. These findings reveal a\nmisalignment between public disapproval of deceptive AI and the political\nincentives of parties, underscoring the need for targeted regulatory oversight.\nRather than banning AI in elections altogether, regulation should distinguish\nbetween harmful and beneficial applications to avoid stifling democratic\ninnovation.\n","authors":["Andreas Jungherr","Adrian Rauchfleisch","Alexander Wuttke"],"pdf_url":"https://arxiv.org/pdf/2408.12613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13280v1","updated":"2025-05-19T16:04:43Z","published":"2025-05-19T16:04:43Z","title":"FlowPure: Continuous Normalizing Flows for Adversarial Purification","summary":"  Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.\n","authors":["Elias Collaert","Abel Rodríguez","Sander Joos","Lieven Desmet","Vera Rimmer"],"pdf_url":"https://arxiv.org/pdf/2505.13280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09493v2","updated":"2025-05-19T15:57:05Z","published":"2024-09-14T17:40:35Z","title":"Hacking, The Lazy Way: LLM Augmented Pentesting","summary":"  In our research, we introduce a new concept called \"LLM Augmented Pentesting\"\ndemonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field\nof ethical hacking by integrating Large Language Models (LLMs) into penetration\ntesting workflows, leveraging the advanced GPT-4-turbo model. Our approach\nfocuses on overcoming the traditional resistance to automation in penetration\ntesting by employing LLMs to automate specific sub-tasks while ensuring a\ncomprehensive understanding of the overall testing process.\n  Pentest Copilot showcases remarkable proficiency in tasks such as utilizing\ntesting tools, interpreting outputs, and suggesting follow-up actions,\nefficiently bridging the gap between automated systems and human expertise. By\nintegrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token\nusage and enhances decision-making processes, leading to more accurate and\ncontext-aware outputs. Additionally, our implementation of Retrieval-Augmented\nGeneration (RAG) minimizes hallucinations and ensures the tool remains aligned\nwith the latest cybersecurity techniques and knowledge. We also highlight a\nunique infrastructure system that supports in-browser penetration testing,\nproviding a robust platform for cybersecurity professionals. Our findings\ndemonstrate that LLM Augmented Pentesting can not only significantly enhance\ntask completion rates in penetration testing but also effectively addresses\nreal-world challenges, marking a substantial advancement in the cybersecurity\ndomain.\n","authors":["Dhruva Goyal","Sitaraman Subramanian","Aditya Peela","Nisha P. Shetty"],"pdf_url":"https://arxiv.org/pdf/2409.09493v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Nisha P. Shetty has been added as an author as the new version includes work\n  under her supervision, enhancing the research. Significant changes have been\n  made in the methodology, survey, and introduction sections"},{"id":"http://arxiv.org/abs/2505.13273v1","updated":"2025-05-19T15:53:32Z","published":"2025-05-19T15:53:32Z","title":"Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion\n  Models","summary":"  Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.\n","authors":["Lucas Berry","Axel Brando","Wei-Di Chang","Juan Camilo Gamboa Higuera","David Meger"],"pdf_url":"https://arxiv.org/pdf/2505.13273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16707v3","updated":"2025-05-19T15:51:40Z","published":"2024-11-21T19:01:07Z","title":"Enhancing LLMs for Power System Simulations: A Feedback-driven\n  Multi-agent Framework","summary":"  The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond.\n","authors":["Mengshuo Jia","Zeyu Cui","Gabriela Hug"],"pdf_url":"https://arxiv.org/pdf/2411.16707v3.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2409.14679v2","updated":"2025-05-19T15:50:40Z","published":"2024-09-23T03:01:50Z","title":"Quantifying Context Bias in Domain Adaptation for Object Detection","summary":"  Domain adaptation for object detection (DAOD) seeks to transfer a trained\nmodel from a source to a target domain. Various DAOD methods exist, some of\nwhich aim to minimize context bias between foreground-background associations\nin various domains. However, no prior work has studied context bias in DAOD by\nanalyzing changes in background features during adaptation and how context bias\nis represented in different domains. Our research experiment highlights the\npotential usability of context bias in DAOD. We address the problem by varying\nactivation values over different layers of two different trained models,\nDetectron2 and YOLOv11, and by masking the background, both of which impact the\nnumber and quality of detections. We use two synthetic datasets, CARLA and\nVirtual KITTI, and two different versions of real open-source data, Cityscapes\nand KITTI semantic, as separate domains to represent and quantify context bias.\nWe utilize different metrics such as Maximum Mean Discrepancy (MMD) and Maximum\nVariance Discrepancy (MVD) to find the layer-specific conditional probability\nestimates of foreground given manipulated background regions for separate\ndomains. We further analyze foreground-background associations across various\ndataset combinations. We find that state-of-the-art domain adaptation methods\nexhibit some form of context bias and apply a potentially simple way to\nalleviate the context bias achieving improved accuracy (from 51.189 to 53.646\nmAP on Cityscapes foggy validation with 63.207 mAP and 64.233 mAP on Cityscapes\nvalidation respectively). We demonstrate through detailed analysis that\nunderstanding of the context bias can affect DAOD approach and focusing solely\non aligning foreground features is insufficient for effective DAOD.\n","authors":["Hojun Son","Asma Almutairi","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2409.14679v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.13268v1","updated":"2025-05-19T15:47:51Z","published":"2025-05-19T15:47:51Z","title":"Representation of perceived prosodic similarity of conversational\n  feedback","summary":"  Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.\n","authors":["Livia Qian","Carol Figueroa","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2505.13268v1.pdf","comment":"Interspeech 2025"},{"id":"http://arxiv.org/abs/2503.10412v4","updated":"2025-05-19T15:46:38Z","published":"2025-03-13T14:35:47Z","title":"dFLMoE: Decentralized Federated Learning via Mixture of Experts for\n  Medical Data Analysis","summary":"  Federated learning has wide applications in the medical field. It enables\nknowledge sharing among different healthcare institutes while protecting\npatients' privacy. However, existing federated learning systems are typically\ncentralized, requiring clients to upload client-specific knowledge to a central\nserver for aggregation. This centralized approach would integrate the knowledge\nfrom each client into a centralized server, and the knowledge would be already\nundermined during the centralized integration before it reaches back to each\nclient. Besides, the centralized approach also creates a dependency on the\ncentral server, which may affect training stability if the server malfunctions\nor connections are unstable. To address these issues, we propose a\ndecentralized federated learning framework named dFLMoE. In our framework,\nclients directly exchange lightweight head models with each other. After\nexchanging, each client treats both local and received head models as\nindividual experts, and utilizes a client-specific Mixture of Experts (MoE)\napproach to make collective decisions. This design not only reduces the\nknowledge damage with client-specific aggregations but also removes the\ndependency on the central server to enhance the robustness of the framework. We\nvalidate our framework on multiple medical tasks, demonstrating that our method\nevidently outperforms state-of-the-art approaches under both model homogeneity\nand heterogeneity settings.\n","authors":["Luyuan Xie","Tianyu Luan","Wenyuan Cai","Guochen Yan","Zhaoyu Chen","Nan Xi","Yuejian Fang","Qingni Shen","Zhonghai Wu","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.10412v4.pdf","comment":"Accapted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.13264v1","updated":"2025-05-19T15:46:12Z","published":"2025-05-19T15:46:12Z","title":"Net-Zero: A Comparative Study on Neural Network Design for\n  Climate-Economic PDEs Under Uncertainty","summary":"  Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change.\n","authors":["Carlos Rodriguez-Pardo","Louis Daumas","Leonardo Chiani","Massimo Tavoni"],"pdf_url":"https://arxiv.org/pdf/2505.13264v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.16565v2","updated":"2025-05-19T15:45:13Z","published":"2025-02-23T13:12:53Z","title":"The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity\n  Tradeoff in Adaptive Multi-Agent Systems","summary":"  Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making.\n","authors":["Zengqing Wu","Takayuki Ito"],"pdf_url":"https://arxiv.org/pdf/2502.16565v2.pdf","comment":"Source codes are available at\n  https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS"},{"id":"http://arxiv.org/abs/2505.13257v1","updated":"2025-05-19T15:39:48Z","published":"2025-05-19T15:39:48Z","title":"WikiPersonas: What Can We Learn From Personalized Alignment to Famous\n  People?","summary":"  Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.\n","authors":["Zilu Tang","Afra Feyza Akyürek","Ekin Akyürek","Derry Wijaya"],"pdf_url":"https://arxiv.org/pdf/2505.13257v1.pdf","comment":"9 pages, preprint"},{"id":"http://arxiv.org/abs/2505.02306v4","updated":"2025-05-19T15:39:14Z","published":"2025-05-05T01:09:02Z","title":"SafeMate: A Modular RAG-Based Agent for Context-Aware Emergency Guidance","summary":"  Despite the abundance of public safety documents and emergency protocols,\nmost individuals remain ill-equipped to interpret and act on such information\nduring crises. Traditional emergency decision support systems (EDSS) are\ndesigned for professionals and rely heavily on static documents like PDFs or\nSOPs, which are difficult for non-experts to navigate under stress. This gap\nbetween institutional knowledge and public accessibility poses a critical\nbarrier to effective emergency preparedness and response. We introduce\nSafeMate, a retrieval-augmented AI assistant that delivers accurate,\ncontext-aware guidance to general users in both preparedness and active\nemergency scenarios. Built on the Model Context Protocol (MCP), SafeMate\ndynamically routes user queries to tools for document retrieval, checklist\ngeneration, and structured summarization. It uses FAISS with cosine similarity\nto identify relevant content from trusted sources.\n","authors":["Junfeng Jiao","Jihyung Park","Yiming Xu","Kristen Sussman","Lucy Atkinson"],"pdf_url":"https://arxiv.org/pdf/2505.02306v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13253v1","updated":"2025-05-19T15:36:34Z","published":"2025-05-19T15:36:34Z","title":"Composing Dextrous Grasping and In-hand Manipulation via Scoring with a\n  Reinforcement Learning Critic","summary":"  In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects.\n","authors":["Lennart Röstel","Dominik Winkelbauer","Johannes Pitz","Leon Sievers","Berthold Bäuml"],"pdf_url":"https://arxiv.org/pdf/2505.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00975v3","updated":"2025-05-19T15:28:43Z","published":"2025-04-01T17:14:01Z","title":"Resource Allocation for RIS-Assisted CoMP-NOMA Networks using\n  Reinforcement Learning","summary":"  This thesis delves into the forefront of wireless communication by exploring\nthe synergistic integration of three transformative technologies: STAR-RIS,\nCoMP, and NOMA. Driven by the ever-increasing demand for higher data rates,\nimproved spectral efficiency, and expanded coverage in the evolving landscape\nof 6G development, this research investigates the potential of these\ntechnologies to revolutionize future wireless networks.\n  The thesis analyzes the performance gains achievable through strategic\ndeployment of STAR-RIS, focusing on mitigating inter-cell interference,\nenhancing signal strength, and extending coverage to cell-edge users. Resource\nsharing strategies for STAR-RIS elements are explored, optimizing both\ntransmission and reflection functionalities. Analytical frameworks are\ndeveloped to quantify the benefits of STAR-RIS assisted CoMP-NOMA networks\nunder realistic channel conditions, deriving key performance metrics such as\nergodic rates and outage probabilities. Additionally, the research delves into\nenergy-efficient design approaches for CoMP-NOMA networks incorporating RIS,\nproposing novel RIS configurations and optimization algorithms to achieve a\nbalance between performance and energy consumption. Furthermore, the\napplication of Deep Reinforcement Learning (DRL) techniques for intelligent and\nadaptive optimization in aerial RIS-assisted CoMP-NOMA networks is explored,\naiming to maximize network sum rate while meeting user quality of service\nrequirements. Through a comprehensive investigation of these technologies and\ntheir synergistic potential, this thesis contributes valuable insights into the\nfuture of wireless communication, paving the way for the development of more\nefficient, reliable, and sustainable networks capable of meeting the demands of\nour increasingly connected world.\n","authors":["Muhammad Umer","Muhammad Ahmed Mohsin","Huma Ghafoor","Syed Ali Hassan"],"pdf_url":"https://arxiv.org/pdf/2504.00975v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14576v2","updated":"2025-05-19T15:28:35Z","published":"2025-03-18T16:03:59Z","title":"SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in\n  Sequential Social Dilemmas","summary":"  Sequential social dilemmas pose a significant challenge in the field of\nmulti-agent reinforcement learning (MARL), requiring environments that\naccurately reflect the tension between individual and collective interests.\nPrevious benchmarks and environments, such as Melting Pot, provide an\nevaluation protocol that measures generalization to new social partners in\nvarious test scenarios. However, running reinforcement learning algorithms in\ntraditional environments requires substantial computational resources. In this\npaper, we introduce SocialJax, a suite of sequential social dilemma\nenvironments and algorithms implemented in JAX. JAX is a high-performance\nnumerical computing library for Python that enables significant improvements in\noperational efficiency. Our experiments demonstrate that the SocialJax training\npipeline achieves at least 50\\texttimes{} speed-up in real-time performance\ncompared to Melting Pot RLlib baselines. Additionally, we validate the\neffectiveness of baseline algorithms within SocialJax environments. Finally, we\nuse Schelling diagrams to verify the social dilemma properties of these\nenvironments, ensuring that they accurately capture the dynamics of social\ndilemmas.\n","authors":["Zihao Guo","Shuqing Shi","Richard Willis","Tristan Tomilin","Joel Z. Leibo","Yali Du"],"pdf_url":"https://arxiv.org/pdf/2503.14576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13246v1","updated":"2025-05-19T15:28:10Z","published":"2025-05-19T15:28:10Z","title":"Agentic Publications: An LLM-Driven Framework for Interactive Scientific\n  Publishing, Supplementing Traditional Papers with AI-Powered Knowledge\n  Systems","summary":"  The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.\n","authors":["Roberto Pugliese","George Kourousias","Francesco Venier","Grazia Garlatti Costa"],"pdf_url":"https://arxiv.org/pdf/2505.13246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19722v2","updated":"2025-05-19T15:26:21Z","published":"2024-11-29T14:14:59Z","title":"JetFormer: An Autoregressive Generative Model of Raw Images and Text","summary":"  Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.\n","authors":["Michael Tschannen","André Susano Pinto","Alexander Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2411.19722v2.pdf","comment":"ICLR 2025. Code available at\n  https://github.com/google-research/big_vision"},{"id":"http://arxiv.org/abs/2502.07830v2","updated":"2025-05-19T15:22:54Z","published":"2025-02-11T00:11:13Z","title":"Captured by Captions: On Memorization and its Mitigation in CLIP Models","summary":"  Multi-modal models, such as CLIP, have demonstrated strong performance in\naligning visual and textual representations, excelling in tasks like image\nretrieval and zero-shot classification. Despite this success, the mechanisms by\nwhich these models utilize training data, particularly the role of\nmemorization, remain unclear. In uni-modal models, both supervised and\nself-supervised, memorization has been shown to be essential for\ngeneralization. However, it is not well understood how these findings would\napply to CLIP, which incorporates elements from both supervised learning via\ncaptions that provide a supervisory signal similar to labels, and from\nself-supervised learning via the contrastive objective. To bridge this gap in\nunderstanding, we propose a formal definition of memorization in CLIP (CLIPMem)\nand use it to quantify memorization in CLIP models. Our results indicate that\nCLIP's memorization behavior falls between the supervised and self-supervised\nparadigms, with \"mis-captioned\" samples exhibiting highest levels of\nmemorization. Additionally, we find that the text encoder contributes more to\nmemorization than the image encoder, suggesting that mitigation strategies\nshould focus on the text domain. Building on these insights, we propose\nmultiple strategies to reduce memorization while at the same time improving\nutility--something that had not been shown before for traditional learning\nparadigms where reducing memorization typically results in utility decrease.\n","authors":["Wenhao Wang","Adam Dziedzic","Grace C. Kim","Michael Backes","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2502.07830v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.08638v2","updated":"2025-05-19T15:15:46Z","published":"2025-05-13T14:55:31Z","title":"TRAIL: Trace Reasoning and Agentic Issue Localization","summary":"  The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.\n","authors":["Darshan Deshpande","Varun Gangal","Hersh Mehta","Jitin Krishnan","Anand Kannappan","Rebecca Qian"],"pdf_url":"https://arxiv.org/pdf/2505.08638v2.pdf","comment":"Dataset: https://huggingface.co/datasets/PatronusAI/TRAIL"},{"id":"http://arxiv.org/abs/2412.05265v3","updated":"2025-05-19T15:12:39Z","published":"2024-12-06T18:53:49Z","title":"Reinforcement Learning: An Overview","summary":"  This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based methods, policy-based methods, model-based methods, multi-agent RL,\nLLMs and RL, and various other topics (e.g., offline RL, hierarchical RL,\nintrinsic reward).\n","authors":["Kevin Murphy"],"pdf_url":"https://arxiv.org/pdf/2412.05265v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13227v1","updated":"2025-05-19T15:09:23Z","published":"2025-05-19T15:09:23Z","title":"Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis","summary":"  Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.\n","authors":["Tianbao Xie","Jiaqi Deng","Xiaochuan Li","Junlin Yang","Haoyuan Wu","Jixuan Chen","Wenjing Hu","Xinyuan Wang","Yuhui Xu","Zekun Wang","Yiheng Xu","Junli Wang","Doyen Sahoo","Tao Yu","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.13227v1.pdf","comment":"49 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.13211v1","updated":"2025-05-19T14:58:50Z","published":"2025-05-19T14:58:50Z","title":"MAGI-1: Autoregressive Video Generation at Scale","summary":"  We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.\n","authors":["Sand. ai","Hansi Teng","Hongyu Jia","Lei Sun","Lingzhi Li","Maolin Li","Mingqiu Tang","Shuai Han","Tianning Zhang","W. Q. Zhang","Weifeng Luo","Xiaoyang Kang","Yuchen Sun","Yue Cao","Yunpeng Huang","Yutong Lin","Yuxin Fang","Zewei Tao","Zheng Zhang","Zhongshu Wang","Zixun Liu","Dai Shi","Guoli Su","Hanwen Sun","Hong Pan","Jie Wang","Jiexin Sheng","Min Cui","Min Hu","Ming Yan","Shucheng Yin","Siran Zhang","Tingting Liu","Xianping Yin","Xiaoyu Yang","Xin Song","Xuan Hu","Yankai Zhang","Yuqiao Li"],"pdf_url":"https://arxiv.org/pdf/2505.13211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13210v1","updated":"2025-05-19T14:58:44Z","published":"2025-05-19T14:58:44Z","title":"Picturized and Recited with Dialects: A Multimodal Chinese\n  Representation Framework for Sentiment Analysis of Classical Chinese Poetry","summary":"  Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.\n","authors":["Xiaocong Du","Haoyu Pei","Haipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13208v1","updated":"2025-05-19T14:57:53Z","published":"2025-05-19T14:57:53Z","title":"Efficient Generation of Parameterised Quantum Circuits from Large Texts","summary":"  Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.\n","authors":["Colin Krawchuk","Nikhil Khatri","Neil John Ortega","Dimitri Kartsaklis"],"pdf_url":"https://arxiv.org/pdf/2505.13208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13201v1","updated":"2025-05-19T14:54:04Z","published":"2025-05-19T14:54:04Z","title":"MatPredict: a dataset and benchmark for learning material properties of\n  diverse indoor objects","summary":"  Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.\n","authors":["Yuzhen Chen","Hojun Son","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2505.13201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13196v1","updated":"2025-05-19T14:51:40Z","published":"2025-05-19T14:51:40Z","title":"A Physics-Inspired Optimizer: Velocity Regularized Adam","summary":"  We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets.\n","authors":["Pranav Vaidhyanathan","Lucas Schorling","Natalia Ares","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2505.13196v1.pdf","comment":"L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.06192v2","updated":"2025-05-19T14:51:05Z","published":"2025-02-10T06:48:04Z","title":"Right Time to Learn:Promoting Generalization via Bio-inspired Spacing\n  Effect in Knowledge Distillation","summary":"  Knowledge distillation (KD) is a powerful strategy for training deep neural\nnetworks (DNNs). Although it was originally proposed to train a more compact\n\"student\" model from a large \"teacher\" model, many recent efforts have focused\non adapting it to promote generalization of the model itself, such as online KD\nand self KD. Here, we propose an accessible and compatible strategy named\nSpaced KD to improve the effectiveness of both online KD and self KD, in which\nthe student model distills knowledge from a teacher model trained with a space\ninterval ahead. This strategy is inspired by a prominent theory named spacing\neffect in biological learning and memory, positing that appropriate intervals\nbetween learning trials can significantly enhance learning performance. With\nboth theoretical and empirical analyses, we demonstrate that the benefits of\nthe proposed Spaced KD stem from convergence to a flatter loss landscape during\nstochastic gradient descent (SGD). We perform extensive experiments to validate\nthe effectiveness of Spaced KD in improving the learning performance of DNNs\n(e.g., the performance gain is up to 2.31% and 3.34% on Tiny-ImageNet over\nonline KD and self KD, respectively). Our codes have been released on github\nhttps://github.com/SunGL001/Spaced-KD.\n","authors":["Guanglong Sun","Hongwei Yan","Liyuan Wang","Qian Li","Bo Lei","Yi Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.06192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13195v1","updated":"2025-05-19T14:50:44Z","published":"2025-05-19T14:50:44Z","title":"Adversarial Testing in LLMs: Insights into Decision-Making\n  Vulnerabilities","summary":"  As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.\n","authors":["Lili Zhang","Haomiaomiao Wang","Long Cheng","Libao Deng","Tomas Ward"],"pdf_url":"https://arxiv.org/pdf/2505.13195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13192v1","updated":"2025-05-19T14:49:10Z","published":"2025-05-19T14:49:10Z","title":"True Zero-Shot Inference of Dynamical Systems Preserving Long-Term\n  Statistics","summary":"  Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.\n","authors":["Christoph Jürgen Hemmer","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2505.13192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13191v1","updated":"2025-05-19T14:48:36Z","published":"2025-05-19T14:48:36Z","title":"Emergence of Fixational and Saccadic Movements in a Multi-Level\n  Recurrent Attention Model for Vision","summary":"  Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks.\n","authors":["Pengcheng Pan","Yonekura Shogo","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2505.13191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13188v1","updated":"2025-05-19T14:45:58Z","published":"2025-05-19T14:45:58Z","title":"When a Reinforcement Learning Agent Encounters Unknown Unknowns","summary":"  An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost.\n","authors":["Juntian Zhu","Miguel de Carvalho","Zhouwang Yang","Fengxiang He"],"pdf_url":"https://arxiv.org/pdf/2505.13188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20565v2","updated":"2025-05-19T14:40:29Z","published":"2025-02-27T22:07:16Z","title":"DPZV: Elevating the Tradeoff between Privacy and Utility in Zeroth-Order\n  Vertical Federated Learning","summary":"  Vertical Federated Learning (VFL) enables collaborative training with\nfeature-partitioned data, yet remains vulnerable to privacy leakage through\ngradient transmissions. Standard differential privacy (DP) techniques such as\nDP-SGD are difficult to apply in this setting due to VFL's distributed nature\nand the high variance incurred by vector-valued noise. On the other hand,\nzeroth-order (ZO) optimization techniques can avoid explicit gradient exposure\nbut lack formal privacy guarantees. In this work, we propose DPZV, the first ZO\noptimization framework for VFL that achieves tunable DP with performance\nguarantees. DPZV overcomes these limitations by injecting low-variance scalar\nnoise at the server, enabling controllable privacy with reduced memory\noverhead. We conduct a comprehensive theoretical analysis showing that DPZV\nmatches the convergence rate of first-order optimization methods while\nsatisfying formal ($\\epsilon, \\delta$)-DP guarantees. Experiments on image and\nlanguage benchmarks demonstrate that DPZV outperforms several baselines in\nterms of accuracy under a wide range of privacy constraints ($\\epsilon \\le\n10$), thereby elevating the privacy-utility tradeoff in VFL.\n","authors":["Jianing Zhang","Evan Chen","Chaoyue Liu","Christopher G. Brinton"],"pdf_url":"https://arxiv.org/pdf/2502.20565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13182v1","updated":"2025-05-19T14:39:41Z","published":"2025-05-19T14:39:41Z","title":"Information Science Principles of Machine Learning: A Causal Chain\n  Meta-Framework Based on Formalized Information Mapping","summary":"  [Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning.\n","authors":["Jianfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2505.13182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13180v1","updated":"2025-05-19T14:38:15Z","published":"2025-05-19T14:38:15Z","title":"ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models","summary":"  Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning.\n","authors":["Matteo Merler","Nicola Dainese","Minttu Alakuijala","Giovanni Bonetta","Pietro Ferrazzi","Yu Tian","Bernardo Magnini","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2505.13180v1.pdf","comment":"9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures\n  and 16 tables including supplementary material"},{"id":"http://arxiv.org/abs/2505.13176v1","updated":"2025-05-19T14:30:46Z","published":"2025-05-19T14:30:46Z","title":"ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models","summary":"  While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.\n","authors":["Zihao Cheng","Hongru Wang","Zeming Liu","Yuhang Guo","Yuanfang Guo","Yunhong Wang","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13176v1.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.13175v1","updated":"2025-05-19T14:30:41Z","published":"2025-05-19T14:30:41Z","title":"Enhancing LLMs for Time Series Forecasting via Structure-Guided\n  Cross-Modal Alignment","summary":"  The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.\n","authors":["Siming Sun","Kai Zhang","Xuejun Jiang","Wenchao Meng","Qinmin Yang"],"pdf_url":"https://arxiv.org/pdf/2505.13175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10849v2","updated":"2025-05-19T14:26:07Z","published":"2024-12-14T14:46:18Z","title":"Superhuman performance of a large language model on the reasoning tasks\n  of a physician","summary":"  A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials.\n","authors":["Peter G. Brodeur","Thomas A. Buckley","Zahir Kanjee","Ethan Goh","Evelyn Bin Ling","Priyank Jain","Stephanie Cabral","Raja-Elie Abdulnour","Adrian D. Haimovich","Jason A. Freed","Andrew Olson","Daniel J. Morgan","Jason Hom","Robert Gallo","Liam G. McCoy","Haadi Mombini","Christopher Lucas","Misha Fotoohi","Matthew Gwiazdon","Daniele Restifo","Daniel Restrepo","Eric Horvitz","Jonathan Chen","Arjun K. Manrai","Adam Rodman"],"pdf_url":"https://arxiv.org/pdf/2412.10849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21706v2","updated":"2025-05-19T14:20:35Z","published":"2025-04-30T14:50:02Z","title":"Vision Transformers in Precision Agriculture: A Comprehensive Survey","summary":"  Detecting plant diseases is a crucial aspect of modern agriculture, as it\nplays a key role in maintaining crop health and increasing overall yield.\nTraditional approaches, though still valuable, often rely on manual inspection\nor conventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering advantages such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This review\nexplores the application of ViTs in precision agriculture, covering a range of\ntasks. We begin by introducing the foundational architecture of ViTs and\ndiscussing their transition from Natural Language Processing (NLP) to Computer\nVision. The discussion includes the concept of inductive bias in traditional\nmodels like Convolutional Neural Networks (CNNs), and how ViTs mitigate these\nbiases. We provide a comprehensive review of recent literature, focusing on key\nmethodologies, datasets, and performance metrics. This study also includes a\ncomparative analysis of CNNs and ViTs, along with a review of hybrid models and\nperformance enhancements. Technical challenges such as data requirements,\ncomputational demands, and model interpretability are addressed, along with\npotential solutions. Finally, we outline future research directions and\ntechnological advancements that could further support the integration of ViTs\nin real-world agricultural settings. Our goal with this study is to offer\npractitioners and researchers a deeper understanding of how ViTs are poised to\ntransform smart and precision agriculture.\n","authors":["Saber Mehdipour","Seyed Abolghasem Mirroshandel","Seyed Amirhossein Tabatabaei"],"pdf_url":"https://arxiv.org/pdf/2504.21706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13157v1","updated":"2025-05-19T14:18:16Z","published":"2025-05-19T14:18:16Z","title":"Role-Playing Evaluation for Large Language Models","summary":"  Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval\n","authors":["Yassine El Boudouri","Walter Nuninger","Julian Alvarez","Yvan Peter"],"pdf_url":"https://arxiv.org/pdf/2505.13157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13156v1","updated":"2025-05-19T14:17:37Z","published":"2025-05-19T14:17:37Z","title":"Tianyi: A Traditional Chinese Medicine all-rounder language model and\n  its Real-World Clinical Practice","summary":"  Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.\n","authors":["Zhi Liu","Tao Yang","Jing Wang","Yexin Chen","Zhan Gao","Jiaxi Yang","Kui Chen","Bingji Lu","Xiaochen Li","Changyong Luo","Yan Li","Xiaohong Gu","Peng Cao"],"pdf_url":"https://arxiv.org/pdf/2505.13156v1.pdf","comment":"23 pages, 4 figures, and 1 tables"},{"id":"http://arxiv.org/abs/2501.04568v2","updated":"2025-05-19T14:15:00Z","published":"2025-01-08T15:32:12Z","title":"Feedback-Driven Vision-Language Alignment with Minimal Human Supervision","summary":"  Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Sampling-based Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on manually curated text-image pairs or preference annotation. SVP\nleverages a small set of manually selected images, self-captioning and a\npre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14 %\naverage improvement in captioning tasks, up to 12 % increase in object recall,\nand significantly reduced hallucinations, while maintaining question-answering\ncapabilities. Using SVP, a small VLM achieves hallucination reductions similar\nto a model five times larger, while a VLM with initially poor referring\ncapabilities more than doubles its performance, approaching parity with a model\ntwice its size.\n","authors":["Giorgio Giannone","Ruoteng Li","Qianli Feng","Evgeny Perevodchikov","Rui Chen","Aleix Martinez"],"pdf_url":"https://arxiv.org/pdf/2501.04568v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.13144v1","updated":"2025-05-19T14:11:14Z","published":"2025-05-19T14:11:14Z","title":"Temporal Distance-aware Transition Augmentation for Offline Model-based\n  Reinforcement Learning","summary":"  The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.\n","authors":["Dongsu Lee","Minhae Kwon"],"pdf_url":"https://arxiv.org/pdf/2505.13144v1.pdf","comment":"2025 ICML"},{"id":"http://arxiv.org/abs/2502.11799v2","updated":"2025-05-19T14:10:55Z","published":"2025-02-17T13:42:12Z","title":"Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning","summary":"  Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.\n","authors":["Peiying Yu","Guoxin Chen","Jingjing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11799v2.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2504.14094v2","updated":"2025-05-19T14:09:26Z","published":"2025-04-18T22:21:06Z","title":"Leakage and Interpretability in Concept-Based Models","summary":"  Concept Bottleneck Models aim to improve interpretability by predicting\nhigh-level intermediate concepts, representing a promising approach for\ndeployment in high-risk scenarios. However, they are known to suffer from\ninformation leakage, whereby models exploit unintended information encoded\nwithin the learned concepts. We introduce an information-theoretic framework to\nrigorously characterise and quantify leakage, and define two complementary\nmeasures: the concepts-task leakage (CTL) and interconcept leakage (ICL)\nscores. We show that these measures are strongly predictive of model behaviour\nunder interventions and outperform existing alternatives in robustness and\nreliability. Using this framework, we identify the primary causes of leakage\nand provide strong evidence that Concept Embedding Models exhibit substantial\nleakage regardless of the hyperparameters choice. Finally, we propose practical\nguidelines for designing concept-based models to reduce leakage and ensure\ninterpretability.\n","authors":["Enrico Parisini","Tapabrata Chakraborti","Chris Harbron","Ben D. MacArthur","Christopher R. S. Banerji"],"pdf_url":"https://arxiv.org/pdf/2504.14094v2.pdf","comment":"35 pages, 24 figures"},{"id":"http://arxiv.org/abs/2505.13136v1","updated":"2025-05-19T14:07:20Z","published":"2025-05-19T14:07:20Z","title":"ModernGBERT: German-only 1B Encoder Model Trained from Scratch","summary":"  Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.\n","authors":["Anton Ehrmanntraut","Julia Wunderle","Jan Pfister","Fotis Jannidis","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2505.13136v1.pdf","comment":"under review @ARR"},{"id":"http://arxiv.org/abs/2406.02613v2","updated":"2025-05-19T14:02:01Z","published":"2024-06-03T08:23:45Z","title":"ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training","summary":"  Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (\\acco), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, \\acco~reduces GPU\nidle time and supports heterogeneous hardware. To mitigate the convergence\nissues caused by delayed updates, we introduce a novel technique ensuring\ntraining dynamics align with standard distributed optimization. Compared to\nZeRO-1, our approach is significantly faster and scales effectively across\nheterogeneous hardware.\n","authors":["Adel Nabli","Louis Fournier","Pierre Erbacher","Louis Serrano","Eugene Belilovsky","Edouard Oyallon"],"pdf_url":"https://arxiv.org/pdf/2406.02613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08293v2","updated":"2025-05-19T14:01:45Z","published":"2025-05-13T07:16:58Z","title":"M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human\n  Motion Synthesis","summary":"  Generating full-body human gestures encompassing face, body, hands, and\nglobal movements from audio is a valuable yet challenging task in virtual\navatar creation. Previous systems focused on tokenizing the human gestures\nframewisely and predicting the tokens of each frame from the input audio.\nHowever, one observation is that the number of frames required for a complete\nexpressive human gesture, defined as granularity, varies among different human\ngesture patterns. Existing systems fail to model these gesture patterns due to\nthe fixed granularity of their gesture tokens. To solve this problem, we\npropose a novel framework named Multi-Granular Gesture Generator (M3G) for\naudio-driven holistic gesture generation. In M3G, we propose a novel\nMulti-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct\nmotion sequences from different temporal granularities. Subsequently, we\nproposed a multi-granular token predictor that extracts multi-granular\ninformation from audio and predicts the corresponding motion tokens. Then M3G\nreconstructs the human gestures from the predicted tokens using the MGVQ-VAE.\nBoth objective and subjective experiments demonstrate that our proposed M3G\nframework outperforms the state-of-the-art methods in terms of generating\nnatural and expressive full-body human gestures.\n","authors":["Zhizhuo Yin","Yuk Hang Tsui","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2505.08293v2.pdf","comment":"9 Pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.07610v2","updated":"2025-05-19T14:00:52Z","published":"2025-05-12T14:31:51Z","title":"Concept-Level Explainability for Auditing & Steering LLM Responses","summary":"  As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior.\n","authors":["Kenza Amara","Rita Sevastjanova","Mennatallah El-Assady"],"pdf_url":"https://arxiv.org/pdf/2505.07610v2.pdf","comment":"9 pages, 7 figures, Submission to Neurips 2025"},{"id":"http://arxiv.org/abs/2505.13130v1","updated":"2025-05-19T14:00:10Z","published":"2025-05-19T14:00:10Z","title":"Adaptive Image Restoration for Video Surveillance: A Real-Time Approach","summary":"  One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable.\n","authors":["Muhammad Awais Amin","Adama Ilboudo","Abdul Samad bin Shahid","Amjad Ali","Waqas Haider Khan Bangyal"],"pdf_url":"https://arxiv.org/pdf/2505.13130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13124v1","updated":"2025-05-19T13:54:29Z","published":"2025-05-19T13:54:29Z","title":"$μ$PC: Scaling Predictive Coding to 100+ Layer Networks","summary":"  The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024).\n","authors":["Francesco Innocenti","El Mehdi Achour","Christopher L. Buckley"],"pdf_url":"https://arxiv.org/pdf/2505.13124v1.pdf","comment":"34 pages, 41 figures"},{"id":"http://arxiv.org/abs/2505.13123v1","updated":"2025-05-19T13:51:57Z","published":"2025-05-19T13:51:57Z","title":"Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video\n  Anomaly Detection","summary":"  Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference.\n","authors":["Snehashis Majhi","Giacomo D'Amicantonio","Antitza Dantcheva","Quan Kong","Lorenzo Garattoni","Gianpiero Francesca","Egor Bondarev","Francois Bremond"],"pdf_url":"https://arxiv.org/pdf/2505.13123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13122v1","updated":"2025-05-19T13:51:49Z","published":"2025-05-19T13:51:49Z","title":"When majority rules, minority loses: bias amplification of gradient\n  descent","summary":"  Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks.\n","authors":["François Bachoc","Jérôme Bolte","Ryan Boustany","Jean-Michel Loubes"],"pdf_url":"https://arxiv.org/pdf/2505.13122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13118v1","updated":"2025-05-19T13:49:05Z","published":"2025-05-19T13:49:05Z","title":"Unveil Sources of Uncertainty: Feature Contribution to Conformal\n  Prediction Intervals","summary":"  Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications.\n","authors":["Marouane Il Idrissi","Agathe Fernandes Machado","Ewen Gallic","Arthur Charpentier"],"pdf_url":"https://arxiv.org/pdf/2505.13118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13116v1","updated":"2025-05-19T13:46:47Z","published":"2025-05-19T13:46:47Z","title":"Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced\n  Data","summary":"  As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms.\n","authors":["Kathrin Lammers","Valerie Vaquet","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2505.13116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13115v1","updated":"2025-05-19T13:46:35Z","published":"2025-05-19T13:46:35Z","title":"Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning","summary":"  The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.\n","authors":["Debarpan Bhattacharya","Apoorva Kulkarni","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2505.13115v1.pdf","comment":"Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands"},{"id":"http://arxiv.org/abs/2504.19627v2","updated":"2025-05-19T13:44:04Z","published":"2025-04-28T09:39:07Z","title":"VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with\n  Vision-Language Instruction Fine-Tuning","summary":"  Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM.\n","authors":["Run Luo","Renke Shan","Longze Chen","Ziqiang Liu","Lu Wang","Min Yang","Xiaobo Xia"],"pdf_url":"https://arxiv.org/pdf/2504.19627v2.pdf","comment":"VCM"},{"id":"http://arxiv.org/abs/2503.03008v2","updated":"2025-05-19T13:39:47Z","published":"2025-03-04T21:08:17Z","title":"MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings","summary":"  Deploying language models often requires navigating accuracy vs. performance\ntrade-offs to meet latency constraints while preserving utility. Traditional\nmodel distillation reduces size but incurs substantial costs through training\nseparate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter\nmulti-exit encoder for code retrieval and classification that employs a novel\nSelf-Distillation mechanism. This approach significantly enhances lower-layer\nrepresentations, enabling flexible deployment of different model portions with\nfavorable performance trade-offs. Our architecture improves text-to-code and\ncode-to-code search by targeting specific encoder layers as exit heads, where\nhigher layers guide earlier ones during training-improving intermediate\nrepresentations at minimal additional cost. We further enhance MoSE with a\nrepository-level contextual loss that maximizes training context window\nutilization. Additionally, we release a new dataset created through code\ntranslation that extends text-to-code benchmarks with cross-language\ncode-to-code pairs. Evaluations demonstrate the effectiveness of\nSelf-Distillation as a principled approach to trading inference cost for\naccuracy across various code understanding tasks.\n","authors":["Andrea Gurioli","Federico Pennino","João Monteiro","Maurizio Gabbrielli"],"pdf_url":"https://arxiv.org/pdf/2503.03008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13109v1","updated":"2025-05-19T13:36:45Z","published":"2025-05-19T13:36:45Z","title":"FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference","summary":"  Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.\n","authors":["Guangda Liu","Chengwei Li","Zhenyu Ning","Jing Lin","Yiwu Yao","Danning Ke","Minyi Guo","Jieru Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.13109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13102v1","updated":"2025-05-19T13:32:34Z","published":"2025-05-19T13:32:34Z","title":"Lightweight Transformer via Unrolling of Mixed Graph Algorithms for\n  Traffic Forecast","summary":"  To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast.\n","authors":["Ji Qi","Tam Thuc Do","Mingxiao Liu","Zhuoshi Pan","Yuzhe Li","Gene Cheung","H. Vicky Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.13102v1.pdf","comment":"19 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2505.13101v1","updated":"2025-05-19T13:31:48Z","published":"2025-05-19T13:31:48Z","title":"ARIW-Framework: Adaptive Robust Iterative Watermarking Framework","summary":"  With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks.\n","authors":["Shaowu Wu","Liting Zeng","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2505.13101v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.13098v1","updated":"2025-05-19T13:29:27Z","published":"2025-05-19T13:29:27Z","title":"LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the\n  Ocean of LLMs","summary":"  Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.\n","authors":["Lars-Peter Meyer","Johannes Frey","Desiree Heim","Felix Brei","Claus Stadler","Kurt Junghanns","Michael Martin"],"pdf_url":"https://arxiv.org/pdf/2505.13098v1.pdf","comment":"Peer reviewed publication at ESWC 2025 Resources Track"},{"id":"http://arxiv.org/abs/2505.13094v1","updated":"2025-05-19T13:25:51Z","published":"2025-05-19T13:25:51Z","title":"Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation","summary":"  Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.\n","authors":["Guo Chen","Kai Li","Runxuan Yang","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2505.13094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18710v3","updated":"2025-05-19T13:22:26Z","published":"2024-10-23T07:55:40Z","title":"Uncovering the Genetic Basis of Glioblastoma Heterogeneity through\n  Multimodal Analysis of Whole Slide Images and RNA Sequencing Data","summary":"  Glioblastoma is a highly aggressive form of brain cancer characterized by\nrapid progression and poor prognosis. Despite advances in treatment, the\nunderlying genetic mechanisms driving this aggressiveness remain poorly\nunderstood. In this study, we employed multimodal deep learning approaches to\ninvestigate glioblastoma heterogeneity using joint image/RNA-seq analysis. Our\nresults reveal novel genes associated with glioblastoma. By leveraging a\ncombination of whole-slide images and RNA-seq, as well as introducing novel\nmethods to encode RNA-seq data, we identified specific genetic profiles that\nmay explain different patterns of glioblastoma progression. These findings\nprovide new insights into the genetic mechanisms underlying glioblastoma\nheterogeneity and highlight potential targets for therapeutic intervention.\nCode and data downloading instructions are available at:\nhttps://github.com/ma3oun/gbheterogeneity.\n","authors":["Ahmad Berjaoui","Louis Roussel","Eduardo Hugo Sanchez","Elizabeth Cohen-Jonathan Moyal"],"pdf_url":"https://arxiv.org/pdf/2410.18710v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13087v1","updated":"2025-05-19T13:22:17Z","published":"2025-05-19T13:22:17Z","title":"Graph Alignment for Benchmarking Graph Neural Networks and Learning\n  Positional Encodings","summary":"  We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures.\n","authors":["Adrien Lagesse","Marc Lelarge"],"pdf_url":"https://arxiv.org/pdf/2505.13087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14888v2","updated":"2025-05-19T13:20:34Z","published":"2025-02-16T14:51:07Z","title":"Multi-Faceted Multimodal Monosemanticity","summary":"  Humans experience the world through multiple modalities, such as, vision,\nlanguage, and speech, making it natural to explore the commonality and\ndistinctions among them. In this work, we take a data-driven approach to\naddress this question by analyzing interpretable, monosemantic features\nextracted from deep multimodal models. Specifically, we investigate CLIP, a\nprominent visual-language representation model trained on massive image-text\npairs. Building on prior research in single-modal interpretability, we develop\na set of multi-modal interpretability tools and measures designed to\ndisentangle and analyze features learned from CLIP. Specifically, we introduce\nthe Modality Dominance Score (MDS) to attribute each CLIP feature to a specific\nmodality. We then map CLIP features into a more interpretable space, enabling\nus to categorize them into three distinct classes: vision features\n(single-modal), language features (single-modal), and visual-language features\n(cross-modal). Interestingly, this data-driven categorization closely aligns\nwith human intuitive understandings of different modalities. We further show\nthat this modality decomposition can benefit multiple downstream tasks,\nincluding reducing bias in gender detection, generating cross-modal adversarial\nexamples, and enabling modal-specific feature control in text-to-image\ngeneration. These results indicate that large-scale multimodal models, when\nequipped with task-agnostic interpretability tools, can offer valuable insights\ninto the relationships between different data modalities.\n","authors":["Hanqi Yan","Xiangxiang Cui","Lu Yin","Paul Pu Liang","Yulan He","Yifei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13082v1","updated":"2025-05-19T13:13:46Z","published":"2025-05-19T13:13:46Z","title":"MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and\n  Voices of Multiple Speakers","summary":"  We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies.\n","authors":["Kyeongman Park","Seongho Joo","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2505.13082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13079v1","updated":"2025-05-19T13:13:18Z","published":"2025-05-19T13:13:18Z","title":"Cross-modal Knowledge Transfer Learning as Graph Matching Based on\n  Optimal Transport for ASR","summary":"  Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach.\n","authors":["Xugang Lu","Peng Shen","Yu Tsao","Hisashi Kawai"],"pdf_url":"https://arxiv.org/pdf/2505.13079v1.pdf","comment":"To appear in Interspeech 2025"},{"id":"http://arxiv.org/abs/2504.21582v2","updated":"2025-05-19T13:12:36Z","published":"2025-04-30T12:41:51Z","title":"MF-LLM: Simulating Population Decision Dynamics via a Mean-Field Large\n  Language Model Framework","summary":"  Simulating collective decision-making involves more than aggregating\nindividual behaviors; it emerges from dynamic interactions among individuals.\nWhile large language models (LLMs) offer strong potential for social\nsimulation, achieving quantitative alignment with real-world data remains a key\nchallenge. To bridge this gap, we propose the Mean-Field LLM (MF-LLM)\nframework, the first to incorporate mean field theory into LLM-based social\nsimulation. MF-LLM models bidirectional interactions between individuals and\nthe population through an iterative process, generating population signals to\nguide individual decisions, which in turn update the signals. This interplay\nproduces coherent trajectories of collective behavior. To improve alignment\nwith real-world data, we introduce IB-Tune, a novel fine-tuning method inspired\nby the Information Bottleneck principle, which retains population signals most\npredictive of future actions while filtering redundant history. Evaluated on a\nreal-world social dataset, MF-LLM reduces KL divergence to human population\ndistributions by 47\\% compared to non-mean-field baselines, enabling accurate\ntrend forecasting and effective intervention planning. Generalizing across 7\ndomains and 4 LLM backbones, MF-LLM provides a scalable, high-fidelity\nfoundation for social simulation.\n","authors":["Qirui Mi","Mengyue Yang","Xiangning Yu","Zhiyu Zhao","Cheng Deng","Bo An","Haifeng Zhang","Xu Chen","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2504.21582v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.13077v1","updated":"2025-05-19T13:11:28Z","published":"2025-05-19T13:11:28Z","title":"Advancing Sequential Numerical Prediction in Autoregressive Models","summary":"  Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.\n","authors":["Xiang Fei","Jinghui Lu","Qi Sun","Hao Feng","Yanjie Wang","Wei Shi","An-Lan Wang","Jingqun Tang","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2505.13077v1.pdf","comment":"Accepted to ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2505.13076v1","updated":"2025-05-19T13:10:29Z","published":"2025-05-19T13:10:29Z","title":"The Hidden Dangers of Browsing AI Agents","summary":"  Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.\n","authors":["Mykyta Mudryi","Markiyan Chaklosh","Grzegorz Wójcik"],"pdf_url":"https://arxiv.org/pdf/2505.13076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13073v1","updated":"2025-05-19T13:09:32Z","published":"2025-05-19T13:09:32Z","title":"Structure-Aware Corpus Construction and User-Perception-Aligned Metrics\n  for Large-Language-Model Code Completion","summary":"  Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance.\n","authors":["Dengfeng Liu","Jucai Zhai","Xiaoguang Jiang","Ziqun Li","Qianjin Yu","Feng Liu","Rui Ye","Huang Liu","Zhiguo Yang","Yongsheng Du","Fang Tan"],"pdf_url":"https://arxiv.org/pdf/2505.13073v1.pdf","comment":"14 pages,8 figures"},{"id":"http://arxiv.org/abs/2309.12132v2","updated":"2025-05-19T13:05:53Z","published":"2023-09-21T14:53:36Z","title":"Automating construction contract review using knowledge graph-enhanced\n  large language models","summary":"  An effective and efficient review of construction contracts is essential for\nminimizing construction projects losses, but current methods are time-consuming\nand error-prone. Studies using methods based on Natural Language Processing\n(NLP) exist, but their scope is often limited to text classification or\nsegmented label prediction. This paper investigates whether integrating Large\nLanguage Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and\ninterpretability of automated contract risk identification. A tuning-free\napproach is proposed that integrates LLMs with a Nested Contract Knowledge\nGraph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework\nfor contract knowledge retrieval and reasoning. Tested on international EPC\ncontracts, the method achieves more accurate risk evaluation and interpretable\nrisk summaries than baseline models. These findings demonstrate the potential\nof combining LLMs and KGs for reliable reasoning in tasks that are\nknowledge-intensive and specialized, such as contract review.\n","authors":["Chunmo Zheng","Saika Wong","Xing Su","Yinqiu Tang","Ahsan Nawaz","Mohamad Kassem"],"pdf_url":"https://arxiv.org/pdf/2309.12132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11496v2","updated":"2025-05-19T12:46:41Z","published":"2025-01-20T14:03:40Z","title":"Generative AI and Large Language Models in Language Preservation:\n  Opportunities and Challenges","summary":"  The global crisis of language endangerment meets a technological turning\npoint as Generative AI (GenAI) and Large Language Models (LLMs) unlock new\nfrontiers in automating corpus creation, transcription, translation, and\ntutoring. However, this promise is imperiled by fragmented practices and the\ncritical lack of a methodology to navigate the fraught balance between LLM\ncapabilities and the profound risks of data scarcity, cultural\nmisappropriation, and ethical missteps. This paper introduces a novel\nanalytical framework that systematically evaluates GenAI applications against\nlanguage-specific needs, embedding community governance and ethical safeguards\nas foundational pillars. We demonstrate its efficacy through the Te Reo M\\=aori\nrevitalization, where it illuminates successes, such as community-led Automatic\nSpeech Recognition achieving 92% accuracy, while critically surfacing\npersistent challenges in data sovereignty and model bias for digital archives\nand educational tools. Our findings underscore that GenAI can indeed\nrevolutionize language preservation, but only when interventions are rigorously\nanchored in community-centric data stewardship, continuous evaluation, and\ntransparent risk management. Ultimately, this framework provides an\nindispensable toolkit for researchers, language communities, and policymakers,\naiming to catalyze the ethical and high-impact deployment of LLMs to safeguard\nthe world's linguistic heritage.\n","authors":["Vincent Koc"],"pdf_url":"https://arxiv.org/pdf/2501.11496v2.pdf","comment":"9 pages, 3 figures, 2 tables, submitted for IEEE publication.\n  Pre-print updated as part of review process"},{"id":"http://arxiv.org/abs/2502.20321v2","updated":"2025-05-19T12:45:03Z","published":"2025-02-27T17:47:01Z","title":"UniTok: A Unified Tokenizer for Visual Generation and Understanding","summary":"  Visual generative and understanding models typically rely on distinct\ntokenizers to process images, presenting a key challenge for unifying them\nwithin a single framework. Recent studies attempt to address this by connecting\nthe training of VQVAE (for autoregressive generation) and CLIP (for\nunderstanding) to build a unified tokenizer. However, directly combining these\ntraining objectives has been observed to cause severe loss conflicts. In this\npaper, we show that reconstruction and semantic supervision do not inherently\nconflict. Instead, the underlying bottleneck stems from limited\nrepresentational capacity of discrete token space. Building on these insights,\nwe introduce UniTok, a unified tokenizer featuring a novel multi-codebook\nquantization mechanism that effectively scales up the vocabulary size and\nbottleneck dimension. In terms of final performance, UniTok sets a new record\nof 0.38 rFID and 78.6% zero-shot accuracy on ImageNet. Besides, UniTok can be\nseamlessly integrated into MLLMs to unlock native visual generation capability,\nwithout compromising the understanding performance. Additionally, we show that\nUniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet\n256$\\times$256 benchmark. GitHub: https://github.com/FoundationVision/UniTok.\n","authors":["Chuofan Ma","Yi Jiang","Junfeng Wu","Jihan Yang","Xin Yu","Zehuan Yuan","Bingyue Peng","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2502.20321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13053v1","updated":"2025-05-19T12:42:23Z","published":"2025-05-19T12:42:23Z","title":"SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive\n  Explanation Generation","summary":"  Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.\n","authors":["Amelie S. Robrecht","Christoph R. Kowalski","Stefan Kopp"],"pdf_url":"https://arxiv.org/pdf/2505.13053v1.pdf","comment":"currently under review at Frontiers in Communication"},{"id":"http://arxiv.org/abs/2503.21729v3","updated":"2025-05-19T12:40:17Z","published":"2025-03-27T17:44:18Z","title":"ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation","summary":"  Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).\n","authors":["Zhicheng Lee","Shulin Cao","Jinxin Liu","Jiajie Zhang","Weichuan Liu","Xiaoyin Che","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2503.21729v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01906v2","updated":"2025-05-19T12:36:10Z","published":"2024-05-03T08:00:19Z","title":"Instance-Conditioned Adaptation for Large-scale Generalization of Neural\n  Routing Solver","summary":"  The neural combinatorial optimization (NCO) method has shown great potential\nfor solving routing problems of intelligent transportation systems without\nrequiring expert knowledge. However, existing constructive NCO methods still\nstruggle to solve large-scale instances, which significantly limits their\napplication prospects. To address these crucial shortcomings, this work\nproposes a novel Instance-Conditioned Adaptation Model (ICAM) for better\nlarge-scale generalization of neural routing solvers. In particular, we design\na simple yet efficient instance-conditioned adaptation function to\nsignificantly improve the generalization performance of existing NCO models\nwith a small time and memory overhead. In addition, with a systematic\ninvestigation on the performance of information incorporation between different\nattention mechanisms, we further propose a powerful yet low-complexity\ninstance-conditioned adaptation module to generate better solutions for\ninstances across different scales. Extensive experimental results on both\nsynthetic and benchmark instances show that our proposed method is capable of\nobtaining promising results with a very fast inference time in solving\nlarge-scale Traveling Salesman Problems (TSPs), Capacitated Vehicle Routing\nProblems (CVRPs), and Asymmetric Traveling Salesman Problems (ATSPs). Our code\nis available at https://github.com/CIAM-Group/ICAM.\n","authors":["Changliang Zhou","Xi Lin","Zhenkun Wang","Xialiang Tong","Mingxuan Yuan","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.01906v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.16348v2","updated":"2025-05-19T12:35:35Z","published":"2025-03-20T17:05:16Z","title":"Palatable Conceptions of Disembodied Being: Terra Incognita in the Space\n  of Possible Minds","summary":"  Is it possible to articulate a conception of consciousness that is compatible\nwith the exotic characteristics of contemporary, disembodied AI systems, and\nthat can stand up to philosophical scrutiny? How would subjective time and\nselfhood show up for an entity that conformed to such a conception? Trying to\nanswer these questions, even metaphorically, stretches the language of\nconsciousness to breaking point. Ultimately, the attempt yields something like\nemptiness, in the Buddhist sense, and helps to undermine our dualistic\ninclinations towards subjectivity and selfhood.\n","authors":["Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2503.16348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13043v1","updated":"2025-05-19T12:33:52Z","published":"2025-05-19T12:33:52Z","title":"A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation","summary":"  Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method.\n","authors":["Hao-Ran Yang","Xiaohui Chen","Chuan-Xian Ren"],"pdf_url":"https://arxiv.org/pdf/2505.13043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13044v1","updated":"2025-05-19T12:33:52Z","published":"2025-05-19T12:33:52Z","title":"CAIM: Development and Evaluation of a Cognitive AI Memory Framework for\n  Long-Term Interaction with Intelligent Agents","summary":"  Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions.\n","authors":["Rebecca Westhäußer","Frederik Berenz","Wolfgang Minker","Sebastian Zepf"],"pdf_url":"https://arxiv.org/pdf/2505.13044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13377v2","updated":"2025-05-19T12:29:10Z","published":"2024-12-17T23:25:47Z","title":"DateLogicQA: Benchmarking Temporal Biases in Large Language Models","summary":"  This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately.\n","authors":["Gagan Bhatia","MingZe Tang","Cristina Mahanta","Madiha Kazi"],"pdf_url":"https://arxiv.org/pdf/2412.13377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13036v1","updated":"2025-05-19T12:21:29Z","published":"2025-05-19T12:21:29Z","title":"KIT's Offline Speech Translation and Instruction Following Submission\n  for IWSLT 2025","summary":"  The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.\n","authors":["Sai Koneru","Maike Züfle","Thai-Binh Nguyen","Seymanur Akti","Jan Niehues","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2505.13036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13033v1","updated":"2025-05-19T12:18:53Z","published":"2025-05-19T12:18:53Z","title":"TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series\n  Analysis","summary":"  The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon.\n","authors":["Vijay Ekambaram","Subodh Kumar","Arindam Jati","Sumanta Mukherjee","Tomoya Sakai","Pankaj Dayama","Wesley M. Gifford","Jayant Kalagnanam"],"pdf_url":"https://arxiv.org/pdf/2505.13033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13031v1","updated":"2025-05-19T12:17:04Z","published":"2025-05-19T12:17:04Z","title":"MindOmni: Unleashing Reasoning Generation in Vision Language Models with\n  RGPO","summary":"  Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}.\n","authors":["Yicheng Xiao","Lin Song","Yukang Chen","Yingmin Luo","Yuxin Chen","Yukang Gan","Wei Huang","Xiu Li","Xiaojuan Qi","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2505.13031v1.pdf","comment":"Code: https://github.com/EasonXiao-888/MindOmni"},{"id":"http://arxiv.org/abs/2411.19477v4","updated":"2025-05-19T12:12:00Z","published":"2024-11-29T05:29:47Z","title":"Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models","summary":"  We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13026v1","updated":"2025-05-19T12:10:17Z","published":"2025-05-19T12:10:17Z","title":"Step-wise Adaptive Integration of Supervised Fine-tuning and\n  Reinforcement Learning for Task-Specific LLMs","summary":"  Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods.\n","authors":["Jack Chen","Fazhong Liu","Naruto Liu","Yuhan Luo","Erqu Qin","Harry Zheng","Tian Dong","Haojin Zhu","Yan Meng","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13025v1","updated":"2025-05-19T12:09:25Z","published":"2025-05-19T12:09:25Z","title":"LiBOG: Lifelong Learning for Black-Box Optimizer Generation","summary":"  Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks.\n","authors":["Jiyuan Pei","Yi Mei","Jialin Liu","Mengjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13025v1.pdf","comment":"Accepted at IJCAI 2025. To appear"},{"id":"http://arxiv.org/abs/2505.13023v1","updated":"2025-05-19T12:07:29Z","published":"2025-05-19T12:07:29Z","title":"Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based\n  Inpainters under Unknown Conditions","summary":"  As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.\n","authors":["Yimao Guo","Zuomin Qu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2505.13023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12819v3","updated":"2025-05-19T12:04:11Z","published":"2024-02-20T08:38:24Z","title":"Comparing Specialised Small and General Large Language Models on Text\n  Classification: 100 Labelled Samples to Achieve Break-Even Performance","summary":"  When solving NLP tasks with limited labelled data, researchers typically\neither use a general large language model without further update, or use a\nsmall number of labelled samples to tune a specialised smaller model. In this\nwork, we answer an important question -- how many labelled samples are required\nfor the specialised small models to outperform general large models, while\ntaking the performance variance into consideration. By observing the behaviour\nof fine-tuning, instruction-tuning, prompting and in-context learning on 8\nlanguage models, we identify such performance break-even points across 8\nrepresentative text classification tasks of varying characteristics. We show\nthat the specialised models often need only few samples (on average $100$) to\nbe on par or better than the general ones. At the same time, the number of\nrequired labels strongly depends on the dataset or task characteristics, with\nfine-tuning on binary datasets requiring significantly more samples. When\nperformance variance is taken into consideration, the number of required labels\nincreases on average by $100 - 200\\%$. Finally, larger models do not\nconsistently lead to better performance and lower variance, with 4-bit\nquantisation having negligible impact.\n","authors":["Branislav Pecher","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2402.12819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13011v1","updated":"2025-05-19T11:54:40Z","published":"2025-05-19T11:54:40Z","title":"Unveiling and Steering Connectome Organization with Interpretable Latent\n  Variables","summary":"  The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks.\n","authors":["Yubin Li","Xingyu Liu","Guozhang Chen"],"pdf_url":"https://arxiv.org/pdf/2505.13011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13010v1","updated":"2025-05-19T11:54:39Z","published":"2025-05-19T11:54:39Z","title":"To Bias or Not to Bias: Detecting bias in News with bias-detector","summary":"  Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.\n","authors":["Himel Ghosh","Ahmed Mosharafa","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2505.13010v1.pdf","comment":"7 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.09546v3","updated":"2025-05-19T11:39:28Z","published":"2025-04-13T12:34:02Z","title":"A simulation-heuristics dual-process model for intuitive physics","summary":"  The role of mental simulation in human physical reasoning is widely\nacknowledged, but whether it is employed across scenarios with varying\nsimulation costs and where its boundary lies remains unclear. Using a\npouring-marble task, our human study revealed two distinct error patterns when\npredicting pouring angles, differentiated by simulation time. While mental\nsimulation accurately captured human judgments in simpler scenarios, a linear\nheuristic model better matched human predictions when simulation time exceeded\na certain boundary. Motivated by these observations, we propose a dual-process\nframework, Simulation-Heuristics Model (SHM), where intuitive physics employs\nsimulation for short-time simulation but switches to heuristics when simulation\nbecomes costly. By integrating computational methods previously viewed as\nseparate into a unified model, SHM quantitatively captures their switching\nmechanism. The SHM aligns more precisely with human behavior and demonstrates\nconsistent predictive performance across diverse scenarios, advancing our\nunderstanding of the adaptive nature of intuitive physical reasoning.\n","authors":["Shiqian Li","Yuxi Ma","Jiajun Yan","Bo Dai","Yujia Peng","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.09546v3.pdf","comment":"8 pages, CogSci 2025"},{"id":"http://arxiv.org/abs/2502.01179v3","updated":"2025-05-19T11:36:10Z","published":"2025-02-03T09:13:09Z","title":"Joint Localization and Activation Editing for Low-Resource Fine-Tuning","summary":"  Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. These methods, due to their extremely small parameter counts,\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola.\n","authors":["Wen Lai","Alexander Fraser","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2502.01179v3.pdf","comment":"Accepted by ICML 2025. The code is released at\n  https://github.com/wenlai-lavine/jola"},{"id":"http://arxiv.org/abs/2505.12996v1","updated":"2025-05-19T11:34:47Z","published":"2025-05-19T11:34:47Z","title":"ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning","summary":"  In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.\n","authors":["Jiaan Wang","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12996v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.01976v3","updated":"2025-05-19T11:31:04Z","published":"2024-07-02T06:29:05Z","title":"A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding","summary":"  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. LayTextLLM\nprojects each bounding box to a single embedding and interleaves it with text,\nefficiently avoiding long sequence issues while leveraging autoregressive\ntraits of LLMs. LayTextLLM not only streamlines the interaction of layout and\ntextual data but also shows enhanced performance in KIE and VQA. Comprehensive\nbenchmark evaluations reveal significant improvements of LayTextLLM, with a\n15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA\nOCR-based LLMs. All resources are available at\nhttps://github.com/LayTextLLM/LayTextLLM.\n","authors":["Jinghui Lu","Haiyang Yu","Yanjie Wang","Yongjie Ye","Jingqun Tang","Ziwei Yang","Binghong Wu","Qi Liu","Hao Feng","Han Wang","Hao Liu","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2407.01976v3.pdf","comment":"Accept to ACL2025 Findings"},{"id":"http://arxiv.org/abs/2505.12992v1","updated":"2025-05-19T11:30:41Z","published":"2025-05-19T11:30:41Z","title":"Fractured Chain-of-Thought Reasoning","summary":"  Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.\n","authors":["Baohao Liao","Hanze Dong","Yuhui Xu","Doyen Sahoo","Christof Monz","Junnan Li","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.12992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06883v2","updated":"2025-05-19T11:28:40Z","published":"2025-05-11T07:23:26Z","title":"FACET: Force-Adaptive Control via Impedance Reference Tracking for\n  Legged Robots","summary":"  Reinforcement learning (RL) has made significant strides in legged robot\ncontrol, enabling locomotion across diverse terrains and complex\nloco-manipulation capabilities. However, the commonly used position or velocity\ntracking-based objectives are agnostic to forces experienced by the robot,\nleading to stiff and potentially dangerous behaviors and poor control during\nforceful interactions. To address this limitation, we present\n\\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).\nInspired by impedance control, we use RL to train a control policy to imitate a\nvirtual mass-spring-damper system, allowing fine-grained control under external\nforces by manipulating the virtual spring. In simulation, we demonstrate that\nour quadruped robot achieves improved robustness to large impulses (up to 200\nNs) and exhibits controllable compliance, achieving an 80% reduction in\ncollision impulse. The policy is deployed to a physical robot to showcase both\ncompliance and the ability to engage with large forces by kinesthetic control\nand pulling payloads up to 2/3 of its weight. Further extension to a legged\nloco-manipulator and a humanoid shows the applicability of our method to more\ncomplex settings to enable whole-body compliance control. Project Website:\nhttps://facet.pages.dev/\n","authors":["Botian Xu","Haoyang Weng","Qingzhou Lu","Yang Gao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2505.06883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12983v1","updated":"2025-05-19T11:18:54Z","published":"2025-05-19T11:18:54Z","title":"An Empirical Study of Many-to-Many Summarization with Large Language\n  Models","summary":"  Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.\n","authors":["Jiaan Wang","Fandong Meng","Zengkui Sun","Yunlong Liang","Yuxuan Cao","Jiarong Xu","Haoxiang Shi","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12983v1.pdf","comment":"Accepted to ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2305.13673v4","updated":"2025-05-19T11:12:27Z","published":"2023-05-23T04:28:16Z","title":"Physics of Language Models: Part 1, Learning Hierarchical Language\n  Structures","summary":"  Transformer-based language models are effective but complex, and\nunderstanding their inner workings and reasoning mechanisms is a significant\nchallenge. Previous research has primarily explored how these models handle\nsimple tasks like name copying or selection, and we extend this by\ninvestigating how these models perform recursive language structure reasoning\ndefined by context-free grammars (CFGs). We introduce a family of synthetic\nCFGs that produce hierarchical rules, capable of generating lengthy sentences\n(e.g., hundreds of tokens) that are locally ambiguous and require dynamic\nprogramming to parse. Despite this complexity, we demonstrate that generative\nmodels like GPT can accurately learn and reason over CFG-defined hierarchies\nand generate sentences based on it. We explore the model's internals, revealing\nthat its hidden states precisely capture the structure of CFGs, and its\nattention patterns resemble the information passing in a dynamic programming\nalgorithm.\n  This paper also presents several corollaries, including showing why absolute\npositional embeddings is inferior to relative and rotary embeddings; uniform\nattention alone is surprisingly effective (motivating our follow-up work on\nCanon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep\nstructure reasoning on CFGs compared to autoregressive models (e.g., GPT); and\ninjecting structural or syntactic noise into pretraining data markedly improves\nrobustness to corrupted language prompts.\n","authors":["Zeyuan Allen-Zhu","Yuanzhi Li"],"pdf_url":"https://arxiv.org/pdf/2305.13673v4.pdf","comment":"V2 polishes writing and adds Appendix G; V3 polishes writing and\n  changes the title; V4 improves writing and adds Appendix H (more uniform\n  attention results)"},{"id":"http://arxiv.org/abs/2504.20734v2","updated":"2025-05-19T11:09:12Z","published":"2025-04-29T13:18:58Z","title":"UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse\n  Modalities and Granularities","summary":"  Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single aggregated corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over various modality-specific and unified\nbaselines.\n","authors":["Woongyeong Yeo","Kangsan Kim","Soyeong Jeong","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2504.20734v2.pdf","comment":"Project page : https://universalrag.github.io"},{"id":"http://arxiv.org/abs/2501.04697v2","updated":"2025-05-19T11:02:53Z","published":"2025-01-08T18:58:48Z","title":"Grokking at the Edge of Numerical Stability","summary":"  Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.\n","authors":["Lucas Prieto","Melih Barsbey","Pedro A. M. Mediano","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2501.04697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12966v1","updated":"2025-05-19T11:01:49Z","published":"2025-05-19T11:01:49Z","title":"Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake\n  Detection","summary":"  Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets.\n","authors":["Zihan Xiong","Xiaohua Wu","Lei Chen","Fangqi Lou"],"pdf_url":"https://arxiv.org/pdf/2505.12966v1.pdf","comment":"9 pages,ICMR accepted"}]},"2025-05-20T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2503.14281v3","updated":"2025-05-20T05:55:59Z","published":"2025-03-18T14:20:54Z","title":"XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants","summary":"  AI coding assistants are widely used for tasks like code generation. These\ntools now require large and complex contexts, automatically sourced from\nvarious origins$\\unicode{x2014}$across files, projects, and\ncontributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs.\nThis automatic context-gathering introduces new vulnerabilities, allowing\nattackers to subtly poison input to compromise the assistant's outputs,\npotentially generating vulnerable code or introducing critical errors. We\npropose a novel attack, Cross-Origin Context Poisoning (XOXO), that is\nchallenging to detect as it relies on adversarial code modifications that are\nsemantically equivalent. Traditional program analysis techniques struggle to\nidentify these perturbations since the semantics of the code remains correct,\nmaking it appear legitimate. This allows attackers to manipulate coding\nassistants into producing incorrect outputs, while shifting the blame to the\nvictim developer. We introduce a novel, task-agnostic, black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving a 75.72% attack success rate on average across five\ntasks and eleven models, including GPT 4.1 and Claude 3.5 Sonnet v2 used by\npopular AI coding assistants. Furthermore, defenses like adversarial\nfine-tuning are ineffective against our attack, underscoring the need for new\nsecurity measures in LLM-powered coding tools.\n","authors":["Adam Štorek","Mukur Gupta","Noopur Bhatt","Aditya Gupta","Janie Kim","Prashast Srivastava","Suman Jana"],"pdf_url":"https://arxiv.org/pdf/2503.14281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13319v2","updated":"2025-05-20T04:32:38Z","published":"2025-05-19T16:30:27Z","title":"SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated\n  Distillation","summary":"  Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems.\n","authors":["Tian Wen","Sheng Sun","Yuwei Wang","Peiyan Chen","Zhiyuan Wu","Min Liu","Bo Gao"],"pdf_url":"https://arxiv.org/pdf/2505.13319v2.pdf","comment":"15 pages, 16 figures, 3 tables, 27 equations"},{"id":"http://arxiv.org/abs/2505.13238v2","updated":"2025-05-20T07:25:25Z","published":"2025-05-19T15:21:08Z","title":"A Geometry-Grounded Data Perimeter in Azure","summary":"  While data perimeter is ubiquitous in cybersecurity speak, it rarely defines\nhow boundary points are arranged. In this paper we show how Azure s blast\nradius ultrametric provides the distance, and how solving the Traveling\nSalesman Problem in this ultrametric space provides the ordering, yielding a\ntrue geometric contour: an actionable perimeter measure for SPN prioritization.\n","authors":["Christophe Parisel"],"pdf_url":"https://arxiv.org/pdf/2505.13238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13028v2","updated":"2025-05-20T07:34:53Z","published":"2025-05-19T12:12:00Z","title":"Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark\n  Dataset","summary":"  Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.\n","authors":["Sayon Palit","Daniel Woods"],"pdf_url":"https://arxiv.org/pdf/2505.13028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12981v2","updated":"2025-05-20T07:02:05Z","published":"2025-05-19T11:17:46Z","title":"From Assistants to Adversaries: Exploring the Security Risks of Mobile\n  LLM Agents","summary":"  The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation.\n","authors":["Liangxuan Wu","Chao Wang","Tianming Liu","Yanjie Zhao","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12442v2","updated":"2025-05-20T11:48:36Z","published":"2025-05-18T14:31:45Z","title":"IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems","summary":"  The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.\n","authors":["Liwen Wang","Wenxuan Wang","Shuai Wang","Zongjie Li","Zhenlan Ji","Zongyi Lyu","Daoyuan Wu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2505.12442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05505v3","updated":"2025-05-20T04:05:24Z","published":"2025-02-08T09:50:30Z","title":"Differentially Private Synthetic Data via APIs 3: Using Simulators\n  Instead of Foundation Model","summary":"  Differentially private (DP) synthetic data, which closely resembles the\noriginal private data while maintaining strong privacy guarantees, has become a\nkey tool for unlocking the value of private data without compromising privacy.\nRecently, Private Evolution (PE) has emerged as a promising method for\ngenerating DP synthetic data. Unlike other training-based approaches, PE only\nrequires access to inference APIs from foundation models, enabling it to\nharness the power of state-of-the-art (SoTA) models. However, a suitable\nfoundation model for a specific private data domain is not always available. In\nthis paper, we discover that the PE framework is sufficiently general to allow\nAPIs beyond foundation models. In particular, we demonstrate that many SoTA\ndata synthesizers that do not rely on neural networks--such as computer\ngraphics-based image generators, which we refer to as simulators--can be\neffectively integrated into PE. This insight significantly broadens PE's\napplicability and unlocks the potential of powerful simulators for DP data\nsynthesis. We explore this approach, named Sim-PE, in the context of image\nsynthesis. Across four diverse simulators, Sim-PE performs well, improving the\ndownstream classification accuracy of PE by up to 3x, reducing FID by up to\n80%, and offering much greater efficiency. We also show that simulators and\nfoundation models can be easily leveraged together within PE to achieve further\nimprovements. The code is open-sourced in the Private Evolution Python library:\nhttps://github.com/microsoft/DPSDA.\n","authors":["Zinan Lin","Tadas Baltrusaitis","Wenyu Wang","Sergey Yekhanin"],"pdf_url":"https://arxiv.org/pdf/2502.05505v3.pdf","comment":"Published in: (1) ICLR 2025 Workshop on Data Problems, (2) ICLR 2025\n  Workshop on Synthetic Data"},{"id":"http://arxiv.org/abs/2505.11790v2","updated":"2025-05-20T07:27:52Z","published":"2025-05-17T02:28:12Z","title":"JULI: Jailbreak Large Language Models by Self-Introspection","summary":"  Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics.\n","authors":["Jesson Wang","Zhanhao Hu","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2505.11790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14673v1","updated":"2025-05-20T17:58:02Z","published":"2025-05-20T17:58:02Z","title":"Training-Free Watermarking for Autoregressive Image Generation","summary":"  Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.\n","authors":["Yu Tong","Zihao Pan","Shuai Yang","Kaiyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14616v1","updated":"2025-05-20T17:00:31Z","published":"2025-05-20T17:00:31Z","title":"TSA-WF: Exploring the Effectiveness of Time Series Analysis for Website\n  Fingerprinting","summary":"  Website fingerprinting (WF) is a technique that allows an eavesdropper to\ndetermine the website a target user is accessing by inspecting the metadata\nassociated with the packets she exchanges via some encrypted tunnel, e.g., Tor.\nRecent WF attacks built using machine learning (and deep learning) process and\nsummarize trace metadata during their feature extraction phases. This\nmethodology leads to predictions that lack information about the instant at\nwhich a given website is detected within a (potentially large) network trace\ncomprised of multiple sequential website accesses -- a setting known as\n\\textit{multi-tab} WF.\n  In this paper, we explore whether classical time series analysis techniques\ncan be effective in the WF setting. Specifically, we introduce TSA-WF, a\npipeline designed to closely preserve network traces' timing and direction\ncharacteristics, which enables the exploration of algorithms designed to\nmeasure time series similarity in the WF context. Our evaluation with Tor\ntraces reveals that TSA-WF achieves a comparable accuracy to existing WF\nattacks in scenarios where website accesses can be easily singled-out from a\ngiven trace (i.e., the \\textit{single-tab} WF setting), even when shielded by\nspecially designed WF defenses. Finally, while TSA-WF did not outperform\nexisting attacks in the multi-tab setting, we show how TSA-WF can help pinpoint\nthe approximate instant at which a given website of interest is visited within\na multi-tab trace.\\footnote{This preprint has not undergone any post-submission\nimprovements or corrections. The Version of Record of this contribution is\npublished in the Proceedings of the 20th International Conference on\nAvailability, Reliability and Security (ARES 2025)}\n","authors":["Michael Wrana","Uzma Maroof","Diogo Barradas"],"pdf_url":"https://arxiv.org/pdf/2505.14616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14607v1","updated":"2025-05-20T16:54:34Z","published":"2025-05-20T16:54:34Z","title":"sudoLLM : On Multi-role Alignment of Language Models","summary":"  User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.\n","authors":["Soumadeep Saha","Akshay Chaturvedi","Joy Mahapatra","Utpal Garain"],"pdf_url":"https://arxiv.org/pdf/2505.14607v1.pdf","comment":"Under review. Code and data to be released later"},{"id":"http://arxiv.org/abs/2505.14592v1","updated":"2025-05-20T16:45:54Z","published":"2025-05-20T16:45:54Z","title":"Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded\n  Intrusion Detection on the Edge","summary":"  Artificial neural network pruning is a method in which artificial neural\nnetwork sizes can be reduced while attempting to preserve the predicting\ncapabilities of the network. This is done to make the model smaller or faster\nduring inference time. In this work we analyze the ability of a selection of\nartificial neural network pruning methods to generalize to a new cybersecurity\ndataset utilizing a simpler network type than was designed for. We analyze each\nmethod using a variety of pruning degrees to best understand how each algorithm\nresponds to the new environment. This has allowed us to determine the most well\nfit pruning method of those we searched for the task. Unexpectedly, we have\nfound that many of them do not generalize to the problem well, leaving only a\nfew algorithms working to an acceptable degree.\n","authors":["Alexandre Broggi","Nathaniel Bastian","Lance Fiondella","Gokhan Kul"],"pdf_url":"https://arxiv.org/pdf/2505.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14565v1","updated":"2025-05-20T16:24:59Z","published":"2025-05-20T16:24:59Z","title":"Towards Verifiability of Total Value Locked (TVL) in Decentralized\n  Finance","summary":"  Total Value Locked (TVL) aims to measure the aggregate value of cryptoassets\ndeposited in Decentralized Finance (DeFi) protocols. Although blockchain data\nis public, the way TVL is computed is not well understood. In practice, its\ncalculation on major TVL aggregators relies on self-reports from community\nmembers and lacks standardization, making it difficult to verify published\nfigures independently. We thus conduct a systematic study on 939 DeFi projects\ndeployed in Ethereum. We study the methodologies used to compute TVL, examine\nfactors hindering verifiability, and ultimately propose standardization\nattempts in the field. We find that 10.5% of the protocols rely on external\nservers; 68 methods alternative to standard balance queries exist, although\ntheir use decreased over time; and 240 equal balance queries are repeated on\nmultiple protocols. These findings indicate limits to verifiability and\ntransparency. We thus introduce ``verifiable Total Value Locked'' (vTVL), a\nmetric measuring the TVL that can be verified relying solely on on-chain data\nand standard balance queries. A case study on 400 protocols shows that our\nestimations align with published figures for 46.5% of protocols. Informed by\nthese findings, we discuss design guidelines that could facilitate a more\nverifiable, standardized, and explainable TVL computation.\n","authors":["Pietro Saggese","Michael Fröwis","Stefan Kitzler","Bernhard Haslhofer","Raphael Auer"],"pdf_url":"https://arxiv.org/pdf/2505.14565v1.pdf","comment":"JEL classification: E42, E58, F31, G12, G19, G23, L50, O33"},{"id":"http://arxiv.org/abs/2505.02392v3","updated":"2025-05-20T16:06:42Z","published":"2025-05-05T06:27:37Z","title":"Moneros Decentralized P2P Exchanges: Functionality, Adoption, and\n  Privacy Risks","summary":"  Privacy-focused cryptocurrencies like Monero remain popular, despite\nincreasing regulatory scrutiny that has led to their delisting from major\ncentralized exchanges. The latter also explains the recent popularity of\ndecentralized exchanges (DEXs) with no centralized ownership structures. These\nplatforms typically leverage peer-to-peer (P2P) networks, promising secure and\nanonymous asset trading. However, questions of liability remain, and the\nacademic literature lacks comprehensive insights into the functionality,\ntrading activity, and privacy claims of these P2P platforms. In this paper, we\nprovide an early systematization of the current landscape of decentralized\npeer-to-peer exchanges within the Monero ecosystem. We examine several recently\ndeveloped DEX platforms, analyzing their popularity, functionality,\narchitectural choices, and potential weaknesses. We further identify and report\non a privacy vulnerability in the recently popularized Haveno exchange,\ndemonstrating that certain Haveno trades could be detected, allowing\ntransactions to be linked across the Monero and Bitcoin blockchains. We hope\nthat our findings can nourish the discussion in the research community about\nmore secure designs, and provide insights for regulators.\n","authors":["Yannik Kopyciok","Friedhelm Victor","Stefan Schmid"],"pdf_url":"https://arxiv.org/pdf/2505.02392v3.pdf","comment":"Accepted submission to the 9th Workshop on Trusted Smart Contracts\n  (WTSC) 2025, to be published in the Springer-Verlag Lecture Notes in Computer\n  Science (LNCS) series"},{"id":"http://arxiv.org/abs/2505.14551v1","updated":"2025-05-20T16:06:25Z","published":"2025-05-20T16:06:25Z","title":"Trustworthy Reputation Games and Applications to Proof-of-Reputation\n  Blockchains","summary":"  Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.\n","authors":["Petros Drineas","Rohit Nema","Rafail Ostrovsky","Vassilis Zikas"],"pdf_url":"https://arxiv.org/pdf/2505.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14549v1","updated":"2025-05-20T16:05:05Z","published":"2025-05-20T16:05:05Z","title":"Can Large Language Models Really Recognize Your Name?","summary":"  Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.\n","authors":["Dzung Pham","Peter Kairouz","Niloofar Mireshghallah","Eugene Bagdasarian","Chau Minh Pham","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2505.14549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05136v8","updated":"2025-05-20T16:04:22Z","published":"2025-03-07T04:29:11Z","title":"The Beginner's Textbook for Fully Homomorphic Encryption","summary":"  Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables\ncomputations to be performed directly on encrypted data, as if the data were in\nplaintext. After all computations are performed on the encrypted data, it can\nbe decrypted to reveal the result. The decrypted value matches the result that\nwould have been obtained if the same computations were applied to the plaintext\ndata.\n  FHE supports basic operations such as addition and multiplication on\nencrypted numbers. Using these fundamental operations, more complex\ncomputations can be constructed, including subtraction, division, logic gates\n(e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such\nas ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions\ncan be implemented either as exact formulas or as approximations, depending on\nthe trade-off between computational efficiency and accuracy.\n  FHE enables privacy-preserving machine learning by allowing a server to\nprocess the client's data in its encrypted form through an ML model. With FHE,\nthe server learns neither the plaintext version of the input features nor the\ninference results. Only the client, using their secret key, can decrypt and\naccess the results at the end of the service protocol. FHE can also be applied\nto confidential blockchain services, ensuring that sensitive data in smart\ncontracts remains encrypted and confidential while maintaining the transparency\nand integrity of the execution process. Other applications of FHE include\nsecure outsourcing of data analytics, encrypted database queries,\nprivacy-preserving searches, efficient multi-party computation for digital\nsignatures, and more.\n  As this book is an open project (https://fhetextbook.github.io), we welcome\nFHE experts to join us as collaborators to help expand the draft.\n","authors":["Ronny Ko"],"pdf_url":"https://arxiv.org/pdf/2503.05136v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14534v1","updated":"2025-05-20T15:54:45Z","published":"2025-05-20T15:54:45Z","title":"Lessons from Defending Gemini Against Indirect Prompt Injections","summary":"  Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.\n","authors":["Chongyang Shi","Sharon Lin","Shuang Song","Jamie Hayes","Ilia Shumailov","Itay Yona","Juliette Pluto","Aneesh Pappu","Christopher A. Choquette-Choo","Milad Nasr","Chawin Sitawarin","Gena Gibson","Andreas Terzis","John \"Four\" Flynn"],"pdf_url":"https://arxiv.org/pdf/2505.14534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00849v2","updated":"2025-05-20T15:50:50Z","published":"2025-05-01T20:21:57Z","title":"TherMod Communication: Low Power or Hot Air?","summary":"  The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme leverages\nstatistical physics to enable secure communication with zero average power flow\nin a wired channel. While the original KLJN scheme requires significant power\nfor operation, a recent wireless modification, TherMod, proposed by Basar\nclaims a \"low power\" implementation. This paper critically examines this claim.\nWe explain that the additional components inherent in Basar's wireless\nadaptation substantially increase power consumption, rendering the \"low power\"\nassertion inappropriate. Furthermore, we clarify that the security claims of\nthe original KLJN scheme do not directly translate to this wireless adaptation,\nimplying significant security breach. Finally, the scheme looks identical one\nof the stealth communicators from 2005, which was shown not to be secure.\n","authors":["Christiana Chamon"],"pdf_url":"https://arxiv.org/pdf/2505.00849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04290v2","updated":"2025-05-20T15:22:28Z","published":"2024-06-06T17:34:48Z","title":"Cassandra: Efficient Enforcement of Sequential Execution for\n  Cryptographic Programs (Extended Version)","summary":"  Constant-time programming is a widely deployed approach to harden\ncryptographic programs against side channel attacks. However, modern processors\noften violate the underlying assumptions of standard constant-time policies by\ntransiently executing unintended paths of the program. Despite many solutions\nproposed, addressing control flow misspeculations in an efficient way without\nlosing performance is an open problem.\n  In this work, we propose Cassandra, a novel hardware/software mechanism to\nenforce sequential execution for constant-time cryptographic code in a highly\nefficient manner. Cassandra explores the radical design point of disabling the\nbranch predictor and recording-and-replaying sequential control flow of the\nprogram. Two key insights that enable our design are that (1) the sequential\ncontrol flow of a constant-time program is mostly static over different runs,\nand (2) cryptographic programs are loop-intensive and their control flow\npatterns repeat in a highly compressible way. These insights allow us to\nperform an upfront branch analysis that significantly compresses control flow\ntraces. We add a small component to a typical processor design, the Branch\nTrace Unit, to store compressed traces and determine fetch redirections\naccording to the sequential model of the program. Despite providing a strong\nsecurity guarantee, Cassandra counterintuitively provides an average 1.85%\nspeedup compared to an unsafe baseline processor, mainly due to enforcing\nnear-perfect fetch redirections.\n","authors":["Ali Hajiabadi","Trevor E. Carlson"],"pdf_url":"https://arxiv.org/pdf/2406.04290v2.pdf","comment":"17 pages, 9 figures, 4 tables, 1 listing"},{"id":"http://arxiv.org/abs/2505.14461v1","updated":"2025-05-20T14:57:04Z","published":"2025-05-20T14:57:04Z","title":"MicroCrypt Assumptions with Quantum Input Sampling and\n  Pseudodeterminism: Constructions and Separations","summary":"  We investigate two natural relaxations of quantum cryptographic primitives.\nThe first involves quantum input sampling, where inputs are generated by a\nquantum algorithm rather than sampled uniformly at random. Applying this to\npseudorandom generators ($\\textsf{PRG}$s) and pseudorandom states\n($\\textsf{PRS}$s), leads to the notions denoted as $\\textsf{PRG}^{qs}$ and\n$\\textsf{PRS}^{qs}$, respectively. The second relaxation,\n$\\bot$-pseudodeterminism, relaxes the determinism requirement by allowing the\noutput to be a special symbol $\\bot$ on an inverse-polynomial fraction of\ninputs.\n  We demonstrate an equivalence between bounded-query logarithmic-size\n$\\textsf{PRS}^{qs}$, logarithmic-size $\\textsf{PRS}^{qs}$, and\n$\\textsf{PRG}^{qs}$. Moreover, we establish that $\\textsf{PRG}^{qs}$ can be\nconstructed from $\\bot$-$\\textsf{PRG}$s, which in turn were built from\nlogarithmic-size $\\textsf{PRS}$. Interestingly, these relations remain unknown\nin the uniform key setting.\n  To further justify these relaxed models, we present black-box separations.\nOur results suggest that $\\bot$-pseudodeterministic primitives may be weaker\nthan their deterministic counterparts, and that primitives based on quantum\ninput sampling may be inherently weaker than those using uniform sampling.\n  Together, these results provide numerous new insights into the structure and\nhierarchy of primitives within MicroCrypt.\n","authors":["Mohammed Barhoush","Ryo Nishimaki","Takashi Yamakawa"],"pdf_url":"https://arxiv.org/pdf/2505.14461v1.pdf","comment":"59 pages"},{"id":"http://arxiv.org/abs/2205.10580v7","updated":"2025-05-20T14:01:03Z","published":"2022-05-21T12:17:21Z","title":"Secure Order Based Voting Using Distributed Tallying","summary":"  Electronic voting systems have significant advantages in comparison with\nphysical voting systems. One of the main challenges in e-voting systems is to\nsecure the voting process: namely, to certify that the computed results are\nconsistent with the cast ballots and that the voters' privacy is preserved. We\npropose herein a secure voting protocol for elections that are governed by\norder-based voting rules. Our protocol, in which the tallying task is\ndistributed among several independent talliers, offers perfect ballot secrecy\nin the sense that it issues only the required output while no other information\non the cast ballots is revealed. Such perfect secrecy, achieved by employing\nsecure multiparty computation tools, may increase the voters' confidence and,\nconsequently, encourage them to vote according to their true preferences. We\nimplemented a demo of a voting system that is based on our protocol and we\ndescribe herein the system's components and its operation. Our implementation\ndemonstrates that our secure order-based voting protocol can be readily\nimplemented in real-life large-scale electronic elections.\n","authors":["Tamir Tassa","Lihi Dery","Arthur Zamarin"],"pdf_url":"https://arxiv.org/pdf/2205.10580v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14368v1","updated":"2025-05-20T13:50:43Z","published":"2025-05-20T13:50:43Z","title":"Is Your Prompt Safe? Investigating Prompt Injection Attacks Against\n  Open-Source LLMs","summary":"  Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies.\n","authors":["Jiawen Wang","Pritha Gupta","Ivan Habernal","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2505.14368v1.pdf","comment":"8 pages, 3 figures, EMNLP 2025 under review"},{"id":"http://arxiv.org/abs/2410.14425v2","updated":"2025-05-20T13:26:45Z","published":"2024-10-18T12:39:32Z","title":"Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation","summary":"  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct comprehensive experiments on three state-of-the-art\nlarge language models and several different backdoor attack algorithms. Our\nempirical results demonstrate the outstanding performance of W2SDefense in\ndefending against backdoor attacks without compromising model performance.\n","authors":["Shuai Zhao","Xiaobao Wu","Cong-Duy Nguyen","Yanhao Jia","Meihuizi Jia","Yichao Feng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2410.14425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14325v1","updated":"2025-05-20T13:09:56Z","published":"2025-05-20T13:09:56Z","title":"Effects of the Cyber Resilience Act (CRA) on Industrial Equipment\n  Manufacturing Companies","summary":"  The Cyber Resilience Act (CRA) is a new European Union (EU) regulation aimed\nat enhancing the security of digital products and services by ensuring they\nmeet stringent cybersecurity requirements. This paper investigates the\nchallenges that industrial equipment manufacturing companies anticipate while\npreparing for compliance with CRA through a comprehensive survey. Key findings\nhighlight significant hurdles such as implementing secure development lifecycle\npractices, managing vulnerability notifications within strict timelines, and\naddressing gaps in cybersecurity expertise. This study provides insights into\nthese specific challenges and offers targeted recommendations on key focus\nareas, such as tooling improvements, to aid industrial equipment manufacturers\nin their preparation for CRA compliance.\n","authors":["Roosa Risto","Mohit Sethi","Mika Katara"],"pdf_url":"https://arxiv.org/pdf/2505.14325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14323v1","updated":"2025-05-20T13:09:22Z","published":"2025-05-20T13:09:22Z","title":"Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction\n  Attacks in Small-Data Regime","summary":"  Training data reconstruction attacks enable adversaries to recover portions\nof a released model's training data. We consider the attacks where a\nreconstructor neural network learns to invert the (random) mapping between\ntraining data and model weights. Prior work has shown that an informed\nadversary with access to released model's weights and all but one training data\npoint can achieve high-quality reconstructions in this way. However,\ndifferential privacy can defend against such an attack with little to no loss\nin model's utility when the amount of training data is sufficiently large. In\nthis work we consider a more realistic adversary who only knows the\ndistribution from which a small training dataset has been sampled and who\nattacks a transfer-learned neural network classifier that has been trained on\nthis dataset. We exhibit an attack that works in this realistic threat model\nand demonstrate that in the small-data regime it cannot be defended against by\nDP-SGD without severely damaging the classifier accuracy. This raises\nsignificant concerns about the use of such transfer-learned classifiers when\nprotection of training-data is paramount. We demonstrate the effectiveness and\nrobustness of our attack on VGG, EfficientNet and ResNet image classifiers\ntransfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we\npoint out that the commonly used (true-positive) reconstruction success rate\nmetric fails to reliably quantify the actual reconstruction effectiveness.\nInstead, we make use of the Neyman-Pearson lemma to construct the receiver\noperating characteristic curve and consider the associated true-positive\nreconstruction rate at a fixed level of the false-positive reconstruction rate.\n","authors":["Tomasz Maciążek","Robert Allison"],"pdf_url":"https://arxiv.org/pdf/2505.14323v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2505.14316v1","updated":"2025-05-20T13:03:15Z","published":"2025-05-20T13:03:15Z","title":"Exploring Jailbreak Attacks on LLMs through Intent Concealment and\n  Diversion","summary":"  Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.\n","authors":["Tiehan Cui","Yanxu Mao","Peipei Liu","Congying Liu","Datao You"],"pdf_url":"https://arxiv.org/pdf/2505.14316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01280v2","updated":"2025-05-20T12:40:51Z","published":"2023-08-02T17:05:40Z","title":"Scalable Time-Lock Puzzle","summary":"  Time-Lock Puzzles (TLPs) enable a client to lock a message such that a server\ncan unlock it only after a specified time. They have diverse applications, such\nas scheduled payments, secret sharing, and zero-knowledge proofs. In this work,\nwe present a scalable TLP designed for real-world scenarios involving a large\nnumber of puzzles, where clients or servers may lack the computational\nresources to handle high workloads. Our contributions are both theoretical and\npractical. From a theoretical standpoint, we formally define the concept of a\nDelegated Time-Lock Puzzle (D-TLP), establish its fundamental properties, and\nintroduce an upper bound for TLPs, addressing a previously overlooked aspect.\nFrom a practical standpoint, we introduce the Efficient Delegated Time-Lock\nPuzzle (ED-TLP) protocol, which implements the D-TLP concept. This protocol\nenables both the client and server to securely outsource their\nresource-intensive tasks to third-party helpers. It enables real-time\nverification of solutions and guarantees their delivery within predefined time\nlimits by integrating an upper bound and a fair payment algorithm. ED-TLP\nallows combining puzzles from different clients, enabling a solver to process\nthem sequentially, significantly reducing computational resources, especially\nfor a large number of puzzles or clients. ED-TLP is the first protocol of its\nkind. We have implemented ED-TLP and conducted a comprehensive analysis of its\nperformance for up to 10,000 puzzles. The results highlight its significant\nefficiency in TLP applications, demonstrating that ED-TLP securely delegates\n99% of the client's workload and 100% of the server's workload with minimal\noverhead.\n","authors":["Aydin Abadi","Dan Ristea","Artem Grigor","Steven J. Murdoch"],"pdf_url":"https://arxiv.org/pdf/2308.01280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14175v1","updated":"2025-05-20T10:28:49Z","published":"2025-05-20T10:28:49Z","title":"Destabilizing Power Grid and Energy Market by Cyberattacks on Smart\n  Inverters","summary":"  Cyberattacks on smart inverters and distributed PV are becoming an imminent\nthreat, because of the recent well-documented vulnerabilities and attack\nincidents. Particularly, the long lifespan of inverter devices, users' oblivion\nof cybersecurity compliance, and the lack of cyber regulatory frameworks\nexacerbate the prospect of cyberattacks on smart inverters. As a result, this\nraises a question -- \"do cyberattacks on smart inverters, if orchestrated on a\nlarge scale, pose a genuine threat of wide-scale instability to the power grid\nand energy market\"? This paper provides a realistic assessment on the\nplausibility and impacts of wide-scale power instability caused by cyberattacks\non smart inverters. We conduct an in-depth study based on the electricity\nmarket data of Australia and the knowledge of practical contingency mechanisms.\nOur key findings reveal: (1) Despite the possibility of disruption to the grid\nby cyberattacks on smart inverters, the impact is only significant under\ncareful planning and orchestration. (2) While the grid can assure certain power\nsystem security to survive inadvertent contingency events, it is insufficient\nto defend against savvy attackers who can orchestrate attacks in an adversarial\nmanner. Our data analysis of Australia's electricity grid also reveals that a\nrelatively low percentage of distributed PV would be sufficient to launch an\nimpactful concerted attack on the grid. Our study casts insights on robust\nstrategies for defending the grid in the presence of cyberattacks for places\nwith high penetration of distributed PV.\n","authors":["Xiangyu Hui","Samuel Karumba","Sid Chi-Kin Chau","Mohiuddin Ahmed"],"pdf_url":"https://arxiv.org/pdf/2505.14175v1.pdf","comment":"Extended version of the conference paper in ACM eEnergy 2025"},{"id":"http://arxiv.org/abs/2503.23866v2","updated":"2025-05-20T09:41:52Z","published":"2025-03-31T09:17:10Z","title":"A Channel-Triggered Backdoor Attack on Wireless Semantic Image\n  Reconstruction","summary":"  This paper investigates backdoor attacks in image-oriented semantic\ncommunications. The threat of backdoor attacks on symbol reconstruction in\nsemantic communication (SemCom) systems has received limited attention.\nPrevious research on backdoor attacks targeting SemCom symbol reconstruction\nprimarily focuses on input-level triggers, which are impractical in scenarios\nwith strict input constraints. In this paper, we propose a novel\nchannel-triggered backdoor attack (CT-BA) framework that exploits inherent\nwireless channel characteristics as activation triggers. Our key innovation\ninvolves utilizing fundamental channel statistics parameters, specifically\nchannel gain with different fading distributions or channel noise with\ndifferent power, as potential triggers. This approach enhances stealth by\neliminating explicit input manipulation, provides flexibility through trigger\nselection from diverse channel conditions, and enables automatic activation via\nnatural channel variations without adversary intervention. We extensively\nevaluate CT-BA across four joint source-channel coding (JSCC) communication\nsystem architectures and three benchmark datasets. Simulation results\ndemonstrate that our attack achieves near-perfect attack success rate (ASR)\nwhile maintaining effective stealth. Finally, we discuss potential defense\nmechanisms against such attacks.\n","authors":["Jialin Wan","Jinglong Shen","Nan Cheng","Zhisheng Yin","Yiliang Liu","Wenchao Xu"," Xuemin"," Shen"],"pdf_url":"https://arxiv.org/pdf/2503.23866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14112v1","updated":"2025-05-20T09:19:06Z","published":"2025-05-20T09:19:06Z","title":"Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM\n  Watermarking","summary":"  Logit-based LLM watermarking traces and verifies AI-generated content by\nmaintaining green and red token lists and increasing the likelihood of green\ntokens during generation. However, it fails in low-entropy scenarios, where\npredictable outputs make green token selection difficult without disrupting\nnatural text flow. Existing approaches address this by assuming access to the\noriginal LLM to calculate entropy and selectively watermark high-entropy\ntokens. However, these methods face two major challenges: (1) high\ncomputational costs and detection delays due to reliance on the original LLM,\nand (2) potential risks of model leakage. To address these limitations, we\npropose Invisible Entropy (IE), a watermarking paradigm designed to enhance\nboth safety and efficiency. Instead of relying on the original LLM, IE\nintroduces a lightweight feature extractor and an entropy tagger to predict\nwhether the entropy of the next token is high or low. Furthermore, based on\ntheoretical analysis, we develop a threshold navigator that adaptively sets\nentropy thresholds. It identifies a threshold where the watermark ratio\ndecreases as the green token count increases, enhancing the naturalness of the\nwatermarked text and improving detection robustness. Experiments on HumanEval\nand MBPP datasets demonstrate that IE reduces parameter size by 99\\% while\nachieving performance on par with state-of-the-art methods. Our work introduces\na safe and efficient paradigm for low-entropy watermarking.\nhttps://github.com/Carol-gutianle/IE\nhttps://huggingface.co/datasets/Carol0110/IE-Tagger\n","authors":["Tianle Gu","Zongqi Wang","Kexin Huang","Yuanqi Yao","Xiangliang Zhang","Yujiu Yang","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2505.14112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14067v1","updated":"2025-05-20T08:15:23Z","published":"2025-05-20T08:15:23Z","title":"In Search of Lost Data: A Study of Flash Sanitization Practices","summary":"  To avoid the disclosure of personal or corporate data, sanitization of\nstorage devices is an important issue when such devices are to be reused. While\npoor sanitization practices have been reported for second-hand hard disk\ndrives, it has been reported that data has been found on original storage\ndevices based on flash technology. Based on insights into the second-hand chip\nmarket in China, we report on the results of the first large-scale study on the\neffects of chip reuse for USB flash drives. We provide clear evidence of poor\nsanitization practices in a non-negligible fraction of USB flash drives from\nthe low-cost Chinese market that were sold as original. More specifically, we\nforensically analyzed 614 USB flash drives and were able to recover non-trivial\nuser data on a total of 75 devices (more than 12 %). This non-negligible\nprobability that any data (including incriminating files) already existed on\nthe drive when it was bought has critical implications to forensic\ninvestigations. The absence of external factors which correlate with finding\ndata on new USB flash drives complicates the matter further.\n","authors":["Janine Schneider","Immanuel Lautner","Denise Moussa","Julian Wolf","Nicole Scheler","Felix Freiling","Jaap Haasnoot","Hans Henseler","Simon Malik","Holger Morgenstern","Martin Westman"],"pdf_url":"https://arxiv.org/pdf/2505.14067v1.pdf","comment":"Proceedings of the Digital Forensics Research Conference Europe\n  (DFRWS EU) 2021, March 29-April 1, 2024"},{"id":"http://arxiv.org/abs/2505.14027v1","updated":"2025-05-20T07:27:51Z","published":"2025-05-20T07:27:51Z","title":"CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model\n  for Complex and Imbalanced Data","summary":"  As computer networks proliferate, the gravity of network intrusions has\nescalated, emphasizing the criticality of network intrusion detection systems\nfor safeguarding security. While deep learning models have exhibited promising\nresults in intrusion detection, they face challenges in managing\nhigh-dimensional, complex traffic patterns and imbalanced data categories. This\npaper presents CSAGC-IDS, a network intrusion detection model based on deep\nlearning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced\nconvolutional conditional generative adversarial network that generates\nhigh-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS\nintegrates CSCA-CNN, a convolutional neural network enhanced through cost\nsensitive learning and channel attention mechanism, to extract features from\ncomplex traffic data for precise detection. Experiments conducted on the\nNSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of\n84.52% in five-class classification task, and an accuracy of 91.09% and an F1\nscore of 92.04% in binary classification task.Furthermore, this paper provides\nan interpretability analysis of the proposed model, using SHAP and LIME to\nexplain the decision-making mechanisms of the model.\n","authors":["Yifan Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.14027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12931v2","updated":"2025-05-20T06:03:24Z","published":"2025-03-17T08:41:29Z","title":"MirrorShield: Towards Universal Defense Against Jailbreaks via\n  Entropy-Guided Mirror Crafting","summary":"  Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies typically rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules fail to accommodate the inherent complexity\nand dynamic nature of real-world jailbreak attacks. In this paper, we focus on\nthe novel challenge of universal defense against diverse jailbreaks. We propose\na new concept ``mirror'', which is a dynamically generated prompt that reflects\nthe syntactic structure of the input while ensuring semantic safety. The\ndiscrepancies between input prompts and their corresponding mirrors serve as\nguiding principles for defense. A novel defense model, MirrorShield, is further\nproposed to detect and calibrate risky inputs based on the crafted mirrors.\nEvaluated on multiple benchmark datasets and compared against ten\nstate-of-the-art attack methods, MirrorShield demonstrates superior defense\nperformance and promising generalization capabilities.\n","authors":["Rui Pu","Chaozhuo Li","Rui Ha","Litian Zhang","Lirong Qiu","Xi Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13964v1","updated":"2025-05-20T06:01:48Z","published":"2025-05-20T06:01:48Z","title":"Zk-SNARK for String Match","summary":"  We present a secure and efficient string-matching platform leveraging\nzk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge) to\naddress the challenge of detecting sensitive information leakage while\npreserving data privacy. Our solution enables organizations to verify whether\nprivate strings appear on public platforms without disclosing the strings\nthemselves. To achieve computational efficiency, we integrate a sliding window\ntechnique with the Rabin-Karp algorithm and Rabin Fingerprint, enabling\nhash-based rolling comparisons to detect string matches. This approach\nsignificantly reduces time complexity compared to traditional\ncharacter-by-character comparisons. We implement the proposed system using\ngnark, a high-performance zk-SNARK library, which generates succinct and\nverifiable proofs for privacy-preserving string matching. Experimental results\ndemonstrate that our solution achieves strong privacy guarantees while\nmaintaining computational efficiency and scalability. This work highlights the\npractical applications of zero-knowledge proofs in secure data verification and\ncontributes a scalable method for privacy-preserving string matching.\n","authors":["Taoran Li","Taobo Liao"],"pdf_url":"https://arxiv.org/pdf/2505.13964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02855v3","updated":"2025-05-20T05:38:08Z","published":"2024-07-03T07:14:05Z","title":"From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending\n  Against Jailbreak Attacks","summary":"  Large Language Models (LLMs) are known to be vulnerable to jailbreak attacks.\nAn important observation is that, while different types of jailbreak attacks\ncan generate significantly different queries, they mostly result in similar\nresponses that are rooted in the same harmful knowledge (e.g., detailed steps\nto make a bomb). Consequently, unlearning-based approaches have been proposed\nto mitigate jailbreak attacks by directly removing harmful knowledge from the\nmodel. In this paper, we identify a novel ripple effect of unlearning, wherein\nLLMs can implicitly unlearn harmful knowledge that was not explicitly\nintroduced during the unlearning phase (e.g., a model unlearning the steps for\ntheft may also implicitly unlearn the steps for making a bomb). Through over\n100 experimental runs spanning multiple models, attack strategies, and defense\nmethods, we empirically validate this phenomenon, which makes unlearning-based\nmethods able to decrease the Attack Success Rate on unseen data from more than\n70% to less than 10% with only 100 training samples. Further analysis reveals\nthat the strong generalization ability of unlearning may stem from the\nintrinsic relatedness among harmful responses across harmful questions (e.g.,\nresponse patterns, shared steps and actions in response, and similarity among\ntheir learned representations in the LLM). We also discuss the potential\nlimitations of unlearning and the observed ripple effect. We hope our research\ncould contribute to a deeper understanding of unlearning. Our code is available\nat https://github.com/thu-coai/SafeUnlearning.\n","authors":["Zhexin Zhang","Junxiao Yang","Yida Lu","Pei Ke","Shiyao Cui","Chujie Zheng","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.02855v3.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2505.13957v1","updated":"2025-05-20T05:37:22Z","published":"2025-05-20T05:37:22Z","title":"Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal\n  Retrieval-Augmented Generation","summary":"  Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by\nintegrating external multimodal databases, but introduce unexplored privacy\nvulnerabilities. While text-based RAG privacy risks have been studied,\nmultimodal data presents unique challenges. We provide the first systematic\nanalysis of MRAG privacy vulnerabilities across vision-language and\nspeech-language modalities. Using a novel compositional structured prompt\nattack in a black-box setting, we demonstrate how attackers can extract private\ninformation by manipulating queries. Our experiments reveal that LMMs can both\ndirectly generate outputs resembling retrieved content and produce descriptions\nthat indirectly expose sensitive information, highlighting the urgent need for\nrobust privacy-preserving MRAG techniques.\n","authors":["Jiankun Zhang","Shenglai Zeng","Jie Ren","Tianqi Zheng","Hui Liu","Xianfeng Tang","Hui Liu","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2505.13957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13942v1","updated":"2025-05-20T05:22:03Z","published":"2025-05-20T05:22:03Z","title":"D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional\n  Optimization","summary":"  Intelligent mechanisms implemented in autonomous vehicles, such as proactive\ndriving assist and collision alerts, reduce traffic accidents. However,\nverifying their correct functionality is difficult due to complex interactions\nwith the environment. This problem is exacerbated in adversarial environments,\nwhere an attacker can control the environment surrounding autonomous vehicles\nto exploit vulnerabilities.\n  To preemptively identify vulnerabilities in these systems, in this paper, we\nimplement a scenario-based framework with a formal method to identify the\nimpact of malicious drivers interacting with autonomous vehicles. The\nformalization of the evaluation requirements utilizes metric temporal logic\n(MTL) to identify a safety condition that we want to test. Our goal is to find,\nthrough a rigorous testing approach, any trace that violates this MTL safety\nspecification. Our results can help designers identify the range of safe\noperational behaviors that prevent malicious drivers from exploiting the\nautonomous features of modern vehicles.\n","authors":["Diego Ortiz Barbosa","Luis Burbano","Carlos Hernandez","Zengxiang Lei","Younghee Park","Satish Ukkusuri","Alvaro A Cardenas"],"pdf_url":"https://arxiv.org/pdf/2505.13942v1.pdf","comment":"Dynamic Data Driven Applications Systems-2024"},{"id":"http://arxiv.org/abs/2505.13922v1","updated":"2025-05-20T04:36:29Z","published":"2025-05-20T04:36:29Z","title":"The Hidden Dangers of Outdated Software: A Cyber Security Perspective","summary":"  Outdated software remains a potent and underappreciated menace in 2025's\ncybersecurity environment, exposing systems to a broad array of threats,\nincluding ransomware, data breaches, and operational outages that can have\ndevastating and far-reaching impacts. This essay explores the unseen threats of\ncyberattacks by presenting robust statistical information, including the\nstaggering reality that 32% of cyberattacks exploit unpatched software\nvulnerabilities, based on a 2025 TechTarget survey. Furthermore, it discusses\nreal case studies, including the MOVEit breach in 2023 and the Log4Shell breach\nin 2021, both of which illustrate the catastrophic consequences of failing to\nperform software updates. The article offers a detailed analysis of the nature\nof software vulnerabilities, the underlying reasons for user resistance to\npatches, and organizational barriers that compound the issue. Furthermore, it\nsuggests actionable solutions, including automation and awareness campaigns, to\naddress these shortcomings. Apart from this, the paper also talks of trends\nsuch as AI-driven vulnerability patching and legal consequences of\nnon-compliance under laws like HIPAA, thus providing a futuristic outlook on\nhow such advancements may define future defenses. Supplemented by tables like\none detailing trends in vulnerability and a graph illustrating technology\nadoption, this report showcases the pressing demand for anticipatory update\nstrategies to safeguard digital ecosystems against the constantly evolving\nthreats that characterize the modern cyber landscape. As it stands, it is a\nvery useful document for practitioners, policymakers, and researchers.\n","authors":["Gogulakrishnan Thiyagarajan","Vinay Bist","Prabhudarshi Nayak"],"pdf_url":"https://arxiv.org/pdf/2505.13922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13895v1","updated":"2025-05-20T03:59:26Z","published":"2025-05-20T03:59:26Z","title":"VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and\n  Management","summary":"  The dynamic landscape of cybersecurity demands precise and scalable solutions\nfor vulnerability management in heterogeneous systems, where\nconfiguration-specific vulnerabilities are often misidentified due to\ninconsistent data in databases like the National Vulnerability Database (NVD).\nInaccurate Common Platform Enumeration (CPE) data in NVD further leads to false\npositives and incomplete vulnerability retrieval. Informed by our systematic\nanalysis of CPE and CVEdeails data, revealing more than 50% vendor name\ninconsistencies, we propose VulCPE, a framework that standardizes data and\nmodels configuration dependencies using a unified CPE schema (uCPE), entity\nrecognition, relation extraction, and graph-based modeling. VulCPE achieves\nsuperior retrieval precision (0.766) and coverage (0.926) over existing tools.\nVulCPE ensures precise, context-aware vulnerability management, enhancing cyber\nresilience.\n","authors":["Yuning Jiang","Feiyang Shang","Freedy Tan Wei You","Huilin Wang","Chia Ren Cong","Qiaoran Meng","Nay Oo","Hoon Wei Lim","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2505.13895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13862v1","updated":"2025-05-20T03:14:57Z","published":"2025-05-20T03:14:57Z","title":"PandaGuard: Systematic Evaluation of LLM Safety in the Era of\n  Jailbreaking Attacks","summary":"  Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.\n","authors":["Guobin Shen","Dongcheng Zhao","Linghao Feng","Xiang He","Jihang Wang","Sicheng Shen","Haibo Tong","Yiting Dong","Jindong Li","Xiang Zheng","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.13862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13861v1","updated":"2025-05-20T03:13:48Z","published":"2025-05-20T03:13:48Z","title":"hChain 4.0: A Secure and Scalable Permissioned Blockchain for EHR\n  Management in Smart Healthcare","summary":"  The growing utilization of Internet of Medical Things (IoMT) devices,\nincluding smartwatches and wearable medical devices, has facilitated real-time\nhealth monitoring and data analysis to enhance healthcare outcomes. These\ngadgets necessitate improved security measures to safeguard sensitive health\ndata while tackling scalability issues in real-time settings. The proposed\nsystem, hChain 4.0, employs a permissioned blockchain to provide a secure and\nscalable data infrastructure designed to fulfill these needs. This stands in\ncontrast to conventional systems, which are vulnerable to security flaws or\nrely on public blockchains, constrained by scalability and expense. The\nproposed approach introduces a high-privacy method in which health data are\nencrypted using the Advanced Encryption Standard (AES) for time-efficient\nencryption, combined with Partial Homomorphic Encryption (PHE) to enable secure\ncomputations on encrypted data, thereby enhancing privacy. Moreover, it\nutilizes private channels that enable isolated communication and ledger between\nstakeholders, ensuring robust privacy while supporting collaborative\noperations. The proposed framework enables anonymized health data sharing for\nmedical research by pseudonymizing patient identity. Additionally, hChain 4.0\nincorporates Attribute-Based Access Control (ABAC) to provide secure electronic\nhealth record (EHR) sharing among authorized parties, where ABAC ensures\nfine-grained permission management vital for multi-organizational healthcare\nsettings. Experimental assessments indicate that the proposed approach achieves\nhigher scalability, cost-effectiveness, and validated security.\n","authors":["Musharraf N. Alruwaill","Saraju P. Mohanty","Elias Kougianos"],"pdf_url":"https://arxiv.org/pdf/2505.13861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11548v2","updated":"2025-05-20T02:50:12Z","published":"2025-05-15T08:14:58Z","title":"One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented\n  Generation Systems","summary":"  Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines.\n","authors":["Zhiyuan Chang","Mingyang Li","Xiaojun Jia","Junjie Wang","Yuekai Huang","Ziyou Jiang","Yang Liu","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11548v2.pdf","comment":"14pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.13848v1","updated":"2025-05-20T02:42:47Z","published":"2025-05-20T02:42:47Z","title":"Quantum Opacity, Classical Clarity: A Hybrid Approach to Quantum Circuit\n  Obfuscation","summary":"  Quantum computing leverages quantum mechanics to achieve computational\nadvantages over classical hardware, but the use of third-party quantum\ncompilers in the Noisy Intermediate-Scale Quantum (NISQ) era introduces risks\nof intellectual property (IP) exposure. We address this by proposing a novel\nobfuscation technique that protects proprietary quantum circuits by inserting\nadditional quantum gates prior to compilation. These gates corrupt the\nmeasurement outcomes, which are later corrected through a lightweight classical\npost-processing step based on the inserted gate structure. Unlike prior methods\nthat rely on complex quantum reversals, barriers, or physical-to-virtual qubit\nmapping, our approach achieves obfuscation using compiler-agnostic classical\ncorrection. We evaluate the technique across five benchmark quantum algorithms\n-- Shor's, QAOA, Bernstein-Vazirani, Grover's, and HHL -- using IBM's Qiskit\nframework. The results demonstrate high Total Variation Distance (above 0.5)\nand consistently negative Degree of Functional Corruption (DFC), confirming\nboth statistical and functional obfuscation. This shows that our method is a\npractical and effective solution for the security of quantum circuit designs in\nuntrusted compilation flows.\n","authors":["Amal Raj","Vivek Balachandran"],"pdf_url":"https://arxiv.org/pdf/2505.13848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13842v1","updated":"2025-05-20T02:31:13Z","published":"2025-05-20T02:31:13Z","title":"Provable Execution in Real-Time Embedded Systems","summary":"  Embedded devices are increasingly ubiquitous and vital, often supporting\nsafety-critical functions. However, due to strict cost and energy constraints,\nthey are typically implemented with Micro-Controller Units (MCUs) that lack\nadvanced architectural security features. Within this space, recent efforts\nhave created low-cost architectures capable of generating Proofs of Execution\n(PoX) of software on potentially compromised MCUs. This capability can ensure\nthe integrity of sensor data from the outset, by binding sensed results to an\nunforgeable cryptographic proof of execution on edge sensor MCUs. However, the\nsecurity of existing PoX requires the proven execution to occur atomically.\nThis requirement precludes the application of PoX to (1) time-shared systems,\nand (2) applications with real-time constraints, creating a direct conflict\nbetween execution integrity and the real-time availability needs of several\nembedded system uses.\n  In this paper, we formulate a new security goal called Real-Time Proof of\nExecution (RT-PoX) that retains the integrity guarantees of classic PoX while\nenabling its application to existing real-time systems. This is achieved by\nrelaxing the atomicity requirement of PoX while dispatching interference\nattempts from other potentially malicious tasks (or compromised operating\nsystems) executing on the same device. To realize the RT-PoX goal, we develop\nProvable Execution Architecture for Real-Time Systems (PEARTS). To the best of\nour knowledge, PEARTS is the first PoX system that can be directly deployed\nalongside a commodity embedded real-time operating system (FreeRTOS). This\nenables both real-time scheduling and execution integrity guarantees on\ncommodity MCUs. To showcase this capability, we develop a PEARTS open-source\nprototype atop FreeRTOS on a single-core ARM Cortex-M33 processor. We evaluate\nand report on PEARTS security and (modest) overheads.\n","authors":["Antonio Joia Neto","Norrathep Rattanavipanon","Ivan De Oliveira Nunes"],"pdf_url":"https://arxiv.org/pdf/2505.13842v1.pdf","comment":"S&P2025"},{"id":"http://arxiv.org/abs/2412.04756v2","updated":"2025-05-20T02:09:07Z","published":"2024-12-06T03:45:49Z","title":"ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large\n  Language Models","summary":"  The increasing frequency and sophistication of cybersecurity vulnerabilities\nin software systems underscores the need for more robust and effective\nvulnerability assessment methods. However, existing approaches often rely on\nhighly technical and abstract frameworks, which hinder understanding and\nincrease the likelihood of exploitation, resulting in severe cyberattacks. In\nthis paper, we introduce ChatNVD, a support tool powered by Large Language\nModels (LLMs) that leverages the National Vulnerability Database (NVD) to\ngenerate accessible, context-rich summaries of software vulnerabilities. We\ndevelop three variants of ChatNVD, utilizing three prominent LLMs: GPT-4o Mini\nby OpenAI, LLaMA 3 by Meta, and Gemini 1.5 Pro by Google. To evaluate their\nperformance, we conduct a comparative evaluation focused on their ability to\nidentify, interpret, and explain software vulnerabilities. Our results\ndemonstrate that GPT-4o Mini outperforms the other models, achieving over 92%\naccuracy and the lowest error rates, making it the most reliable option for\nreal-world vulnerability assessment.\n","authors":["Shivansh Chopra","Hussain Ahmad","Diksha Goel","Claudia Szabo"],"pdf_url":"https://arxiv.org/pdf/2412.04756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13819v1","updated":"2025-05-20T01:58:43Z","published":"2025-05-20T01:58:43Z","title":"Fragments to Facts: Partial-Information Fragment Inference from LLMs","summary":"  Large language models (LLMs) can leak sensitive training data through\nmemorization and membership inference attacks. Prior work has primarily focused\non strong adversarial assumptions, including attacker access to entire samples\nor long, ordered prefixes, leaving open the question of how vulnerable LLMs are\nwhen adversaries have only partial, unordered sample information. For example,\nif an attacker knows a patient has \"hypertension,\" under what conditions can\nthey query a model fine-tuned on patient data to learn the patient also has\n\"osteoarthritis?\" In this paper, we introduce a more general threat model under\nthis weaker assumption and show that fine-tuned LLMs are susceptible to these\nfragment-specific extraction attacks. To systematically investigate these\nattacks, we propose two data-blind methods: (1) a likelihood ratio attack\ninspired by methods from membership inference, and (2) a novel approach, PRISM,\nwhich regularizes the ratio by leveraging an external prior. Using examples\nfrom both medical and legal settings, we show that both methods are competitive\nwith a data-aware baseline classifier that assumes access to labeled\nin-distribution data, underscoring their robustness.\n","authors":["Lucas Rosenblatt","Bin Han","Robert Wolfe","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2505.13819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13804v1","updated":"2025-05-20T01:34:04Z","published":"2025-05-20T01:34:04Z","title":"QUT-DV25: A Dataset for Dynamic Analysis of Next-Gen Software Supply\n  Chain Attacks","summary":"  Securing software supply chains is a growing challenge due to the inadequacy\nof existing datasets in capturing the complexity of next-gen attacks, such as\nmultiphase malware execution, remote access activation, and dynamic payload\ngeneration. Existing datasets, which rely on metadata inspection and static\ncode analysis, are inadequate for detecting such attacks. This creates a\ncritical gap because these datasets do not capture what happens during and\nafter a package is installed. To address this gap, we present QUT-DV25, a\ndynamic analysis dataset specifically designed to support and advance research\non detecting and mitigating supply chain attacks within the Python Package\nIndex (PyPI) ecosystem. This dataset captures install and post-install-time\ntraces from 14,271 Python packages, of which 7,127 are malicious. The packages\nare executed in an isolated sandbox environment using an extended Berkeley\nPacket Filter (eBPF) kernel and user-level probes. It captures 36 real-time\nfeatures, that includes system calls, network traffic, resource usages,\ndirectory access patterns, dependency logs, and installation behaviors,\nenabling the study of next-gen attack vectors. ML analysis using the QUT-DV25\ndataset identified four malicious PyPI packages previously labeled as benign,\neach with thousands of downloads. These packages deployed covert remote access\nand multi-phase payloads, were reported to PyPI maintainers, and subsequently\nremoved. This highlights the practical value of QUT-DV25, as it outperforms\nreactive, metadata, and static datasets, offering a robust foundation for\ndeveloping and benchmarking advanced threat detection within the evolving\nsoftware supply chain ecosystem.\n","authors":["Sk Tanzir Mehedi","Raja Jurdak","Chadni Islam","Gowri Ramachandran"],"pdf_url":"https://arxiv.org/pdf/2505.13804v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.11541v2","updated":"2025-05-20T00:24:09Z","published":"2025-05-14T13:11:16Z","title":"MorphMark: Flexible Adaptive Watermarking for Large Language Models","summary":"  Watermarking by altering token sampling probabilities based on red-green list\nis a promising method for tracing the origin of text generated by large\nlanguage models (LLMs). However, existing watermark methods often struggle with\na fundamental dilemma: improving watermark effectiveness (the detectability of\nthe watermark) often comes at the cost of reduced text quality. This trade-off\nlimits their practical application. To address this challenge, we first\nformalize the problem within a multi-objective trade-off analysis framework.\nWithin this framework, we identify a key factor that influences the dilemma.\nUnlike existing methods, where watermark strength is typically treated as a\nfixed hyperparameter, our theoretical insights lead to the development of\nMorphMarka method that adaptively adjusts the watermark strength in response to\nchanges in the identified factor, thereby achieving an effective resolution of\nthe dilemma. In addition, MorphMark also prioritizes flexibility since it is a\nmodel-agnostic and model-free watermark method, thereby offering a practical\nsolution for real-world deployment, particularly in light of the rapid\nevolution of AI models. Extensive experiments demonstrate that MorphMark\nachieves a superior resolution of the effectiveness-quality dilemma, while also\noffering greater flexibility and time and space efficiency.\n","authors":["Zongqi Wang","Tianle Gu","Baoyuan Wu","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2505.11541v2.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.14914v1","updated":"2025-05-20T21:06:35Z","published":"2025-05-20T21:06:35Z","title":"Sei Giga","summary":"  We introduce the Sei Giga, a multi-concurrent producer parallelized execution\nEVM layer one blockchain. In an internal testnet Giga has achieved >5\ngigagas/sec throughput and sub 400ms finality. Giga uses Autobahn for consensus\nwith separate DA and consensus layers requiring f+1 votes for a PoA on the DA\nlayer before consensus. Giga reaches consensus over ordering and uses async\nblock execution and state agreement to remove execution from the consensus\nbottleneck.\n","authors":["Benjamin Marsh","Steven Landers","Jayendra Jog"],"pdf_url":"https://arxiv.org/pdf/2505.14914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14898v1","updated":"2025-05-20T20:49:34Z","published":"2025-05-20T20:49:34Z","title":"Topology-aware Detection and Localization of Distributed\n  Denial-of-Service Attacks in Network-on-Chips","summary":"  Network-on-Chip (NoC) enables on-chip communication between diverse cores in\nmodern System-on-Chip (SoC) designs. With its shared communication fabric, NoC\nhas become a focal point for various security threats, especially in\nheterogeneous and high-performance computing platforms. Among these attacks,\nDistributed Denial of Service (DDoS) attacks occur when multiple malicious\nentities collaborate to overwhelm and disrupt access to critical system\ncomponents, potentially causing severe performance degradation or complete\ndisruption of services. These attacks are particularly challenging to detect\ndue to their distributed nature and dynamic traffic patterns in NoC, which\noften evade static detection rules or simple profiling. This paper presents a\nframework to conduct topology-aware detection and localization of DDoS attacks\nusing Graph Neural Networks (GNNs) by analyzing NoC traffic patterns.\nSpecifically, by modeling the NoC as a graph, our method utilizes\nspatiotemporal traffic features to effectively identify and localize DDoS\nattacks. Unlike prior works that rely on handcrafted features or\nthreshold-based detection, our GNN-based approach operates directly on raw\ninter-flit delay data, learning complex traffic dependencies without manual\nintervention. Experimental results demonstrate that our approach can detect and\nlocalize DDoS attacks with high accuracy (up to 99\\%) while maintaining\nconsistent performance under diverse attack strategies. Furthermore, the\nproposed method exhibits strong robustness across varying numbers and\nplacements of malicious IPs, different packet injection rates, application\nworkloads, and architectural configurations, including both 2D mesh and 3D\nTSV-based NoCs. Our work provides a scalable, flexible, and\narchitecture-agnostic defense mechanism, significantly improving the\navailability and trustworthiness of on-chip communication in future SoC\ndesigns.\n","authors":["Hansika Weerasena","Xiaoguo Jia","Prabhat Mishra"],"pdf_url":"https://arxiv.org/pdf/2505.14898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14891v1","updated":"2025-05-20T20:35:00Z","published":"2025-05-20T20:35:00Z","title":"On the (in)security of Proofs-of-Space based Longest-Chain Blockchains","summary":"  The Nakamoto consensus protocol underlying the Bitcoin blockchain uses proof\nof work as a voting mechanism. Honest miners who contribute hashing power\ntowards securing the chain try to extend the longest chain they are aware of.\nDespite its simplicity, Nakamoto consensus achieves meaningful security\nguarantees assuming that at any point in time, a majority of the hashing power\nis controlled by honest parties. This also holds under ``resource\nvariability'', i.e., if the total hashing power varies greatly over time.\n  Proofs of space (PoSpace) have been suggested as a more sustainable\nreplacement for proofs of work. Unfortunately, no construction of a\n``longest-chain'' blockchain based on PoSpace, that is secure under dynamic\navailability, is known. In this work, we prove that without additional\nassumptions no such protocol exists. We exactly quantify this impossibility\nresult by proving a bound on the length of the fork required for double\nspending as a function of the adversarial capabilities. This bound holds for\nany chain selection rule, and we also show a chain selection rule (albeit a\nvery strange one) that almost matches this bound.\n  Concretely, we consider a security game in which the honest parties at any\npoint control $\\phi>1$ times more space than the adversary. The adversary can\nchange the honest space by a factor $1\\pm \\varepsilon$ with every block\n(dynamic availability), and ``replotting'' the space takes as much time as\n$\\rho$ blocks.\n  We prove that no matter what chain selection rule is used, in this game the\nadversary can create a fork of length $\\phi^2\\cdot \\rho / \\varepsilon$ that\nwill be picked as the winner by the chain selection rule.\n  We also provide an upper bound that matches the lower bound up to a factor\n$\\phi$. There exists a chain selection rule which in the above game requires\nforks of length at least $\\phi\\cdot \\rho / \\varepsilon$.\n","authors":["Mirza Ahad Baig","Krzysztof Pietrzak"],"pdf_url":"https://arxiv.org/pdf/2505.14891v1.pdf","comment":"Accepted at Financial Cryptography and Data Security 2025"},{"id":"http://arxiv.org/abs/2505.14835v1","updated":"2025-05-20T18:57:38Z","published":"2025-05-20T18:57:38Z","title":"Robust and Efficient AI-Based Attack Recovery in Autonomous Drones","summary":"  We introduce an autonomous attack recovery architecture to add common sense\nreasoning to plan a recovery action after an attack is detected. We outline\nuse-cases of our architecture using drones, and then discuss how to implement\nthis architecture efficiently and securely in edge devices.\n","authors":["Diego Ortiz Barbosa","Luis Burbano","Siwei Yang","Zijun Wang","Alvaro A. Cardenas","Cihang Xie","Yinzhi Cao"],"pdf_url":"https://arxiv.org/pdf/2505.14835v1.pdf","comment":"Genzero Workshop 2024"},{"id":"http://arxiv.org/abs/2505.14797v1","updated":"2025-05-20T18:08:15Z","published":"2025-05-20T18:08:15Z","title":"Efficient Privacy-Preserving Cross-Silo Federated Learning with\n  Multi-Key Homomorphic Encryption","summary":"  Federated Learning (FL) is susceptible to privacy attacks, such as data\nreconstruction attacks, in which a semi-honest server or a malicious client\ninfers information about other clients' datasets from their model updates or\ngradients. To enhance the privacy of FL, recent studies combined Multi-Key\nHomomorphic Encryption (MKHE) and FL, making it possible to aggregate the\nencrypted model updates using different keys without having to decrypt them.\nDespite the privacy guarantees of MKHE, existing approaches are not well-suited\nfor real-world deployment due to their high computation and communication\noverhead. We propose MASER, an efficient MKHE-based Privacy-Preserving FL\nframework that combines consensus-based model pruning and slicing techniques to\nreduce this overhead. Our experimental results show that MASER is 3.03 to 8.29\ntimes more efficient than existing MKHE-based FL approaches in terms of\ncomputation and communication overhead while maintaining comparable\nclassification accuracy to standard FL algorithms. Compared to a vanilla FL\nalgorithm, the overhead of MASER is only 1.48 to 5 times higher, striking a\ngood balance between privacy, accuracy, and efficiency in both IID and non-IID\nsettings.\n","authors":["Abdullah Al Omar","Xin Yang","Euijin Choo","Omid Ardakanian"],"pdf_url":"https://arxiv.org/pdf/2505.14797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14251v1","updated":"2025-05-20T12:04:29Z","published":"2025-05-20T12:04:29Z","title":"A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable\n  Input","summary":"  We study the problem of differentially private second moment estimation and\npresent a new algorithm that achieve strong privacy-utility trade-offs even for\nworst-case inputs under subsamplability assumptions on the data. We call an\ninput $(m,\\alpha,\\beta)$-subsamplable if a random subsample of size $m$ (or\nlarger) preserves w.p $\\geq 1-\\beta$ the spectral structure of the original\nsecond moment matrix up to a multiplicative factor of $1\\pm \\alpha$. Building\nupon subsamplability, we give a recursive algorithmic framework similar to\nKamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)\nwhile preserving w.h.p. the accuracy of the second moment estimation upto an\narbitrary factor of $(1\\pm\\gamma)$. We then show how to apply our algorithm to\napproximate the second moment matrix of a distribution $\\mathcal{D}$, even when\na noticeable fraction of the input are outliers.\n","authors":["Bar Mahpud","Or Sheffet"],"pdf_url":"https://arxiv.org/pdf/2505.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14024v1","updated":"2025-05-20T07:26:54Z","published":"2025-05-20T07:26:54Z","title":"FedGraM: Defending Against Untargeted Attacks in Federated Learning via\n  Embedding Gram Matrix","summary":"  Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods.\n","authors":["Di Wu","Qian Li","Heng Yang","Yong Han"],"pdf_url":"https://arxiv.org/pdf/2505.14024v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.13393v2","updated":"2025-05-20T09:52:05Z","published":"2025-05-19T17:33:15Z","title":"IG Parser: A Software Package for the Encoding of Institutional\n  Statements using the Institutional Grammar","summary":"  This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocial) norms, and strategies (such as conventions) -- referred to as\ninstitutions -- that govern social systems and operate configurally to describe\ninstitutional systems. To this end, the IG Parser employs a distinctive syntax\nthat ensures rigorous encoding of natural language, while automating the\ntransformation into various formats that support the downstream analysis using\ndiverse analytical techniques. The conceptual core of the IG Parser is an\nassociated syntax, IG Script, that operationalizes the conceptual foundations\nof the Institutional Grammar, and more specifically the Institutional Grammar\n2.0, an analytical paradigm for institutional analysis. This article presents\nthe IG Parser, including its conceptual foundations, the syntax specification\nof IG Script, and its architectural principles. This overview is augmented with\nselective illustrative examples that highlight its use and the associated\nbenefits.\n","authors":["Christopher K. Frantz"],"pdf_url":"https://arxiv.org/pdf/2505.13393v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2505.13358v2","updated":"2025-05-20T14:05:02Z","published":"2025-05-19T16:59:47Z","title":"One-Step Offline Distillation of Diffusion-based Models via Koopman\n  Modeling","summary":"  Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.\n","authors":["Nimrod Berman","Ilan Naiman","Moshe Eliasof","Hedi Zisling","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2505.13358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13346v2","updated":"2025-05-20T14:57:18Z","published":"2025-05-19T16:50:35Z","title":"J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization","summary":"  To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.\n","authors":["Austin Xu","Yilun Zhou","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2505.13346v2.pdf","comment":"25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark"},{"id":"http://arxiv.org/abs/2505.13232v2","updated":"2025-05-20T12:27:33Z","published":"2025-05-19T15:15:35Z","title":"StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment","summary":"  Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.\n","authors":["Younghyun Kim","Jongheon Jeong","Sangkyung Kwak","Kyungmin Lee","Juho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2505.13232v2.pdf","comment":"IJCAI 2025; Code is available at https://github.com/alinlab/StarFT"},{"id":"http://arxiv.org/abs/2505.13126v2","updated":"2025-05-20T13:53:50Z","published":"2025-05-19T13:58:15Z","title":"Zero-Shot Iterative Formalization and Planning in Partially Observable\n  Environments","summary":"  Using LLMs not to predict plans but to formalize an environment into the\nPlanning Domain Definition Language (PDDL) has been shown to improve\nperformance and control. Existing work focuses on fully observable\nenvironments; we tackle the more realistic and challenging partially observable\nenvironments that lack of complete, reliable information. We propose PDDLego+,\na framework to iteratively formalize, plan, grow, and refine PDDL\nrepresentations in a zero-shot manner, without needing access to any existing\ntrajectories. On two textual simulated environments, we show that PDDLego+\nimproves goal reaching success and exhibits robustness against problem\ncomplexity. We also show that the domain knowledge captured after a successful\ntrial can benefit future tasks.\n","authors":["Liancheng Gong","Wang Zhu","Jesse Thomason","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01776v5","updated":"2025-05-20T02:32:33Z","published":"2025-03-03T17:59:48Z","title":"Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation","summary":"  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n","authors":["Tiansheng Wen","Yifei Wang","Zequn Zeng","Zhong Peng","Yudi Su","Xinyang Liu","Bo Chen","Hongwei Liu","Stefanie Jegelka","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2503.01776v5.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2505.13028v2","updated":"2025-05-20T07:34:53Z","published":"2025-05-19T12:12:00Z","title":"Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark\n  Dataset","summary":"  Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.\n","authors":["Sayon Palit","Daniel Woods"],"pdf_url":"https://arxiv.org/pdf/2505.13028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12981v2","updated":"2025-05-20T07:02:05Z","published":"2025-05-19T11:17:46Z","title":"From Assistants to Adversaries: Exploring the Security Risks of Mobile\n  LLM Agents","summary":"  The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation.\n","authors":["Liangxuan Wu","Chao Wang","Tianming Liu","Yanjie Zhao","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14681v1","updated":"2025-05-20T17:59:16Z","published":"2025-05-20T17:59:16Z","title":"Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training","summary":"  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.\n","authors":["Mengru Wang","Xingyu Chen","Yue Wang","Zhiwei He","Jiahao Xu","Tian Liang","Qiuzhi Liu","Yunzhi Yao","Wenxuan Wang","Ruotian Ma","Haitao Mi","Ningyu Zhang","Zhaopeng Tu","Xiaolong Li","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.14681v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.14680v1","updated":"2025-05-20T17:59:13Z","published":"2025-05-20T17:59:13Z","title":"NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search","summary":"  Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.\n","authors":["Sunhao Dai","Wenjie Wang","Liang Pang","Jun Xu","See-Kiong Ng","Ji-Rong Wen","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2505.14680v1.pdf","comment":"SIGIR 2025 Perspective Paper"},{"id":"http://arxiv.org/abs/2505.14673v1","updated":"2025-05-20T17:58:02Z","published":"2025-05-20T17:58:02Z","title":"Training-Free Watermarking for Autoregressive Image Generation","summary":"  Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.\n","authors":["Yu Tong","Zihao Pan","Shuai Yang","Kaiyang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14668v1","updated":"2025-05-20T17:55:25Z","published":"2025-05-20T17:55:25Z","title":"ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory\n  Perceptions","summary":"  Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.\n","authors":["Bufang Yang","Lilin Xu","Liekang Zeng","Kaiwei Liu","Siyang Jiang","Wenrui Lu","Hongkai Chen","Xiaofan Jiang","Guoliang Xing","Zhenyu Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14667v1","updated":"2025-05-20T17:54:54Z","published":"2025-05-20T17:54:54Z","title":"SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early\n  Alignment","summary":"  Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.\n","authors":["Wonje Jeung","Sangyeon Yoon","Minsuk Kahng","Albert No"],"pdf_url":"https://arxiv.org/pdf/2505.14667v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.07158v3","updated":"2025-05-20T17:53:08Z","published":"2025-02-11T00:53:36Z","title":"Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health\n  Records via Multimodal Fused Transformer","summary":"  Early prediction of pediatric cardiac arrest (CA) is critical for timely\nintervention in high-risk intensive care settings. We introduce PedCA-FT, a\nnovel transformer-based framework that fuses tabular view of EHR with the\nderived textual view of EHR to fully unleash the interactions of\nhigh-dimensional risk factors and their dynamics. By employing dedicated\ntransformer modules for each modality view, PedCA-FT captures complex temporal\nand contextual patterns to produce robust CA risk estimates. Evaluated on a\ncurated pediatric cohort from the CHOA-CICU database, our approach outperforms\nten other artificial intelligence models across five key performance metrics\nand identifies clinically meaningful risk factors. These findings underscore\nthe potential of multimodal fusion techniques to enhance early CA detection and\nimprove patient care.\n","authors":["Jiaying Lu","Stephanie R. Brown","Songyuan Liu","Shifan Zhao","Kejun Dong","Del Bold","Michael Fundora","Alaa Aljiffry","Alex Fedorov","Jocelyn Grunwell","Xiao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.07158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14664v1","updated":"2025-05-20T17:52:03Z","published":"2025-05-20T17:52:03Z","title":"AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of\n  Cross-Modal Embeddings","summary":"  Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.\n","authors":["Yilin Ye","Junchao Huang","Xingchen Zeng","Jiazhi Xia","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.14664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15364v3","updated":"2025-05-20T17:50:11Z","published":"2025-04-21T18:12:46Z","title":"KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments","summary":"  We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.\n","authors":["Junyoung Park","Dalton Jones","Matthew J Morse","Raghavv Goel","Mingu Lee","Chris Lott"],"pdf_url":"https://arxiv.org/pdf/2504.15364v3.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.14661v1","updated":"2025-05-20T17:49:46Z","published":"2025-05-20T17:49:46Z","title":"Abacus: A Cost-Based Optimizer for Semantic Operator Systems","summary":"  LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.\n","authors":["Matthew Russo","Sivaprasad Sudhir","Gerardo Vitagliano","Chunwei Liu","Tim Kraska","Samuel Madden","Michael Cafarella"],"pdf_url":"https://arxiv.org/pdf/2505.14661v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.14660v1","updated":"2025-05-20T17:47:04Z","published":"2025-05-20T17:47:04Z","title":"EmoGist: Efficient In-Context Learning for Visual Emotion Understanding","summary":"  In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.\n","authors":["Ronald Seoh","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2505.14660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14659v1","updated":"2025-05-20T17:46:09Z","published":"2025-05-20T17:46:09Z","title":"Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless\n  Networks","summary":"  As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.\n","authors":["Navneet Kaur","Lav Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.14659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14656v1","updated":"2025-05-20T17:43:33Z","published":"2025-05-20T17:43:33Z","title":"Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning","summary":"  While LLMs excel at open-ended reasoning, they often struggle with\ncost-sensitive planning, either treating all actions as having equal cost or\nfailing to stay within strict budgets. In this paper, we introduce\nCost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings\nexplicit cost-awareness into LLM-guided planning. Tight cost constraints push\nthe planner to quickly identify infeasible solutions, while looser constraints\nencourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,\nClaude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their\nperformance in cost-sensitive scenarios. Our experiments suggest that raw LLMs\nsuch as GPT-4.1 often falter under tight budgets, whereas CATS consistently\ndelivers strong performance, achieving higher task success rates and better\ncost efficiency. CATS provides an effective solution for budget-aware\ndecision-making by combining the reasoning power of LLMs with structured\nsearch.\n","authors":["Zihao Zhang","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14654v1","updated":"2025-05-20T17:42:34Z","published":"2025-05-20T17:42:34Z","title":"Beyond Words: Multimodal LLM Knows When to Speak","summary":"  While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.\n","authors":["Zikai Liao","Yi Ouyang","Yi-Lun Lee","Chen-Ping Yu","Yi-Hsuan Tsai","Zhaozheng Yin"],"pdf_url":"https://arxiv.org/pdf/2505.14654v1.pdf","comment":"Project page: https://github.com/lzk901372/MM-When2Speak"},{"id":"http://arxiv.org/abs/2505.11750v2","updated":"2025-05-20T17:42:26Z","published":"2025-05-16T23:22:07Z","title":"Improving Medium Range Severe Weather Prediction through Transformer\n  Post-processing of AI Weather Forecasts","summary":"  Improving the skill of medium-range (1-8 day) severe weather prediction is\ncrucial for mitigating societal impacts. This study introduces a novel approach\nleveraging decoder-only transformer networks to post-process AI-based weather\nforecasts, specifically from the Pangu-Weather model, for improved severe\nweather guidance. Unlike traditional post-processing methods that use a dense\nneural network to predict the probability of severe weather using discrete\nforecast samples, our method treats forecast lead times as sequential\n``tokens'', enabling the transformer to learn complex temporal relationships\nwithin the evolving atmospheric state. We compare this approach against\npost-processing of the Global Forecast System (GFS) using both a traditional\ndense neural network and our transformer, as well as configurations that\nexclude convective parameters to fairly evaluate the impact of using the\nPangu-Weather AI model. Results demonstrate that the transformer-based\npost-processing significantly enhances forecast skill compared to dense neural\nnetworks. Furthermore, AI-driven forecasts, particularly Pangu-Weather\ninitialized from high resolution analysis, exhibit superior performance to GFS\nin the medium-range, even without explicit convective parameters. Our approach\noffers improved accuracy, and reliability, which also provides interpretability\nthrough feature attribution analysis, advancing medium-range severe weather\nprediction capabilities.\n","authors":["Zhanxiang Hua","Ryan Sobash","David John Gagne II","Yingkai Sha","Alexandra Anderson-Frey"],"pdf_url":"https://arxiv.org/pdf/2505.11750v2.pdf","comment":"16 pages, 10 figures; update fix issues with section reference number"},{"id":"http://arxiv.org/abs/2505.14646v1","updated":"2025-05-20T17:34:44Z","published":"2025-05-20T17:34:44Z","title":"CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided\n  Design Code Generation","summary":"  Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.\n","authors":["Anna C. Doris","Md Ferdous Alam","Amin Heyrani Nobari","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2505.14646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14633v1","updated":"2025-05-20T17:24:09Z","published":"2025-05-20T17:24:09Z","title":"Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas","summary":"  Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.\n","authors":["Yu Ying Chiu","Zhilin Wang","Sharan Maiya","Yejin Choi","Kyle Fish","Sydney Levine","Evan Hubinger"],"pdf_url":"https://arxiv.org/pdf/2505.14633v1.pdf","comment":"34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues"},{"id":"http://arxiv.org/abs/2504.14783v2","updated":"2025-05-20T17:22:21Z","published":"2025-04-21T00:46:31Z","title":"How Effective Can Dropout Be in Multiple Instance Learning ?","summary":"  Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout.\n","authors":["Wenhui Zhu","Peijie Qiu","Xiwen Chen","Zhangsihao Yang","Aristeidis Sotiras","Abolfazl Razi","Yalin Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14783v2.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2406.14761v2","updated":"2025-05-20T17:21:03Z","published":"2024-06-20T22:22:28Z","title":"Diffusion-Based Failure Sampling for Evaluating Safety-Critical\n  Autonomous Systems","summary":"  Validating safety-critical autonomous systems in high-dimensional domains\nsuch as robotics presents a significant challenge. Existing black-box\napproaches based on Markov chain Monte Carlo may require an enormous number of\nsamples, while methods based on importance sampling often rely on simple\nparametric families that may struggle to represent the distribution over\nfailures. We propose to sample the distribution over failures using a\nconditional denoising diffusion model, which has shown success in complex\nhigh-dimensional problems such as robotic task planning. We iteratively train a\ndiffusion model to produce state trajectories closer to failure. We demonstrate\nthe effectiveness of our approach on high-dimensional robotic validation tasks,\nimproving sample efficiency and mode coverage compared to existing black-box\ntechniques.\n","authors":["Harrison Delecki","Marc R. Schlichting","Mansur Arief","Anthony Corso","Marcell Vazquez-Chanlatte","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2406.14761v2.pdf","comment":"Appears in IEEE International Conference on Engineering Reliable\n  Autonomous Systems (ERAS) 2025"},{"id":"http://arxiv.org/abs/2505.14629v1","updated":"2025-05-20T17:19:57Z","published":"2025-05-20T17:19:57Z","title":"KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models","summary":"  Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.\n","authors":["Fnu Mohbat","Mohammed J Zaki"],"pdf_url":"https://arxiv.org/pdf/2505.14629v1.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2505.14627v1","updated":"2025-05-20T17:18:17Z","published":"2025-05-20T17:18:17Z","title":"Debating for Better Reasoning: An Unsupervised Multimodal Approach","summary":"  As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.\n","authors":["Ashutosh Adhikari","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2505.14627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14625v1","updated":"2025-05-20T17:16:44Z","published":"2025-05-20T17:16:44Z","title":"TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning","summary":"  Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.\n","authors":["Zhangchen Xu","Yuetai Li","Fengqing Jiang","Bhaskar Ramasubramanian","Luyao Niu","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2505.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07482v2","updated":"2025-05-20T17:09:53Z","published":"2025-01-13T16:58:32Z","title":"TiEBe: Tracking Language Model Recall of Notable Worldwide Events\n  Through Time","summary":"  As the knowledge landscape evolves and large language models (LLMs) become\nincreasingly widespread, there is a growing need to keep these models updated\nwith current events. While existing benchmarks assess general factual recall,\nfew studies explore how LLMs retain knowledge over time or across different\nregions. To address these gaps, we present the Timely Events Benchmark (TiEBe),\na dataset of over 23,000 question-answer pairs centered on notable global and\nregional events, spanning more than 10 years of events, 23 regions, and 13\nlanguages. TiEBe leverages structured retrospective data from Wikipedia to\nidentify notable events through time. These events are then used to construct a\nbenchmark to evaluate LLMs' understanding of global and regional developments,\ngrounded in factual evidence beyond Wikipedia itself. Our results reveal\nsignificant geographic disparities in factual recall, emphasizing the need for\nmore balanced global representation in LLM training. We also observe a Pearson\ncorrelation of more than 0.7 between models' performance in TiEBe and various\ncountries' socioeconomic indicators, such as HDI. In addition, we examine the\nimpact of language on factual recall by posing questions in the native language\nof the region where each event occurred, uncovering substantial performance\ngaps for low-resource languages.\n","authors":["Thales Sales Almeida","Giovana Kerche Bonás","João Guilherme Alves Santos","Hugo Abonizio","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2501.07482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14615v1","updated":"2025-05-20T17:00:22Z","published":"2025-05-20T17:00:22Z","title":"SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle\n  Generation from SAT Formulas","summary":"  We introduce SATBench, a benchmark for evaluating the logical reasoning\ncapabilities of large language models (LLMs) through logical puzzles derived\nfrom Boolean satisfiability (SAT) problems. Unlike prior work that focuses on\ninference rule-based reasoning, which often involves deducing conclusions from\na set of premises, our approach leverages the search-based nature of SAT\nproblems, where the objective is to find a solution that fulfills a specified\nset of logical constraints. Each instance in SATBench is generated from a SAT\nformula, then translated into a story context and conditions using LLMs. The\ngeneration process is fully automated and allows for adjustable difficulty by\nvarying the number of clauses. All 2100 puzzles are validated through both\nLLM-assisted and solver-based consistency checks, with human validation on a\nsubset. Experimental results show that even the strongest model, o4-mini,\nachieves only 65.0% accuracy on hard UNSAT problems, close to the random\nbaseline of 50%. SATBench exposes fundamental limitations in the search-based\nlogical reasoning abilities of current LLMs and provides a scalable testbed for\nfuture research in logical reasoning.\n","authors":["Anjiang Wei","Yuheng Wu","Yingjia Wan","Tarun Suresh","Huanmi Tan","Zhanke Zhou","Sanmi Koyejo","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2505.14615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14608v1","updated":"2025-05-20T16:55:44Z","published":"2025-05-20T16:55:44Z","title":"Language Models Optimized to Fool Detectors Still Have a Distinct Style\n  (And How to Change It)","summary":"  Despite considerable progress in the development of machine-text detectors,\nit has been suggested that the problem is inherently hard, and therefore, that\nstakeholders should proceed under the assumption that machine-generated text\ncannot be reliably detected as such. We examine a recent such claim by Nicks et\nal. (2024) regarding the ease with which language models can be optimized to\ndegrade the performance of machine-text detectors, including detectors not\nspecifically optimized against. We identify a feature space$\\unicode{x2013}$the\nstylistic feature space$\\unicode{x2013}$that is robust to such optimization,\nand show that it may be used to reliably detect samples from language models\noptimized to prevent detection. Furthermore, we show that even when models are\nexplicitly optimized against stylistic detectors, detection performance remains\nsurprisingly unaffected. We then seek to understand if stylistic detectors are\ninherently more robust. To study this question, we explore a new paraphrasing\napproach that simultaneously aims to close the gap between human writing and\nmachine writing in stylistic feature space while avoiding detection using\ntraditional features. We show that when only a single sample is available for\ndetection, this attack is universally effective across all detectors\nconsidered, including those that use writing style. However, as the number of\nsamples available for detection grows, the human and machine distributions\nbecome distinguishable. This observation encourages us to introduce AURA, a\nmetric that estimates the overlap between human and machine-generated\ndistributions by analyzing how detector performance improves as more samples\nbecome available. Overall, our findings underscore previous recommendations to\navoid reliance on machine-text detection.\n","authors":["Rafael Rivera Soto","Barry Chen","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2505.14608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10368v2","updated":"2025-05-20T16:52:53Z","published":"2025-04-14T16:13:23Z","title":"S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models","summary":"  We introduce S1-Bench, a novel benchmark designed to evaluate the performance\nof Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their heavy reliance on system 2 thinking may limit their system 1\nthinking capabilities. However, there is a lack of an appropriate benchmark for\nevaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench\nintroduces a suite of simple, diverse, and natural questions across multiple\ndomains and languages, specifically designed to assess LRMs' performance on\nquestions more suitable for system 1 . We conduct extensive evaluations across\n28 LRMs, revealing their inefficiency, inadequate accuracy, and limited\nrobustness when handling simple questions. Additionally, we observe a gap\nbetween their difficulty perception and generation length. Overall, this work\npaves the way toward dual-system compatibility in the development of LRMs.\n","authors":["Wenyuan Zhang","Shuaiyi Nie","Xinghua Zhang","Zefeng Zhang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10368v2.pdf","comment":"31 pages, 9 figures, 16 tables"},{"id":"http://arxiv.org/abs/2505.14603v1","updated":"2025-05-20T16:52:11Z","published":"2025-05-20T16:52:11Z","title":"Towards a Foundation Model for Communication Systems","summary":"  Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.\n","authors":["Davide Buffelli","Sowmen Das","Yu-Wei Lin","Sattar Vakili","Chien-Yi Wang","Masoud Attarifar","Pritthijit Nath","Da-shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2505.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14599v1","updated":"2025-05-20T16:49:40Z","published":"2025-05-20T16:49:40Z","title":"Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models","summary":"  Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.\n","authors":["Guangzhi Xiong","Eric Xie","Corey Williams","Myles Kim","Amir Hassan Shariatmadari","Sikun Guo","Stefan Bekiranov","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14599v1.pdf","comment":"Accepted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2502.16901v2","updated":"2025-05-20T16:45:00Z","published":"2025-02-24T06:54:50Z","title":"Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs","summary":"  We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT.\n","authors":["Himanshu Beniwal","Sailesh Panda","Birudugadda Srivibhav","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2502.16901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v4","updated":"2025-05-20T16:29:52Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Konstantina Mellou","Marco Molinaro","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17401v3","updated":"2025-05-20T16:29:43Z","published":"2024-08-30T16:36:53Z","title":"Exploring the Effect of Explanation Content and Format on User\n  Comprehension and Trust in Healthcare","summary":"  AI-driven tools for healthcare are widely acknowledged as potentially\nbeneficial to health practitioners and patients, e.g. the QCancer regression\ntool for cancer risk prediction. However, for these tools to be trusted, they\nneed to be supplemented with explanations. We examine how explanations' content\nand format affect user comprehension and trust when explaining QCancer's\npredictions. Regarding content, we deploy SHAP and Occlusion-1. Regarding\nformat, we present SHAP explanations, conventionally, as charts (SC) and\nOcclusion-1 explanations as charts (OC) as well as text (OT), to which their\nsimpler nature lends itself. We conduct experiments with two sets of\nstakeholders: the general public (representing patients) and medical students\n(representing healthcare practitioners). Our experiments showed higher\nsubjective comprehension and trust for Occlusion-1 over SHAP explanations based\non content. However, when controlling for format, only OT outperformed SC,\nsuggesting this trend is driven by preferences for text. Other findings\ncorroborated that explanation format, rather than content, is often the\ncritical factor.\n","authors":["Antonio Rago","Bence Palfi","Purin Sukpanichnant","Hannibal Nabli","Kavyesh Vivek","Olga Kostopoulou","James Kinross","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2408.17401v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2311.16515v4","updated":"2025-05-20T16:29:22Z","published":"2023-11-25T14:24:49Z","title":"Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for\n  Composed Person Retrieval","summary":"  Person retrieval has attracted rising attention. Existing methods are mainly\ndivided into two retrieval modes, namely image-only and text-only. However,\nthey are unable to make full use of the available information and are difficult\nto meet diverse application requirements. To address the above limitations, we\npropose a new Composed Person Retrieval (CPR) task, which combines visual and\ntextual queries to identify individuals of interest from large-scale person\nimage databases. Nevertheless, the foremost difficulty of the CPR task is the\nlack of available annotated datasets. Therefore, we first introduce a scalable\nautomatic data synthesis pipeline, which decomposes complex multimodal data\ngeneration into the creation of textual quadruples followed by\nidentity-consistent image synthesis using fine-tuned generative models.\nMeanwhile, a multimodal filtering method is designed to ensure the resulting\nSynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.\nAdditionally, to improve the representation of composed person queries, we\npropose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework\nthrough fine-grained dynamic alignment and masked feature reasoning. Moreover,\nfor objective evaluation, we manually annotate the Image-Text Composed Person\nRetrieval (ITCPR) test set. The extensive experiments demonstrate the\neffectiveness of the SynCPR dataset and the superiority of the proposed FAFA\nframework when compared with the state-of-the-art methods. All code and data\nwill be provided at\nhttps://github.com/Delong-liu-bupt/Composed_Person_Retrieval.\n","authors":["Delong Liu","Haiwen Li","Zhaohui Hou","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14569v1","updated":"2025-05-20T16:28:08Z","published":"2025-05-20T16:28:08Z","title":"Agent Context Protocols Enhance Collective Inference","summary":"  AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.\n","authors":["Devansh Bhardwaj","Arjun Beniwal","Shreyas Chaudhari","Ashwin Kalyan","Tanmay Rajpurohit","Karthik R. Narasimhan","Ameet Deshpande","Vishvak Murahari"],"pdf_url":"https://arxiv.org/pdf/2505.14569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10940v2","updated":"2025-05-20T16:27:05Z","published":"2025-02-16T01:05:16Z","title":"CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation","summary":"  The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms.\n","authors":["Ziyue Liu","Ruijie Zhang","Zhengyang Wang","Zi Yang","Paul Hovland","Bogdan Nicolae","Franck Cappello","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.10940v2.pdf","comment":"v2"},{"id":"http://arxiv.org/abs/2505.14566v1","updated":"2025-05-20T16:25:41Z","published":"2025-05-20T16:25:41Z","title":"KIPPO: Koopman-Inspired Proximal Policy Optimization","summary":"  Reinforcement Learning (RL) has made significant strides in various domains,\nand policy gradient methods like Proximal Policy Optimization (PPO) have gained\npopularity due to their balance in performance, training stability, and\ncomputational efficiency. These methods directly optimize policies through\ngradient-based updates. However, developing effective control policies for\nenvironments with complex and non-linear dynamics remains a challenge. High\nvariance in gradient estimates and non-convex optimization landscapes often\nlead to unstable learning trajectories. Koopman Operator Theory has emerged as\na powerful framework for studying non-linear systems through an\ninfinite-dimensional linear operator that acts on a higher-dimensional space of\nmeasurement functions. In contrast with their non-linear counterparts, linear\nsystems are simpler, more predictable, and easier to analyze. In this paper, we\npresent Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an\napproximately linear latent-space representation of the underlying system's\ndynamics while retaining essential features for effective policy learning. This\nis achieved through a Koopman-approximation auxiliary network that can be added\nto the baseline policy optimization algorithms without altering the\narchitecture of the core policy or value function. Extensive experimental\nresults demonstrate consistent improvements over the PPO baseline with 6-60%\nincreased performance while reducing variability by up to 91% when evaluated on\nvarious continuous control tasks.\n","authors":["Andrei Cozma","Landon Harris","Hairong Qi"],"pdf_url":"https://arxiv.org/pdf/2505.14566v1.pdf","comment":"Accepted for IJCAI 2025. This arXiv submission is the full version of\n  the conference paper, including the appendix and supplementary material\n  omitted from the IJCAI proceedings"},{"id":"http://arxiv.org/abs/2505.14564v1","updated":"2025-05-20T16:24:42Z","published":"2025-05-20T16:24:42Z","title":"Bellman operator convergence enhancements in reinforcement learning\n  algorithms","summary":"  This paper reviews the topological groundwork for the study of reinforcement\nlearning (RL) by focusing on the structure of state, action, and policy spaces.\nWe begin by recalling key mathematical concepts such as complete metric spaces,\nwhich form the foundation for expressing RL problems. By leveraging the Banach\ncontraction principle, we illustrate how the Banach fixed-point theorem\nexplains the convergence of RL algorithms and how Bellman operators, expressed\nas operators on Banach spaces, ensure this convergence. The work serves as a\nbridge between theoretical mathematics and practical algorithm design, offering\nnew approaches to enhance the efficiency of RL. In particular, we investigate\nalternative formulations of Bellman operators and demonstrate their impact on\nimproving convergence rates and performance in standard RL environments such as\nMountainCar, CartPole, and Acrobot. Our findings highlight how a deeper\nmathematical understanding of RL can lead to more effective algorithms for\ndecision-making problems.\n","authors":["David Krame Kadurha","Domini Jocema Leko Moutouo","Yae Ulrich Gaba"],"pdf_url":"https://arxiv.org/pdf/2505.14564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17388v3","updated":"2025-05-20T16:24:22Z","published":"2024-11-26T12:46:57Z","title":"Can LLMs be Good Graph Judge for Knowledge Graph Construction?","summary":"  In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.\n","authors":["Haoyu Huang","Chong Chen","Zeang Sheng","Yang Li","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.17388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14561v1","updated":"2025-05-20T16:19:34Z","published":"2025-05-20T16:19:34Z","title":"SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised\n  Speaker Verification","summary":"  Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.\n","authors":["Theo Lepage","Reda Dehak"],"pdf_url":"https://arxiv.org/pdf/2505.14561v1.pdf","comment":"accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2502.12466v2","updated":"2025-05-20T16:19:18Z","published":"2025-02-18T02:54:25Z","title":"EquiBench: Benchmarking Large Language Models' Understanding of Program\n  Semantics via Equivalence Checking","summary":"  As large language models (LLMs) become integral to code-related tasks, a\ncentral question emerges: do LLMs truly understand program execution semantics?\nWe introduce EquiBench, a new benchmark for evaluating LLMs through equivalence\nchecking, i.e., determining whether two programs produce identical outputs for\nall possible inputs. Unlike prior code generation benchmarks, this task\ndirectly tests a model's understanding of code execution semantics. EquiBench\nconsists of 2400 program pairs across four languages and six categories. These\npairs are generated through program analysis, compiler scheduling, and\nsuperoptimization, ensuring high-confidence labels, nontrivial difficulty, and\nfull automation. The transformations span syntactic edits, structural\nmodifications, and algorithmic changes, covering a broad spectrum of semantic\nvariation. We evaluate 19 state-of-the-art LLMs and find that in the most\nchallenging categories, the best accuracies are 63.8% and 76.2%, only modestly\nabove the 50% random baseline. Further analysis reveals that models often rely\non syntactic similarity rather than exhibiting robust reasoning over execution\nsemantics, highlighting fundamental limitations.\n","authors":["Anjiang Wei","Jiannan Cao","Ran Li","Hongyu Chen","Yuhui Zhang","Ziheng Wang","Yuan Liu","Thiago S. F. X. Teixeira","Diyi Yang","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2502.12466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18169v4","updated":"2025-05-20T16:15:13Z","published":"2024-12-24T05:07:46Z","title":"KunServe: Efficient Parameter-centric Memory Management for LLM Serving","summary":"  Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept.\n","authors":["Rongxin Cheng","Yuxin Lai","Xingda Wei","Rong Chen","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.18169v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14555v1","updated":"2025-05-20T16:13:20Z","published":"2025-05-20T16:13:20Z","title":"Physics-Guided Learning of Meteorological Dynamics for Weather\n  Downscaling and Forecasting","summary":"  Weather forecasting is essential but remains computationally intensive and\nphysically incomplete in traditional numerical weather prediction (NWP)\nmethods. Deep learning (DL) models offer efficiency and accuracy but often\nignore physical laws, limiting interpretability and generalization. We propose\nPhyDL-NWP, a physics-guided deep learning framework that integrates physical\nequations with latent force parameterization into data-driven models. It\npredicts weather variables from arbitrary spatiotemporal coordinates, computes\nphysical terms via automatic differentiation, and uses a physics-informed loss\nto align predictions with governing dynamics. PhyDL-NWP enables resolution-free\ndownscaling by modeling weather as a continuous function and fine-tunes\npre-trained models with minimal overhead, achieving up to 170x faster inference\nwith only 55K parameters. Experiments show that PhyDL-NWP improves both\nforecasting performance and physical consistency.\n","authors":["Yingtao Luo","Shikai Fang","Binqing Wu","Qingsong Wen","Liang Sun"],"pdf_url":"https://arxiv.org/pdf/2505.14555v1.pdf","comment":"Published/Accepted in KDD 2025 (February Cycle)"},{"id":"http://arxiv.org/abs/2505.14551v1","updated":"2025-05-20T16:06:25Z","published":"2025-05-20T16:06:25Z","title":"Trustworthy Reputation Games and Applications to Proof-of-Reputation\n  Blockchains","summary":"  Reputation systems play an essential role in the Internet era, as they enable\npeople to decide whom to trust, by collecting and aggregating data about users'\nbehavior. Recently, several works proposed the use of reputation for the design\nand scalability improvement of decentralized (blockchain) ledgers; however,\nsuch systems are prone to manipulation and to our knowledge no game-theoretic\ntreatment exists that can support their economic robustness.\n  In this work we put forth a new model for the design of what we call, {\\em\ntrustworthy reputation systems}. Concretely, we describe a class of games,\nwhich we term {\\em trustworthy reputation games}, that enable a set of users to\nreport a function of their beliefs about the trustworthiness of each server in\na set -- i.e., their estimate of the probability that this server will behave\naccording to its specified strategy -- in a way that satisfies the following\nproperties:\n  1. It is $(\\epsilon$-)best response for any rational user in the game to play\na prescribed (truthful) strategy according to their true belief.\n  2. Assuming that the users' beliefs are not too far from the {\\em true}\ntrustworthiness of the servers, playing the above ($\\epsilon-$)Nash equilibrium\nallows anyone who observes the users' strategies to estimate the relative\ntrustworthiness of any two servers.\n  Our utilities and decoding function build on a connection between the well\nknown PageRank algorithm and the problem of trustworthiness discovery, which\ncan be of independent interest. Finally, we show how the above games are\nmotivated by and can be leveraged in proof-of-reputation (PoR) blockchains.\n","authors":["Petros Drineas","Rohit Nema","Rafail Ostrovsky","Vassilis Zikas"],"pdf_url":"https://arxiv.org/pdf/2505.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14549v1","updated":"2025-05-20T16:05:05Z","published":"2025-05-20T16:05:05Z","title":"Can Large Language Models Really Recognize Your Name?","summary":"  Large language models (LLMs) are increasingly being used to protect sensitive\nuser data. However, current LLM-based privacy solutions assume that these\nmodels can reliably detect personally identifiable information (PII),\nparticularly named entities. In this paper, we challenge that assumption by\nrevealing systematic failures in LLM-based privacy tasks. Specifically, we show\nthat modern LLMs regularly overlook human names even in short text snippets due\nto ambiguous contexts, which cause the names to be misinterpreted or\nmishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous\nhuman names, leveraging the name regularity bias phenomenon, embedded within\nconcise text snippets along with benign prompt injections. Our experiments on\nmodern LLMs tasked to detect PII as well as specialized tools show that recall\nof ambiguous names drops by 20--40% compared to more recognizable names.\nFurthermore, ambiguous human names are four times more likely to be ignored in\nsupposedly privacy-preserving summaries generated by LLMs when benign prompt\ninjections are present. These findings highlight the underexplored risks of\nrelying solely on LLMs to safeguard user privacy and underscore the need for a\nmore systematic investigation into their privacy failure modes.\n","authors":["Dzung Pham","Peter Kairouz","Niloofar Mireshghallah","Eugene Bagdasarian","Chau Minh Pham","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2505.14549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14544v1","updated":"2025-05-20T15:59:44Z","published":"2025-05-20T15:59:44Z","title":"Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic\n  Signal Optimization: A Simulation Study","summary":"  Urban traffic congestion, particularly at intersections, significantly\nimpacts travel time, fuel consumption, and emissions. Traditional fixed-time\nsignal control systems often lack the adaptability to manage dynamic traffic\npatterns effectively. This study explores the application of multi-agent\nreinforcement learning (MARL) to optimize traffic signal coordination across\nmultiple intersections within a simulated environment. Utilizing Pygame, a\nsimulation was developed to model a network of interconnected intersections\nwith randomly generated vehicle flows to reflect realistic traffic variability.\nA decentralized MARL controller was implemented, in which each traffic signal\noperates as an autonomous agent, making decisions based on local observations\nand information from neighboring agents. Performance was evaluated against a\nbaseline fixed-time controller using metrics such as average vehicle wait time\nand overall throughput. The MARL approach demonstrated statistically\nsignificant improvements, including reduced average waiting times and improved\nthroughput. These findings suggest that MARL-based dynamic control strategies\nhold substantial promise for improving urban traffic management efficiency.\nMore research is recommended to address scalability and real-world\nimplementation challenges.\n","authors":["Saahil Mahato"],"pdf_url":"https://arxiv.org/pdf/2505.14544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14539v1","updated":"2025-05-20T15:56:34Z","published":"2025-05-20T15:56:34Z","title":"A Logic of General Attention Using Edge-Conditioned Event Models\n  (Extended Version)","summary":"  In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.\n","authors":["Gaia Belardinelli","Thomas Bolander","Sebastian Watzl"],"pdf_url":"https://arxiv.org/pdf/2505.14539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12909v2","updated":"2025-05-20T15:54:36Z","published":"2025-05-19T09:45:18Z","title":"Sinusoidal Initialization, Time for a New Start","summary":"  Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.9% in final\nvalidation accuracy and 20.9% in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems.\n","authors":["Alberto Fernández-Hernández","Jose I. Mestre","Manuel F. Dolz","Jose Duato","Enrique S. Quintana-Ortí"],"pdf_url":"https://arxiv.org/pdf/2505.12909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14533v1","updated":"2025-05-20T15:52:43Z","published":"2025-05-20T15:52:43Z","title":"Energy-Efficient Deep Reinforcement Learning with Spiking Transformers","summary":"  Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.\n","authors":["Mohammad Irfan Uddin","Nishad Tasnim","Md Omor Faruk","Zejian Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14526v1","updated":"2025-05-20T15:48:23Z","published":"2025-05-20T15:48:23Z","title":"NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based\n  Autonomous Navigation","summary":"  Autonomous robots must navigate and operate in diverse environments, from\nterrestrial and aquatic settings to aerial and space domains. While\nReinforcement Learning (RL) has shown promise in training policies for specific\nautonomous robots, existing benchmarks are often constrained to unique\nplatforms, limiting generalization and fair comparisons across different\nmobility systems. In this paper, we present NavBench, a multi-domain benchmark\nfor training and evaluating RL-based navigation policies across diverse robotic\nplatforms and operational environments. Built on IsaacLab, our framework\nstandardizes task definitions, enabling different robots to tackle various\nnavigation challenges without the need for ad-hoc task redesigns or custom\nevaluation metrics. Our benchmark addresses three key challenges: (1) Unified\ncross-medium benchmarking, enabling direct evaluation of diverse actuation\nmethods (thrusters, wheels, water-based propulsion) in realistic environments;\n(2) Scalable and modular design, facilitating seamless robot-task\ninterchangeability and reproducible training pipelines; and (3) Robust\nsim-to-real validation, demonstrated through successful policy transfer to\nmultiple real-world robots, including a satellite robotic simulator, an\nunmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency\nbetween simulation and real-world deployment, NavBench simplifies the\ndevelopment of adaptable RL-based navigation strategies. Its modular design\nallows researchers to easily integrate custom robots and tasks by following the\nframework's predefined templates, making it accessible for a wide range of\napplications. Our code is publicly available at NavBench.\n","authors":["Matteo El-Hariry","Antoine Richard","Ricard M. Castan","Luis F. W. Batista","Matthieu Geist","Cedric Pradalier","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2505.14526v1.pdf","comment":"Submitted for publication. Under review (2025)"},{"id":"http://arxiv.org/abs/2502.11051v3","updated":"2025-05-20T15:47:22Z","published":"2025-02-16T09:23:50Z","title":"MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of\n  Multimodal Large Language Models","summary":"  Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient ascent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.\n","authors":["Jiahao Huo","Yibo Yan","Xu Zheng","Yuanhuiyi Lyu","Xin Zou","Zhihua Wei","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11051v3.pdf","comment":"Accepted as ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.14524v1","updated":"2025-05-20T15:46:59Z","published":"2025-05-20T15:46:59Z","title":"Guarded Query Routing for Large Language Models","summary":"  Query routing, the task to route user queries to different large language\nmodel (LLM) endpoints, can be considered as a text classification problem.\nHowever, out-of-distribution queries must be handled properly, as those could\nbe questions about unrelated domains, queries in other languages, or even\ncontain unsafe text. Here, we thus study a \\emph{guarded} query routing\nproblem, for which we first introduce the Guarded Query Routing Benchmark\n(GQR-Bench), which covers three exemplary target domains (law, finance, and\nhealthcare), and seven datasets to test robustness against out-of-distribution\nqueries. We then use GQR-Bench to contrast the effectiveness and efficiency of\nLLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),\nstandard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo\nGuardrails), continuous bag-of-words classifiers (WideMLP, fastText), and\ntraditional machine learning models (SVM, XGBoost). Our results show that\nWideMLP, enhanced with out-of-domain detection capabilities, yields the best\ntrade-off between accuracy (88\\%) and speed (<4ms). The embedding-based\nfastText excels at speed (<1ms) with acceptable accuracy (80\\%), whereas LLMs\nyield the highest accuracy (91\\%) but are comparatively slow (62ms for local\nLlama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge\nthe automatic reliance on LLMs for (guarded) query routing and provide concrete\nrecommendations for practical applications. GQR-Bench will be released as a\nPython package -- \\texttt{gqr}.\n","authors":["Richard Šléher","William Brach","Tibor Sloboda","Kristián Košťál","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2505.14524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14523v1","updated":"2025-05-20T15:46:44Z","published":"2025-05-20T15:46:44Z","title":"Exploring Graph Representations of Logical Forms for Language Modeling","summary":"  We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.\n","authors":["Michael Sullivan"],"pdf_url":"https://arxiv.org/pdf/2505.14523v1.pdf","comment":"To be published in ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.14513v1","updated":"2025-05-20T15:41:05Z","published":"2025-05-20T15:41:05Z","title":"Latent Flow Transformer","summary":"  Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.\n","authors":["Yen-Chen Wu","Feng-Ting Liao","Meng-Hsi Chen","Pei-Chen Ho","Farhang Nabiei","Da-shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2505.14513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14510v1","updated":"2025-05-20T15:39:05Z","published":"2025-05-20T15:39:05Z","title":"BACON: A fully explainable AI model with graded logic for decision\n  making problems","summary":"  As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.\n","authors":["Haishi Bai","Jozo Dujmovic","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14505v1","updated":"2025-05-20T15:34:36Z","published":"2025-05-20T15:34:36Z","title":"ModRWKV: Transformer Multimodality in Linear Time","summary":"  Currently, most multimodal studies are based on large language models (LLMs)\nwith quadratic-complexity Transformer architectures. While linear models like\nRNNs enjoy low inference costs, their application has been largely limited to\nthe text-only modality. This work explores the capabilities of modern RNN\narchitectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal\nframework built upon the RWKV7 architecture as its LLM backbone-which achieves\nmulti-source information fusion through dynamically adaptable heterogeneous\nmodality encoders. We designed the multimodal modules in ModRWKV with an\nextremely lightweight architecture and, through extensive experiments,\nidentified a configuration that achieves an optimal balance between performance\nand computational efficiency. ModRWKV leverages the pretrained weights of the\nRWKV7 LLM for initialization, which significantly accelerates multimodal\ntraining. Comparative experiments with different pretrained checkpoints further\ndemonstrate that such initialization plays a crucial role in enhancing the\nmodel's ability to understand multimodal signals. Supported by extensive\nexperiments, we conclude that modern RNN architectures present a viable\nalternative to Transformers in the domain of multimodal large language models\n(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV\narchitecture through systematic exploration.\n","authors":["Jiale Kang","Ziyin Yue","Qingyu Yin","Jiang Rui","Weile Li","Zening Lu","Zhouran Ji"],"pdf_url":"https://arxiv.org/pdf/2505.14505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09083v2","updated":"2025-05-20T15:29:37Z","published":"2024-10-06T08:33:39Z","title":"Evaluating the Correctness of Inference Patterns Used by LLMs for\n  Judgment","summary":"  This paper presents a method to analyze the inference patterns used by Large\nLanguage Models (LLMs) for judgment in a case study on legal LLMs, so as to\nidentify potential incorrect representations of the LLM, according to human\ndomain knowledge. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed inference\npatterns of an LLM behind its seemingly correct outputs. To this end, we\nquantify the interactions between input phrases used by the LLM as primitive\ninference patterns, because recent theoretical achievements have proven several\nmathematical guarantees of the faithfulness of the interaction-based\nexplanation. We design a set of metrics to evaluate the detailed inference\npatterns of LLMs. Experiments show that even when the language generation\nresults appear correct, a significant portion of the inference patterns used by\nthe LLM for the legal judgment may represent misleading or irrelevant logic.\n","authors":["Lu Chen","Yuxuan Huang","Yixing Li","Dongrui Liu","Qihan Ren","Shuai Zhao","Kun Kuang","Zilong Zheng","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09083v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14499v1","updated":"2025-05-20T15:28:26Z","published":"2025-05-20T15:28:26Z","title":"Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated\n  Rationales","summary":"  There has been growing interest in Multimodal Aspect-Based Sentiment Analysis\n(MABSA) in recent years. Existing methods predominantly rely on pre-trained\nsmall language models (SLMs) to collect information related to aspects and\nsentiments from both image and text, with an aim to align these two modalities.\nHowever, small SLMs possess limited capacity and knowledge, often resulting in\ninaccurate identification of meaning, aspects, sentiments, and their\ninterconnections in textual and visual data. On the other hand, Large language\nmodels (LLMs) have shown exceptional capabilities in various tasks by\neffectively exploring fine-grained information in multimodal data. However,\nsome studies indicate that LLMs still fall short compared to fine-tuned small\nmodels in the field of ABSA. Based on these findings, we propose a novel\nframework, termed LRSA, which combines the decision-making capabilities of SLMs\nwith additional information provided by LLMs for MABSA. Specifically, we inject\nexplanations generated by LLMs as rationales into SLMs and employ a dual\ncross-attention mechanism for enhancing feature interaction and fusion, thereby\naugmenting the SLMs' ability to identify aspects and sentiments. We evaluated\nour method using two baseline models, numerous experiments highlight the\nsuperiority of our approach on three widely-used benchmarks, indicating its\ngeneralizability and applicability to most pre-trained models for MABSA.\n","authors":["Jun Cao","Jiyi Li","Ziwei Yang","Renjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06993v2","updated":"2025-05-20T15:25:12Z","published":"2025-05-11T14:37:30Z","title":"Technical Report: Quantifying and Analyzing the Generalization Power of\n  a DNN","summary":"  This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.\n","authors":["Yuxuan He","Junpeng Zhang","Lei Cheng","Hongyuan Zhang","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.06993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09948v3","updated":"2025-05-20T15:19:09Z","published":"2024-03-15T01:18:08Z","title":"RadCLIP: Enhancing Radiologic Image Analysis through Contrastive\n  Language-Image Pre-training","summary":"  The integration of artificial intelligence (AI) with radiology marks a\ntransformative era in medicine. Vision foundation models have been adopted to\nenhance radiologic imaging analysis. However, the distinct complexities of\nradiologic 2D and 3D radiologic data pose unique challenges that existing\nmodels, pre-trained on general non-medical images, fail to address adequately.\nTo bridge this gap and capitalize on the diagnostic precision required in\nradiologic imaging, we introduce Radiologic Contrastive Language-Image\nPre-training (RadCLIP): a cross-modal vision-language foundational model that\nharnesses Vision Language Pre-training (VLP) framework to improve radiologic\nimage analysis. Building upon Contrastive Language-Image Pre-training (CLIP),\nRadCLIP incorporates a slice pooling mechanism tailored for volumetric image\nanalysis and is pre-trained using a large and diverse dataset of radiologic\nimage-text pairs. The RadCLIP was pre-trained to effectively align radiologic\nimages with their corresponding text annotations, creating a robust vision\nbackbone for radiologic images. Extensive experiments demonstrate RadCLIP's\nsuperior performance in both uni-modal radiologic image classification and\ncross-modal image-text matching, highlighting its significant promise for\nimproving diagnostic accuracy and efficiency in clinical settings. Our Key\ncontributions include curating a large dataset with diverse radiologic 2D/3D\nradiologic image-text pairs, a slice pooling adapter using an attention\nmechanism for integrating 2D images, and comprehensive evaluations of RadCLIP\non various radiologic downstream tasks.\n","authors":["Zhixiu Lu","Hailong Li","Nehal A. Parikh","Jonathan R. Dillman","Lili He"],"pdf_url":"https://arxiv.org/pdf/2403.09948v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14489v1","updated":"2025-05-20T15:19:00Z","published":"2025-05-20T15:19:00Z","title":"Reasoning Models Better Express Their Confidence","summary":"  Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.\n","authors":["Dongkeun Yoon","Seungone Kim","Sohee Yang","Sunkyoung Kim","Soyeon Kim","Yongil Kim","Eunbi Choi","Yireun Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2505.14489v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.02508v2","updated":"2025-05-20T15:17:39Z","published":"2024-12-03T15:39:05Z","title":"Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation\n  Benchmark","summary":"  Producing emotionally dynamic 3D facial avatars with text derived from spoken\nwords (Emo3D) has been a pivotal research topic in 3D avatar generation. While\nprogress has been made in general-purpose 3D avatar generation, the exploration\nof generating emotional 3D avatars remains scarce, primarily due to the\ncomplexities of identifying and rendering rich emotions from spoken words. This\npaper reexamines Emo3D generation and draws inspiration from human processes,\nbreaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping\n(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in\ndetermining the quality of Emo3D generation and encompasses three key\nchallenges: Expression Diversity, Emotion-Content Consistency, and Expression\nFluidity. To address these challenges, we introduce a novel benchmark to\nadvance research in Emo3D generation. First, we present EmoAva, a large-scale,\nhigh-quality dataset for T3DEM, comprising 15,000 text-to-3D expression\nmappings that characterize the aforementioned three challenges in Emo3D\ngeneration. Furthermore, we develop various metrics to effectively evaluate\nmodels against these identified challenges. Next, to effectively model the\nconsistency, diversity, and fluidity of human expressions in the T3DEM step, we\npropose the Continuous Text-to-Expression Generator, which employs an\nautoregressive Conditional Variational Autoencoder for expression code\ngeneration, enhanced with Latent Temporal Attention and Expression-wise\nAttention mechanisms. Finally, to further enhance the 3DAR step on rendering\nhigher-quality subtle expressions, we present the Globally-informed Gaussian\nAvatar (GiGA) model. GiGA incorporates a global information mechanism into 3D\nGaussian representations, enabling the capture of subtle micro-expressions and\nseamless transitions between emotional states.\n","authors":["Haidong Xu","Meishan Zhang","Hao Ju","Zhedong Zheng","Erik Cambria","Min Zhang","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2412.02508v2.pdf","comment":"19 pages. Project website: https://github.com/WalkerMitty/EmoAva"},{"id":"http://arxiv.org/abs/2505.14479v1","updated":"2025-05-20T15:13:32Z","published":"2025-05-20T15:13:32Z","title":"Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach","summary":"  Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.\n","authors":["Oren Sultan","Eitan Stern","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2505.14479v1.pdf","comment":"long paper"},{"id":"http://arxiv.org/abs/2409.17994v5","updated":"2025-05-20T15:08:41Z","published":"2024-09-26T16:06:38Z","title":"CRoP: Context-wise Robust Static Human-Sensing Personalization","summary":"  The advancement in deep learning and internet-of-things have led to diverse\nhuman sensing applications. However, distinct patterns in human sensing,\ninfluenced by various factors or contexts, challenge the generic neural network\nmodel's performance due to natural distribution shifts. To address this,\npersonalization tailors models to individual users. Yet most personalization\nstudies overlook intra-user heterogeneity across contexts in sensory data,\nlimiting intra-user generalizability. This limitation is especially critical in\nclinical applications, where limited data availability hampers both\ngeneralizability and personalization. Notably, intra-user sensing attributes\nare expected to change due to external factors such as treatment progression,\nfurther complicating the challenges. To address the intra-user generalization\nchallenge, this work introduces CRoP, a novel static personalization approach.\nCRoP leverages off-the-shelf pre-trained models as generic starting points and\ncaptures user-specific traits through adaptive pruning on a minimal sub-network\nwhile allowing generic knowledge to be incorporated in remaining parameters.\nCRoP demonstrates superior personalization effectiveness and intra-user\nrobustness across four human-sensing datasets, including two from real-world\nhealth domains, underscoring its practical and social impact. Additionally, to\nsupport CRoP's generalization ability and design choices, we provide empirical\njustification through gradient inner product analysis, ablation studies, and\ncomparisons against state-of-the-art baselines.\n","authors":["Sawinder Kaur","Avery Gump","Yi Xiao","Jingyu Xin","Harshit Sharma","Nina R Benway","Jonathan L Preston","Asif Salekin"],"pdf_url":"https://arxiv.org/pdf/2409.17994v5.pdf","comment":"34 pages, 6 figues and 15 tables"},{"id":"http://arxiv.org/abs/2505.14469v1","updated":"2025-05-20T15:05:03Z","published":"2025-05-20T15:05:03Z","title":"Attributional Safety Failures in Large Language Models under Code-Mixed\n  Perturbations","summary":"  Recent advancements in LLMs have raised significant safety concerns,\nparticularly when dealing with code-mixed inputs and outputs. Our study\nsystematically investigates the increased susceptibility of LLMs to produce\nunsafe outputs from code-mixed prompts compared to monolingual English prompts.\nUtilizing explainability methods, we dissect the internal attribution shifts\ncausing model's harmful behaviors. In addition, we explore cultural dimensions\nby distinguishing between universally unsafe and culturally-specific unsafe\nqueries. This paper presents novel experimental insights, clarifying the\nmechanisms driving this phenomenon.\n","authors":["Somnath Banerjee","Pratyush Chatterjee","Shanu Kumar","Sayan Layek","Parag Agrawal","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2505.14469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10657v2","updated":"2025-05-20T14:59:21Z","published":"2025-03-08T04:07:07Z","title":"RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore\n  Model-level Scaling Up in LLMs","summary":"  Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial.\n","authors":["Zhongzhan Huang","Guoming Ling","Yupei Lin","Yandong Chen","Shanshan Zhong","Hefeng Wu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2503.10657v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.18463v3","updated":"2025-05-20T14:55:01Z","published":"2024-11-26T15:13:17Z","title":"Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive\n  Extension","summary":"  Peptides, short chains of amino acids, interact with target proteins, making\nthem a unique class of protein-based therapeutics for treating human diseases.\nRecently, deep generative models have shown great promise in peptide\ngeneration. However, several challenges remain in designing effective peptide\nbinders. First, not all residues contribute equally to peptide-target\ninteractions. Second, the generated peptides must adopt valid geometries due to\nthe constraints of peptide bonds. Third, realistic tasks for peptide drug\ndevelopment are still lacking. To address these challenges, we introduce\nPepHAR, a hot-spot-driven autoregressive generative model for designing\npeptides targeting specific proteins. Building on the observation that certain\nhot spot residues have higher interaction potentials, we first use an\nenergy-based density model to fit and sample these key residues. Next, to\nensure proper peptide geometry, we autoregressively extend peptide fragments by\nestimating dihedral angles between residue frames. Finally, we apply an\noptimization process to iteratively refine fragment assembly, ensuring correct\npeptide structures. By combining hot spot sampling with fragment-based\nextension, our approach enables de novo peptide design tailored to a target\nprotein and allows the incorporation of key hot spot residues into peptide\nscaffolds. Extensive experiments, including peptide design and peptide scaffold\ngeneration, demonstrate the strong potential of PepHAR in computational peptide\nbinder design. Source code will be available at\nhttps://github.com/Ced3-han/PepHAR.\n","authors":["Jiahan Li","Tong Chen","Shitong Luo","Chaoran Cheng","Jiaqi Guan","Ruihan Guo","Sheng Wang","Ge Liu","Jian Peng","Jianzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2411.18463v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.14455v1","updated":"2025-05-20T14:52:41Z","published":"2025-05-20T14:52:41Z","title":"CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block\n  Prediction and Controllable Generation","summary":"  Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.\n","authors":["Chihan Huang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07078v2","updated":"2025-05-20T14:51:24Z","published":"2025-05-11T18:02:21Z","title":"Can LLM-based Financial Investing Strategies Outperform the Market in\n  Long Run?","summary":"  Large Language Models (LLMs) have recently been leveraged for asset pricing\ntasks and stock trading applications, enabling AI agents to generate investment\ndecisions from unstructured financial data. However, most evaluations of LLM\ntiming-based investing strategies are conducted on narrow timeframes and\nlimited stock universes, overstating effectiveness due to survivorship and\ndata-snooping biases. We critically assess their generalizability and\nrobustness by proposing FINSABER, a backtesting framework evaluating\ntiming-based strategies across longer periods and a larger universe of symbols.\nSystematic backtests over two decades and 100+ symbols reveal that previously\nreported LLM advantages deteriorate significantly under broader cross-section\nand over a longer-term evaluation. Our market regime analysis further\ndemonstrates that LLM strategies are overly conservative in bull markets,\nunderperforming passive benchmarks, and overly aggressive in bear markets,\nincurring heavy losses. These findings highlight the need to develop LLM\nstrategies that are able to prioritise trend detection and regime-aware risk\ncontrols over mere scaling of framework complexity.\n","authors":["Weixian Waylon Li","Hyeonjun Kim","Mihai Cucuringu","Tiejun Ma"],"pdf_url":"https://arxiv.org/pdf/2505.07078v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2505.14451v1","updated":"2025-05-20T14:51:07Z","published":"2025-05-20T14:51:07Z","title":"RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data\n  Imputation","summary":"  Missing values in high-dimensional, mixed-type datasets pose significant\nchallenges for data imputation, particularly under Missing Not At Random (MNAR)\nmechanisms. Existing methods struggle to integrate local and global data\ncharacteristics, limiting performance in MNAR and high-dimensional settings. We\npropose an innovative framework, RefiDiff, combining local machine learning\npredictions with a novel Mamba-based denoising network capturing\ninterrelationships among distant features and samples. Our approach leverages\npre-refinement for initial warm-up imputations and post-refinement to polish\nresults, enhancing stability and accuracy. By encoding mixed-type data into\nunified tokens, RefiDiff enables robust imputation without architectural or\nhyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods\nacross missing-value settings, excelling in MNAR with a 4x faster training time\nthan SOTA DDPM-based approaches. Extensive evaluations on nine real-world\ndatasets demonstrate its robustness, scalability, and effectiveness in handling\ncomplex missingness patterns.\n","authors":["Md Atik Ahamed","Qiang Ye","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.14451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08828v2","updated":"2025-05-20T14:49:55Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v2.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2505.11983v2","updated":"2025-05-20T14:49:41Z","published":"2025-05-17T12:31:12Z","title":"Online Iterative Self-Alignment for Radiology Report Generation","summary":"  Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.\n","authors":["Ting Xiao","Lei Shi","Yang Zhang","HaoFeng Yang","Zhe Wang","Chenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2505.11983v2.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.14442v1","updated":"2025-05-20T14:43:41Z","published":"2025-05-20T14:43:41Z","title":"Creative Preference Optimization","summary":"  While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.\n","authors":["Mete Ismayilzada","Antonio Laverghetta Jr.","Simone A. Luchini","Reet Patel","Antoine Bosselut","Lonneke van der Plas","Roger Beaty"],"pdf_url":"https://arxiv.org/pdf/2505.14442v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2505.14436v1","updated":"2025-05-20T14:42:03Z","published":"2025-05-20T14:42:03Z","title":"Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models","summary":"  Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.\n","authors":["Yuqiao Tan","Shizhu He","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.14436v1.pdf","comment":"Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility"},{"id":"http://arxiv.org/abs/2505.14435v1","updated":"2025-05-20T14:41:56Z","published":"2025-05-20T14:41:56Z","title":"Choosing a Model, Shaping a Future: Comparing LLM Perspectives on\n  Sustainability and its Relationship with AI","summary":"  As organizations increasingly rely on AI systems for decision support in\nsustainability contexts, it becomes critical to understand the inherent biases\nand perspectives embedded in Large Language Models (LLMs). This study\nsystematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,\nGPT, LLaMA, and Mistral - conceptualize sustainability and its relationship\nwith AI. We administered validated, psychometric sustainability-related\nquestionnaires - each 100 times per model -- to capture response patterns and\nvariability. Our findings revealed significant inter-model differences: For\nexample, GPT exhibited skepticism about the compatibility of AI and\nsustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect\nscores for several Sustainable Development Goals (SDGs). Models also diverged\nin attributing institutional responsibility for AI and sustainability\nintegration, a results that holds implications for technology governance\napproaches. Our results demonstrate that model selection could substantially\ninfluence organizational sustainability strategies, highlighting the need for\nawareness of model-specific biases when deploying LLMs for\nsustainability-related decision-making.\n","authors":["Annika Bush","Meltem Aksoy","Markus Pauly","Greta Ontrup"],"pdf_url":"https://arxiv.org/pdf/2505.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14428v1","updated":"2025-05-20T14:38:39Z","published":"2025-05-20T14:38:39Z","title":"Interpretable Neural System Dynamics: Combining Deep Learning with\n  System Dynamics Modeling to Support Critical Applications","summary":"  The objective of this proposal is to bridge the gap between Deep Learning\n(DL) and System Dynamics (SD) by developing an interpretable neural system\ndynamics framework. While DL excels at learning complex models and making\naccurate predictions, it lacks interpretability and causal reliability.\nTraditional SD approaches, on the other hand, provide transparency and causal\ninsights but are limited in scalability and require extensive domain knowledge.\nTo overcome these limitations, this project introduces a Neural System Dynamics\npipeline, integrating Concept-Based Interpretability, Mechanistic\nInterpretability, and Causal Machine Learning. This framework combines the\npredictive power of DL with the interpretability of traditional SD models,\nresulting in both causal reliability and scalability. The efficacy of the\nproposed pipeline will be validated through real-world applications of the\nEU-funded AutoMoTIF project, which is focused on autonomous multimodal\ntransportation systems. The long-term goal is to collect actionable insights\nthat support the integration of explainability and safety in autonomous\nsystems.\n","authors":["Riccardo D'Elia"],"pdf_url":"https://arxiv.org/pdf/2505.14428v1.pdf","comment":"To be submitted to CEUR-WS.org for publication in the Doctoral\n  Consortium Proceedings of XAI 2025, The World Conference on Explainable\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2504.14150v2","updated":"2025-05-20T14:36:36Z","published":"2025-04-19T02:51:20Z","title":"Walk the Talk? Measuring the Faithfulness of Large Language Model\n  Explanations","summary":"  Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.\n","authors":["Katie Matton","Robert Osazuwa Ness","John Guttag","Emre Kıcıman"],"pdf_url":"https://arxiv.org/pdf/2504.14150v2.pdf","comment":"66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready"},{"id":"http://arxiv.org/abs/2502.13160v3","updated":"2025-05-20T14:34:54Z","published":"2025-02-16T03:02:48Z","title":"Attention Mechanism for LLM-based Agents Dynamic Diffusion under\n  Information Asymmetry","summary":"  Large language models have been used to simulate human society using\nmulti-agent systems. Most current social simulation research emphasizes\ninteractive behaviors in fixed environments, ignoring information opacity,\nrelationship variability, and diffusion diversity. In this paper, we first\npropose a general framework for exploring multi-agent information diffusion. We\nidentified LLMs' deficiency in the perception and utilization of social\nrelationships, as well as diverse actions. Then, we designed a dynamic\nattention mechanism to help agents allocate attention to different information,\naddressing the limitations of the LLM attention mechanism. Agents start by\nresponding to external information stimuli within a five-agent group,\nincreasing group size and forming information circles while developing\nrelationships and sharing information. Additionally, we explore the information\ndiffusion features in the asymmetric open environment by observing the\nevolution of information gaps, diffusion patterns, and the accumulation of\nsocial capital, which are closely linked to psychological, sociological, and\ncommunication theories.\n","authors":["Yiwen Zhang","Yifu Wu","Wenyue Hua","Xiang Lu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.13160v3.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.14419v1","updated":"2025-05-20T14:31:15Z","published":"2025-05-20T14:31:15Z","title":"SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated\n  Process Annotation","summary":"  Process Reward Models (PRMs) have demonstrated promising results in\nmathematical reasoning, but existing process annotation approaches, whether\nthrough human annotations or Monte Carlo simulations, remain computationally\nexpensive. In this paper, we introduce Step COmpression for Process Estimation\n(SCOPE), a novel compression-based approach that significantly reduces\nannotation costs. We first translate natural language reasoning steps into code\nand normalize them through Abstract Syntax Tree, then merge equivalent steps to\nconstruct a prefix tree. Unlike simulation-based methods that waste numerous\nsamples on estimation, SCOPE leverages a compression-based prefix tree where\neach root-to-leaf path serves as a training sample, reducing the complexity\nfrom $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K\nsamples with only 5% of the computational resources required by previous\nmethods. Empirical results demonstrate that PRMs trained on our dataset\nconsistently outperform existing automated annotation approaches on both\nBest-of-N strategy and ProcessBench.\n","authors":["Huimin Xu","Xin Mao","Feng-Lin Li","Xiaobao Wu","Wang Chen","Wei Zhang","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2505.14419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14412v1","updated":"2025-05-20T14:26:19Z","published":"2025-05-20T14:26:19Z","title":"PRL: Prompts from Reinforcement Learning","summary":"  Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .\n","authors":["Paweł Batorski","Adrian Kosmala","Paul Swoboda"],"pdf_url":"https://arxiv.org/pdf/2505.14412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13794v2","updated":"2025-05-20T14:22:45Z","published":"2025-03-18T00:50:40Z","title":"LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation","summary":"  Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.\n","authors":["Yang Zhou","Shiyu Zhao","Yuxiao Chen","Zhenting Wang","Can Jin","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2503.13794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12938v2","updated":"2025-05-20T14:22:15Z","published":"2025-05-19T10:22:04Z","title":"Leveraging LLM Inconsistency to Boost Pass@k Performance","summary":"  Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.\n","authors":["Uri Dalal","Meirav Segal","Zvika Ben-Haim","Dan Lahav","Omer Nevo"],"pdf_url":"https://arxiv.org/pdf/2505.12938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03941v2","updated":"2025-05-20T14:21:41Z","published":"2025-05-06T19:38:07Z","title":"GRAML: Goal Recognition As Metric Learning","summary":"  Goal Recognition (GR) is the problem of recognizing an agent's objectives\nbased on observed actions. Recent data-driven approaches for GR alleviate the\nneed for costly, manually crafted domain models. However, these approaches can\nonly reason about a pre-defined set of goals, and time-consuming training is\nneeded for new emerging goals. To keep this model-learning automated while\nenabling quick adaptation to new goals, this paper introduces GRAML: Goal\nRecognition As Metric Learning. GRAML uses a Siamese network to treat GR as a\ndeep metric learning task, employing an RNN that learns a metric over an\nembedding space, where the embeddings for observation traces leading to\ndifferent goals are distant, and embeddings of traces leading to the same goals\nare close. This metric is especially useful when adapting to new goals, even if\ngiven just one example observation trace per goal. Evaluated on a versatile set\nof environments, GRAML shows speed, flexibility, and runtime improvements over\nthe state-of-the-art GR while maintaining accurate recognition.\n","authors":["Matan Shamir","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2505.03941v2.pdf","comment":"Accepted for publication in International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2505.14403v1","updated":"2025-05-20T14:16:49Z","published":"2025-05-20T14:16:49Z","title":"Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning","summary":"  Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.\n","authors":["Zhaohui Yang","Shilei Jiang","Chen Hu","Linjing Li","Shihong Deng","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.14403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14398v1","updated":"2025-05-20T14:14:38Z","published":"2025-05-20T14:14:38Z","title":"Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation","summary":"  While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.\n","authors":["Peter Baile Chen","Yi Zhang","Dan Roth","Samuel Madden","Jacob Andreas","Michael Cafarella"],"pdf_url":"https://arxiv.org/pdf/2505.14398v1.pdf","comment":"Data and code are available at https://peterbaile.github.io/lag/"},{"id":"http://arxiv.org/abs/2505.14396v1","updated":"2025-05-20T14:14:05Z","published":"2025-05-20T14:14:05Z","title":"Causal Cartographer: From Mapping to Reasoning Over Counterfactual\n  Worlds","summary":"  Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.\n","authors":["Gaël Gendron","Jože M. Rožanec","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2505.14396v1.pdf","comment":"29 pages, 9 pages for the main paper, 20 pages for the references and\n  appendix, 25 figures"},{"id":"http://arxiv.org/abs/2505.14395v1","updated":"2025-05-20T14:14:00Z","published":"2025-05-20T14:14:00Z","title":"MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language","summary":"  Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks ($r$ > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.\n","authors":["Seyoung Song","Seogyeong Jeong","Eunsu Kim","Jiho Jin","Dongkwan Kim","Jay Shin","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2505.14395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14394v1","updated":"2025-05-20T14:13:59Z","published":"2025-05-20T14:13:59Z","title":"Knowledge Graph Based Repository-Level Code Generation","summary":"  Recent advancements in Large Language Models (LLMs) have transformed code\ngeneration from natural language queries. However, despite their extensive\nknowledge and ability to produce high-quality code, LLMs often struggle with\ncontextual accuracy, particularly in evolving codebases. Current code search\nand retrieval methods frequently lack robustness in both the quality and\ncontextual relevance of retrieved results, leading to suboptimal code\ngeneration. This paper introduces a novel knowledge graph-based approach to\nimprove code search and retrieval leading to better quality of code generation\nin the context of repository-level tasks. The proposed approach represents code\nrepositories as graphs, capturing structural and relational information for\nenhanced context-aware code generation. Our framework employs a hybrid approach\nfor code retrieval to improve contextual relevance, track inter-file modular\ndependencies, generate more robust code and ensure consistency with the\nexisting codebase. We benchmark the proposed approach on the Evolutionary Code\nBenchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,\nand demonstrate that our method significantly outperforms the baseline\napproach. These findings suggest that knowledge graph based code generation\ncould advance robust, context-sensitive coding assistance tools.\n","authors":["Mihir Athale","Vishal Vaddina"],"pdf_url":"https://arxiv.org/pdf/2505.14394v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.14391v1","updated":"2025-05-20T14:12:05Z","published":"2025-05-20T14:12:05Z","title":"Beyond the First Error: Process Reward Models for Reflective\n  Mathematical Reasoning","summary":"  Many studies focus on data annotation techniques for training effective PRMs.\nHowever, current methods encounter a significant issue when applied to long CoT\nreasoning processes: they tend to focus solely on the first incorrect step and\nall preceding steps, assuming that all subsequent steps are incorrect. These\nmethods overlook the unique self-correction and reflection mechanisms inherent\nin long CoT, where correct reasoning steps may still occur after initial\nreasoning mistakes. To address this issue, we propose a novel data annotation\nmethod for PRMs specifically designed to score the long CoT reasoning process.\nGiven that under the reflection pattern, correct and incorrect steps often\nalternate, we introduce the concepts of Error Propagation and Error Cessation,\nenhancing PRMs' ability to identify both effective self-correction behaviors\nand reasoning based on erroneous steps. Leveraging an LLM-based judger for\nannotation, we collect 1.7 million data samples to train a 7B PRM and evaluate\nit at both solution and step levels. Experimental results demonstrate that\ncompared to existing open-source PRMs and PRMs trained on open-source datasets,\nour PRM achieves superior performance across various metrics, including search\nguidance, BoN, and F1 scores. Compared to widely used MC-based annotation\nmethods, our annotation approach not only achieves higher data efficiency but\nalso delivers superior performance. Detailed analysis is also conducted to\ndemonstrate the stability and generalizability of our method.\n","authors":["Zhaohui Yang","Chenghua He","Xiaowen Shi","Linjing Li","Qiyue Yin","Shihong Deng","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.14391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23326v2","updated":"2025-05-20T14:09:58Z","published":"2025-03-30T05:48:53Z","title":"Exploring Explainable Multi-player MCTS-minimax Hybrids in Board Game\n  Using Process Mining","summary":"  Monte-Carlo Tree Search (MCTS) is a family of sampling-based search\nalgorithms widely used for online planning in sequential decision-making\ndomains and at the heart of many recent advances in artificial intelligence.\nUnderstanding the behavior of MCTS agents is difficult for developers and users\ndue to the frequently large and complex search trees that result from the\nsimulation of many possible futures, their evaluations, and their\nrelationships. This paper presents our ongoing investigation into potential\nexplanations for the decision-making and behavior of MCTS. A weakness of MCTS\nis that it constructs a highly selective tree and, as a result, can miss\ncrucial moves and fall into tactical traps. Full-width minimax search\nconstitutes the solution. We integrate shallow minimax search into the rollout\nphase of multi-player MCTS and use process mining technique to explain agents'\nstrategies in 3v3 checkers.\n","authors":["Yiyu Qian","Tim Miller","Zheng Qian","Liyuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.23326v2.pdf","comment":"38 pages, AAAI 2025 PRL"},{"id":"http://arxiv.org/abs/2505.14381v1","updated":"2025-05-20T14:03:24Z","published":"2025-05-20T14:03:24Z","title":"SCAN: Semantic Document Layout Analysis for Textual and Visual\n  Retrieval-Augmented Generation","summary":"  With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.\n","authors":["Yuyang Dong","Nobuhiro Ueda","Krisztián Boros","Daiki Ito","Takuya Sera","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2505.14381v1.pdf","comment":"v1"},{"id":"http://arxiv.org/abs/2505.14377v1","updated":"2025-05-20T14:00:28Z","published":"2025-05-20T14:00:28Z","title":"When Bias Backfires: The Modulatory Role of Counterfactual Explanations\n  on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making","summary":"  Although the integration of artificial intelligence (AI) into everyday tasks\nimproves efficiency and objectivity, it also risks transmitting bias to human\ndecision-making. In this study, we conducted a controlled experiment that\nsimulated hiring decisions to examine how biased AI recommendations - augmented\nwith or without counterfactual explanations - influence human judgment over\ntime. Participants, acting as hiring managers, completed 60 decision trials\ndivided into a baseline phase without AI, followed by a phase with biased (X)AI\nrecommendations (favoring either male or female candidates), and a final\npost-interaction phase without AI. Our results indicate that the participants\nfollowed the AI recommendations 70% of the time when the qualifications of the\ngiven candidates were comparable. Yet, only a fraction of participants detected\nthe gender bias (8 out of 294). Crucially, exposure to biased AI altered\nparticipants' inherent preferences: in the post-interaction phase,\nparticipants' independent decisions aligned with the bias when no\ncounterfactual explanations were provided before, but reversed the bias when\nexplanations were given. Reported trust did not differ significantly across\nconditions. Confidence varied throughout the study phases after exposure to\nmale-biased AI, indicating nuanced effects of AI bias on decision certainty.\nOur findings point to the importance of calibrating XAI to avoid unintended\nbehavioral shifts in order to safeguard equitable decision-making and prevent\nthe adoption of algorithmic bias.\n","authors":["Ulrike Kuhl","Annika Bush"],"pdf_url":"https://arxiv.org/pdf/2505.14377v1.pdf","comment":"Accepted for XAI2025"},{"id":"http://arxiv.org/abs/2505.14366v1","updated":"2025-05-20T13:49:09Z","published":"2025-05-20T13:49:09Z","title":"Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds","summary":"  We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.\n","authors":["Joel Currie","Gioele Migno","Enrico Piacenti","Maria Elena Giannaccini","Patric Bach","Davide De Tommaso","Agnieszka Wykowska"],"pdf_url":"https://arxiv.org/pdf/2505.14366v1.pdf","comment":"Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late\n  Breaking Report"},{"id":"http://arxiv.org/abs/2505.14351v1","updated":"2025-05-20T13:35:55Z","published":"2025-05-20T13:35:55Z","title":"FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis\n  for Ü-Tsang, Amdo and Kham Speech Dataset Generation","summary":"  Tibetan is a low-resource language with minimal parallel speech corpora\nspanning its three major dialects-\\\"U-Tsang, Amdo, and Kham-limiting progress\nin speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,\nmulti-speaker, multi-dialect text-to-speech framework that synthesizes parallel\ndialectal speech from limited reference audio and explicit dialect labels. Our\nmethod features a novel speaker-dialect fusion module and a Dialect-Specialized\nDynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and\nlinguistic variations across dialects while preserving speaker identity.\nExtensive objective and subjective evaluations demonstrate that FMSD-TTS\nsignificantly outperforms baselines in both dialectal expressiveness and\nspeaker similarity. We further validate the quality and utility of the\nsynthesized speech through a challenging speech-to-speech dialect conversion\ntask. Our contributions include: (1) a novel few-shot TTS system tailored for\nTibetan multi-dialect speech synthesis, (2) the public release of a large-scale\nsynthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source\nevaluation toolkit for standardized assessment of speaker similarity, dialect\nconsistency, and audio quality.\n","authors":["Yutong Liu","Ziyue Zhang","Ban Ma-bao","Yuqing Cai","Yongbin Yu","Renzeng Duojie","Xiangxiang Wang","Fan Gao","Cheng Huang","Nyima Tashi"],"pdf_url":"https://arxiv.org/pdf/2505.14351v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2505.14349v1","updated":"2025-05-20T13:31:43Z","published":"2025-05-20T13:31:43Z","title":"Upgrading Democracies with Fairer Voting Methods","summary":"  Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.\n","authors":["Evangelos Pournaras","Srijoni Majumdar","Thomas Wellings","Joshua C. Yang","Fatemeh B. Heravan","Regula Hänggli Fricker","Dirk Helbing"],"pdf_url":"https://arxiv.org/pdf/2505.14349v1.pdf","comment":"Includes Supplementary Information"},{"id":"http://arxiv.org/abs/2505.14345v1","updated":"2025-05-20T13:29:04Z","published":"2025-05-20T13:29:04Z","title":"Enhancing Classification with Semi-Supervised Deep Learning Using\n  Distance-Based Sample Weights","summary":"  Recent advancements in semi-supervised deep learning have introduced\neffective strategies for leveraging both labeled and unlabeled data to improve\nclassification performance. This work proposes a semi-supervised framework that\nutilizes a distance-based weighting mechanism to prioritize critical training\nsamples based on their proximity to test data. By focusing on the most\ninformative examples, the method enhances model generalization and robustness,\nparticularly in challenging scenarios with noisy or imbalanced datasets.\nBuilding on techniques such as uncertainty consistency and graph-based\nrepresentations, the approach addresses key challenges of limited labeled data\nwhile maintaining scalability. Experiments on twelve benchmark datasets\ndemonstrate significant improvements across key metrics, including accuracy,\nprecision, and recall, consistently outperforming existing methods. This\nframework provides a robust and practical solution for semi-supervised\nlearning, with potential applications in domains such as healthcare and\nsecurity where data limitations pose significant challenges.\n","authors":["Aydin Abedinia","Shima Tabakhi","Vahid Seydi"],"pdf_url":"https://arxiv.org/pdf/2505.14345v1.pdf","comment":"5 pages, 6 figures. This paper has been accepted for publication and\n  oral presentation at the 2025 10th IEEE International Conference on Machine\n  Learning Technologies (ICMLT 2025). The final authenticated version will be\n  available in IEEE Xplore following the conference"},{"id":"http://arxiv.org/abs/2505.14341v1","updated":"2025-05-20T13:27:52Z","published":"2025-05-20T13:27:52Z","title":"Replace in Translation: Boost Concept Alignment in Counterfactual\n  Text-to-Image","summary":"  Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.\n","authors":["Sifan Li","Ming Tao","Hao Zhao","Ling Shao","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2505.14341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14425v2","updated":"2025-05-20T13:26:45Z","published":"2024-10-18T12:39:32Z","title":"Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation","summary":"  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct comprehensive experiments on three state-of-the-art\nlarge language models and several different backdoor attack algorithms. Our\nempirical results demonstrate the outstanding performance of W2SDefense in\ndefending against backdoor attacks without compromising model performance.\n","authors":["Shuai Zhao","Xiaobao Wu","Cong-Duy Nguyen","Yanhao Jia","Meihuizi Jia","Yichao Feng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2410.14425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14330v1","updated":"2025-05-20T13:16:55Z","published":"2025-05-20T13:16:55Z","title":"Handloom Design Generation Using Generative Networks","summary":"  This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.\n","authors":["Rajat Kanti Bhattacharjee","Meghali Nandi","Amrit Jha","Gunajit Kalita","Ferdous Ahmed Barbhuiya"],"pdf_url":"https://arxiv.org/pdf/2505.14330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00379v4","updated":"2025-05-20T13:10:54Z","published":"2025-02-01T09:35:51Z","title":"Latent Action Learning Requires Supervision in the Presence of\n  Distractors","summary":"  Recently, latent action learning, pioneered by Latent Action Policies (LAPO),\nhave shown remarkable pre-training efficiency on observation-only data,\noffering potential for leveraging vast amounts of video available on the web\nfor embodied AI. However, prior work has focused on distractor-free data, where\nchanges between observations are primarily explained by ground-truth actions.\nUnfortunately, real-world videos contain action-correlated distractors that may\nhinder latent action learning. Using Distracting Control Suite (DCS) we\nempirically investigate the effect of distractors on latent action learning and\ndemonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO\nmodification that improves the quality of latent actions by 8x, as measured by\nlinear probing. Importantly, we show that providing supervision with\nground-truth actions, as few as 2.5% of the full dataset, during latent action\nlearning improves downstream performance by 4.2x on average. Our findings\nsuggest that integrating supervision during Latent Action Models (LAM) training\nis critical in the presence of distractors, challenging the conventional\npipeline of first learning LAM and only then decoding from latent to\nground-truth actions.\n","authors":["Alexander Nikulin","Ilya Zisman","Denis Tarasov","Nikita Lyubaykin","Andrei Polubarov","Igor Kiselev","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2502.00379v4.pdf","comment":"ICML 2025, Poster, Source code: https://github.com/dunnolab/laom"},{"id":"http://arxiv.org/abs/2505.11568v2","updated":"2025-05-20T13:09:44Z","published":"2025-05-16T09:46:08Z","title":"BioCube: A Multimodal Dataset for Biodiversity Research","summary":"  Biodiversity research requires complete and detailed information to study\necosystem dynamics at different scales. Employing data-driven methods like\nMachine Learning is getting traction in ecology and more specific biodiversity,\noffering alternative modelling pathways. For these methods to deliver accurate\nresults there is the need for large, curated and multimodal datasets that offer\ngranular spatial and temporal resolutions. In this work, we introduce BioCube,\na multimodal, fine-grained global dataset for ecology and biodiversity\nresearch. BioCube incorporates species observations through images, audio\nrecordings and descriptions, environmental DNA, vegetation indices,\nagricultural, forest, land indicators, and high-resolution climate variables.\nAll observations are geospatially aligned under the WGS84 geodetic system,\nspanning from 2000 to 2020. The dataset will become available at\nhttps://huggingface.co/datasets/BioDT/BioCube while the acquisition and\nprocessing code base at https://github.com/BioDT/bfm-data.\n","authors":["Stylianos Stasinos","Martino Mensio","Elena Lazovik","Athanasios Trantas"],"pdf_url":"https://arxiv.org/pdf/2505.11568v2.pdf","comment":"submitted to BiDS'25, 5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.13652v3","updated":"2025-05-20T13:06:00Z","published":"2024-09-20T17:02:00Z","title":"OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition","summary":"  The recent paradigm shift to large-scale foundation models has brought about\na new era for deep learning that, while has found great success in practice,\nhas also been plagued by prohibitively expensive costs in terms of high memory\nconsumption and compute. To mitigate these issues, there has been a concerted\neffort in post-hoc neural network pruning techniques that do not require costly\nretraining. Despite the considerable progress being made, existing methods\noften exhibit a steady drop in model performance as the compression increases.\nIn this paper, we present a novel approach to compressing large transformers,\ncoined OATS, that utilizes the second moment information in the input\nembeddings to decompose the model weights into a sum of sparse and low-rank\nmatrices. Without any retraining, OATS achieves state-of-the-art performance\nwhen compressing models by up to $60\\%$ on large language models such as\nLlama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while\ndelivering up to $1.37\\times$ the CPU acceleration versus a model that was\ncomparably pruned.\n","authors":["Stephen Zhang","Vardan Papyan"],"pdf_url":"https://arxiv.org/pdf/2409.13652v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14316v1","updated":"2025-05-20T13:03:15Z","published":"2025-05-20T13:03:15Z","title":"Exploring Jailbreak Attacks on LLMs through Intent Concealment and\n  Diversion","summary":"  Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs.\n","authors":["Tiehan Cui","Yanxu Mao","Peipei Liu","Congying Liu","Datao You"],"pdf_url":"https://arxiv.org/pdf/2505.14316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14312v1","updated":"2025-05-20T13:00:43Z","published":"2025-05-20T13:00:43Z","title":"MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional\n  Evaluation in Tabular Domains","summary":"  Despite the widespread use of tabular data in real-world applications, most\nbenchmarks rely on average-case metrics, which fail to reveal how model\nbehavior varies across diverse data regimes. To address this, we propose\nMultiTab, a benchmark suite and evaluation framework for multi-dimensional,\ndata-aware analysis of tabular learning algorithms. Rather than comparing\nmodels only in aggregate, MultiTab categorizes 196 publicly available datasets\nalong key data characteristics, including sample size, label imbalance, and\nfeature interaction, and evaluates 13 representative models spanning a range of\ninductive biases. Our analysis shows that model performance is highly sensitive\nto such regimes: for example, models using sample-level similarity excel on\ndatasets with large sample sizes or high inter-feature correlation, while\nmodels encoding inter-feature dependencies perform best with weakly correlated\nfeatures. These findings reveal that inductive biases do not always behave as\nintended, and that regime-aware evaluation is essential for understanding and\nimproving model behavior. MultiTab enables more principled model design and\noffers practical guidance for selecting models tailored to specific data\ncharacteristics. All datasets, code, and optimization logs are publicly\navailable at https://huggingface.co/datasets/LGAI-DILab/Multitab.\n","authors":["Kyungeun Lee","Moonjung Eo","Hye-Seung Cho","Dongmin Kim","Ye Seul Sim","Seoyoon Kim","Min-Kook Suh","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2505.14312v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.12711v2","updated":"2025-05-20T12:57:58Z","published":"2025-05-19T05:07:34Z","title":"Any-to-Any Learning in Computational Pathology via Triplet Multimodal\n  Pretraining","summary":"  Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.\n","authors":["Qichen Sun","Zhengrui Guo","Rui Peng","Hao Chen","Jinzhuo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14300v1","updated":"2025-05-20T12:49:58Z","published":"2025-05-20T12:49:58Z","title":"SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring\n  Deceptive Behaviors","summary":"  High-risk industries like nuclear and aviation use real-time monitoring to\ndetect dangerous system conditions. Similarly, Large Language Models (LLMs)\nneed monitoring safeguards. We propose a real-time framework to predict harmful\nAI outputs before they occur by using an unsupervised approach that treats\nnormal behavior as the baseline and harmful outputs as outliers. Our study\nfocuses specifically on backdoor-triggered responses -- where specific input\nphrases activate hidden vulnerabilities causing the model to generate unsafe\ncontent like violence, pornography, or hate speech. We address two key\nchallenges: (1) identifying true causal indicators rather than surface\ncorrelations, and (2) preventing advanced models from deception -- deliberately\nevading monitoring systems. Hence, we approach this problem from an\nunsupervised lens by drawing parallels to human deception: just as humans\nexhibit physical indicators while lying, we investigate whether LLMs display\ndistinct internal behavioral signatures when generating harmful content. Our\nstudy addresses two critical challenges: 1) designing monitoring systems that\ncapture true causal indicators rather than superficial correlations; and\n2)preventing intentional evasion by increasingly capable \"Future models''. Our\nfindings show that models can produce harmful content through causal mechanisms\nand can become deceptive by: (a) alternating between linear and non-linear\nrepresentations, and (b) modifying feature relationships. To counter this, we\ndeveloped Safety-Net -- a multi-detector framework that monitors different\nrepresentation dimensions, successfully detecting harmful behavior even when\ninformation is shifted across representational spaces to evade individual\nmonitors. Our evaluation shows 96% accuracy in detecting harmful cases using\nour unsupervised ensemble approach.\n","authors":["Maheep Chaudhary","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2505.14300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17980v2","updated":"2025-05-20T12:48:26Z","published":"2024-10-23T15:51:13Z","title":"Scaling Stick-Breaking Attention: An Efficient Implementation and\n  In-depth Study","summary":"  The self-attention mechanism traditionally relies on the softmax operator,\nnecessitating positional embeddings like RoPE, or position biases to account\nfor token order. But current methods using still face length generalisation\nchallenges. We investigate an alternative attention mechanism based on the\nstick-breaking process in larger scale settings. The method works as follows:\nFor each token before the current, we determine a break point, which represents\nthe proportion of the stick, the weight of the attention, to allocate to the\ncurrent token. We repeat this on the remaining stick, until all tokens are\nallocated a weight, resulting in a sequence of attention weights. This process\nnaturally incorporates recency bias, which has linguistic motivations for\ngrammar parsing. We study the implications of replacing the conventional\nsoftmax-based attention mechanism with stick-breaking attention. We then\ndiscuss implementation of numerically stable stick-breaking attention and adapt\nFlash Attention to accommodate this mechanism. When used as a drop-in\nreplacement for current softmax+RoPE attention systems, we find that\nstick-breaking attention performs competitively with current methods on length\ngeneralisation and downstream tasks. Stick-breaking also performs well at\nlength generalisation, allowing a model trained with $2^{11}$ context window to\nperform well at $2^{14}$ with perplexity improvements.\n","authors":["Shawn Tan","Songlin Yang","Aaron Courville","Rameswar Panda","Yikang Shen"],"pdf_url":"https://arxiv.org/pdf/2410.17980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14232v2","updated":"2025-05-20T12:46:42Z","published":"2025-03-18T13:09:01Z","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","summary":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure. However, existing methods struggle with\nunder-erasure, leaving residual traces of targeted concepts, or over-erasure,\nmistakenly eliminating unrelated but visually similar concepts. To address\nthese limitations, we introduce CRCE, a novel concept erasure framework that\nleverages Large Language Models to identify both semantically related concepts\nthat should be erased alongside the target and distinct concepts that should be\npreserved. By explicitly modelling coreferential and retained concepts\nsemantically, CRCE enables more precise concept removal, without unintended\nerasure. Experiments demonstrate that CRCE outperforms existing methods on\ndiverse erasure tasks, including real-world object, person identities, and\nabstract intellectual property characteristics. The constructed dataset\nCorefConcept and the source code will be release upon acceptance.\n","authors":["Yuyang Xue","Edward Moroshko","Feng Chen","Jingyu Sun","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2503.14232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14295v1","updated":"2025-05-20T12:44:14Z","published":"2025-05-20T12:44:14Z","title":"Benchmarking data encoding methods in Quantum Machine Learning","summary":"  Data encoding plays a fundamental and distinctive role in Quantum Machine\nLearning (QML). While classical approaches process data directly as vectors,\nQML may require transforming classical data into quantum states through\nencoding circuits, known as quantum feature maps or quantum embeddings. This\nstep leverages the inherently high-dimensional and non-linear nature of Hilbert\nspace, enabling more efficient data separation in complex feature spaces that\nmay be inaccessible to classical methods. This encoding part significantly\naffects the performance of the QML model, so it is important to choose the\nright encoding method for the dataset to be encoded. However, this choice is\ngenerally arbitrary, since there is no \"universal\" rule for knowing which\nencoding to choose based on a specific set of data. There are currently a\nvariety of encoding methods using different quantum logic gates. We studied the\nmost commonly used types of encoding methods and benchmarked them using\ndifferent datasets.\n","authors":["Orlane Zang","Grégoire Barrué","Tony Quertier"],"pdf_url":"https://arxiv.org/pdf/2505.14295v1.pdf","comment":"30 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.06330v2","updated":"2025-05-20T12:43:04Z","published":"2025-05-09T15:35:11Z","title":"Prompting Large Language Models for Training-Free Non-Intrusive Load\n  Monitoring","summary":"  Non-intrusive load monitoring (NILM) aims to disaggregate aggregate household\nelectricity consumption into individual appliance usage and thus enables more\neffective energy management. While deep learning has advanced NILM, it remains\nlimited by its dependence on labeled data, restricted generalization, and lack\nof explainability. This paper introduces the first prompt-based NILM framework\nthat leverages large language models (LLMs) with in-context learning. We design\nand evaluate prompt strategies that integrate appliance features, timestamps\nand contextual information, as well as representative time-series examples on\nwidely used open datasets. With optimized prompts, LLMs achieve competitive\nstate detection accuracy and demonstrate robust generalization without the need\nfor fine-tuning. LLMs also enhance explainability by providing clear,\nhuman-readable explanations for their predictions. Our results show that LLMs\ncan reduce data requirements, improve adaptability, and provide transparent\nenergy disaggregation in NILM applications.\n","authors":["Junyu Xue","Xudong Wang","Xiaoling He","Shicheng Liu","Yi Wang","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2505.06330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10271v3","updated":"2025-05-20T12:41:51Z","published":"2024-05-16T17:27:41Z","title":"Federated Hybrid Model Pruning through Loss Landscape Exploration","summary":"  As the era of connectivity and unprecedented data generation expands,\ncollaborative intelligence emerges as a key driver for machine learning,\nencouraging global-scale model development. Federated learning (FL) stands at\nthe heart of this transformation, enabling distributed systems to work\ncollectively on complex tasks while respecting strict constraints on privacy\nand security. Despite its vast potential, specially in the age of complex\nmodels, FL encounters challenges such as elevated communication costs,\ncomputational constraints, and the heterogeneous data distributions. In this\ncontext, we present AutoFLIP, a novel framework that optimizes FL through an\nadaptive hybrid pruning approach, grounded in a federated loss exploration\nphase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP\nefficiently identifies model substructures for pruning both at structured and\nunstructured levels. This targeted optimization fosters a symbiotic\nintelligence loop, reducing computational burdens and boosting model\nperformance on resource-limited devices for a more inclusive and democratized\nmodel usage. Our extensive experiments across multiple datasets and FL tasks\nshow that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in\ncomputational overhead, a 35.5% decrease in communication costs, and a notable\nimprovement in global accuracy. By significantly reducing these overheads,\nAutoFLIP offer the way for efficient FL deployment in real-world applications\nfor a scalable and broad applicability.\n","authors":["Christian Internò","Elena Raponi","Niki van Stein","Thomas Bäck","Markus Olhofer","Yaochu Jin","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2405.10271v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14289v1","updated":"2025-05-20T12:41:05Z","published":"2025-05-20T12:41:05Z","title":"EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection","summary":"  As multimodal agents are increasingly trained to operate graphical user\ninterfaces (GUIs) to complete user tasks, they face a growing threat from\nindirect prompt injection, attacks in which misleading instructions are\nembedded into the agent's visual environment, such as popups or chat messages,\nand misinterpreted as part of the intended task. A typical example is\nenvironmental injection, in which GUI elements are manipulated to influence\nagent behavior without directly modifying the user prompt. To address these\nemerging attacks, we propose EVA, a red teaming framework for indirect prompt\ninjection which transforms the attack into a closed loop optimization by\ncontinuously monitoring an agent's attention distribution over the GUI and\nupdating adversarial cues, keywords, phrasing, and layout, in response.\nCompared with prior one shot methods that generate fixed prompts without regard\nfor how the model allocates visual attention, EVA dynamically adapts to\nemerging attention hotspots, yielding substantially higher attack success rates\nand far greater transferability across diverse GUI scenarios. We evaluate EVA\non six widely used generalist and specialist GUI agents in realistic settings\nsuch as popup manipulation, chat based phishing, payments, and email\ncomposition. Experimental results show that EVA substantially improves success\nrates over static baselines. Under goal agnostic constraints, where the\nattacker does not know the agent's task intent, EVA still discovers effective\npatterns. Notably, we find that injection styles transfer well across models,\nrevealing shared behavioral biases in GUI agents. These results suggest that\nevolving indirect prompt injection is a powerful tool not only for red teaming\nagents, but also for uncovering common vulnerabilities in their multimodal\ndecision making.\n","authors":["Yijie Lu","Tianjie Ju","Manman Zhao","Xinbei Ma","Yuan Guo","ZhuoSheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08399v2","updated":"2025-05-20T12:38:58Z","published":"2025-04-11T10:03:55Z","title":"Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models","summary":"  Self-report questionnaires have long been used to assess LLM personality\ntraits, yet they fail to capture behavioral nuances due to biases and\nmeta-knowledge contamination. This paper proposes a novel multi-observer\nframework for personality trait assessments in LLM agents that draws on\ninformant-report methods in psychology. Instead of relying on self-assessments,\nwe employ multiple observer agents. Each observer is configured with a specific\nrelational context (e.g., family member, friend, or coworker) and engages the\nsubject LLM in dialogue before evaluating its behavior across the Big Five\ndimensions. We show that these observer-report ratings align more closely with\nhuman judgments than traditional self-reports and reveal systematic biases in\nLLM self-assessments. We also found that aggregating responses from 5 to 7\nobservers reduces systematic biases and achieves optimal reliability. Our\nresults highlight the role of relationship context in perceiving personality\nand demonstrate that a multi-observer paradigm offers a more reliable,\ncontext-sensitive approach to evaluating LLM personality traits.\n","authors":["Yin Jou Huang","Rafik Hadfi"],"pdf_url":"https://arxiv.org/pdf/2504.08399v2.pdf","comment":"16 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.05863v2","updated":"2025-05-20T12:37:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14285v1","updated":"2025-05-20T12:35:43Z","published":"2025-05-20T12:35:43Z","title":"AquaSignal: An Integrated Framework for Robust Underwater Acoustic\n  Analysis","summary":"  This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains.\n","authors":["Eirini Panteli","Paulo E. Santos","Nabil Humphrey"],"pdf_url":"https://arxiv.org/pdf/2505.14285v1.pdf","comment":"8 pages; 9 figures"},{"id":"http://arxiv.org/abs/2505.14279v1","updated":"2025-05-20T12:30:46Z","published":"2025-05-20T12:30:46Z","title":"YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering","summary":"  Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.\n","authors":["Jennifer D'Souza","Hamed Babaei Giglou","Quentin Münch"],"pdf_url":"https://arxiv.org/pdf/2505.14279v1.pdf","comment":"8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)"},{"id":"http://arxiv.org/abs/2505.07447v2","updated":"2025-05-20T12:27:53Z","published":"2025-05-12T11:15:39Z","title":"Unified Continuous Generative Models","summary":"  Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.\n","authors":["Peng Sun","Yi Jiang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2505.07447v2.pdf","comment":"https://github.com/LINs-lab/UCGM"},{"id":"http://arxiv.org/abs/2505.14273v1","updated":"2025-05-20T12:26:03Z","published":"2025-05-20T12:26:03Z","title":"X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary\n  Rule-Based Machine Learning","summary":"  Function approximation is a critical task in various fields. However,\nexisting neural network approaches struggle with locally complex or\ndiscontinuous functions due to their reliance on a single global model covering\nthe entire problem space. We propose X-KAN, a novel method that optimizes\nmultiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary\nrule-based machine learning framework called XCSF. X-KAN combines KAN's high\nexpressiveness with XCSF's adaptive partitioning capability by implementing\nlocal KAN models as rule consequents and defining local regions via rule\nantecedents. Our experimental results on artificial test functions and\nreal-world datasets demonstrate that X-KAN significantly outperforms\nconventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms\nof approximation accuracy. Notably, X-KAN effectively handles functions with\nlocally complex or discontinuous structures that are challenging for\nconventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules).\nThese results validate the effectiveness of using KAN as a local model in XCSF,\nwhich evaluates the rule fitness based on both accuracy and generality. Our\nX-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.\n","authors":["Hiroki Shiraishi","Hisao Ishibuchi","Masaya Nakata"],"pdf_url":"https://arxiv.org/pdf/2505.14273v1.pdf","comment":"Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.14268v1","updated":"2025-05-20T12:19:10Z","published":"2025-05-20T12:19:10Z","title":"Think-J: Learning to Think for Generative LLM-as-a-Judge","summary":"  LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.\n","authors":["Hui Huang","Yancheng He","Hongli Zhou","Rui Zhang","Wei Liu","Weixun Wang","Wenbo Su","Bo Zheng","Jiaheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14268v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.05047v2","updated":"2025-05-20T12:17:15Z","published":"2025-04-07T13:17:52Z","title":"Debate Only When Necessary: Adaptive Multiagent Collaboration for\n  Efficient LLM Reasoning","summary":"  Multiagent collaboration has emerged as a promising framework for enhancing\nthe reasoning capabilities of large language models (LLMs). Despite\nimprovements in reasoning, the approach introduces substantial computational\noverhead resulting from iterative agent interactions. Furthermore, engaging in\nunnecessary debates increases the risk of generating erroneous responses. To\naddress these challenges, we propose Debate Only When Necessary (DOWN), an\nadaptive multiagent debate framework that selectively activates debate based on\nthe confidence score of the agent's initial response. Debate is activated only\nfor queries requiring further deliberation, during which agents refine their\noutputs by referencing peer responses and associated confidence scores.\nEvaluations on benchmarks show that DOWN improves efficiency by up to six times\nwhile preserving or even outperforming the performance of existing methods.\nFurther analysis indicates that DOWN effectively mitigates the risk of error\npropagation stemming from the unnecessary debate process. These findings\ndemonstrate the effectiveness of our approach in delivering high-performance\nLLM solutions at a lower computational cost.\n","authors":["Sugyeong Eo","Hyeonseok Moon","Evelyn Hayoon Zi","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2504.05047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16747v2","updated":"2025-05-20T12:14:20Z","published":"2025-02-23T23:23:51Z","title":"SQLong: Enhanced NL2SQL for Longer Contexts with LLMs","summary":"  Open-weight large language models (LLMs) have significantly advanced\nperformance in the Natural Language to SQL (NL2SQL) task. However, their\neffectiveness diminishes when dealing with large database schemas, as the\ncontext length increases. To address this limitation, we present SQLong, a\nnovel and efficient data augmentation framework designed to enhance LLM\nperformance in long-context scenarios for the NL2SQL task. SQLong generates\naugmented datasets by extending existing database schemas with additional\nsynthetic CREATE TABLE commands and corresponding data rows, sampled from\ndiverse schemas in the training data. This approach effectively simulates\nlong-context scenarios during finetuning and evaluation. Through experiments on\nthe Spider and BIRD datasets, we demonstrate that LLMs finetuned with\nSQLong-augmented data significantly outperform those trained on standard\ndatasets. These imply SQLong's practical implementation and its impact on\nimproving NL2SQL capabilities in real-world settings with complex database\nschemas.\n","authors":["Dai Quoc Nguyen","Cong Duy Vu Hoang","Duy Vu","Gioacchino Tangari","Thanh Tien Vu","Don Dharmasiri","Yuan-Fang Li","Long Duong"],"pdf_url":"https://arxiv.org/pdf/2502.16747v2.pdf","comment":"Accepted to Table Representation Learning Workshop at ACL 2025"},{"id":"http://arxiv.org/abs/2505.14260v1","updated":"2025-05-20T12:12:17Z","published":"2025-05-20T12:12:17Z","title":"Speculative Decoding Reimagined for Multimodal Large Language Models","summary":"  This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD.\n","authors":["Luxi Lin","Zhihang Lin","Zhanpeng Zeng","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2505.14260v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2505.08155v3","updated":"2025-05-20T12:09:36Z","published":"2025-05-13T01:24:09Z","title":"Efficient and Scalable Neural Symbolic Search for Knowledge Graph\n  Complex Query Answering","summary":"  Complex Query Answering (CQA) aims to retrieve answer sets for complex\nlogical formulas from incomplete knowledge graphs, which is a crucial yet\nchallenging task in knowledge graph reasoning. While neuro-symbolic search\nutilized neural link predictions achieve superior accuracy, they encounter\nsignificant complexity bottlenecks: (i) Data complexity typically scales\nquadratically with the number of entities in the knowledge graph, and (ii)\nQuery complexity becomes NP-hard for cyclic queries. Consequently, these\napproaches struggle to effectively scale to larger knowledge graphs and more\ncomplex queries. To address these challenges, we propose an efficient and\nscalable symbolic search framework. First, we propose two constraint strategies\nto compute neural logical indices to reduce the domain of variables, thereby\ndecreasing the data complexity of symbolic search. Additionally, we introduce\nan approximate algorithm based on local search to tackle the NP query\ncomplexity of cyclic queries. Experiments on various CQA benchmarks demonstrate\nthat our framework reduces the computational load of symbolic methods by 90\\%\nwhile maintaining nearly the same performance, thus alleviating both efficiency\nand scalability issues.\n","authors":["Weizhi Fei","Zihao Wang","hang Yin","Shukai Zhao","Wei Zhang","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2505.08155v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14256v1","updated":"2025-05-20T12:09:17Z","published":"2025-05-20T12:09:17Z","title":"FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation","summary":"  In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.\n","authors":["Shaolin Zhu","Tianyu Dong","Bo Li","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.14256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14252v1","updated":"2025-05-20T12:05:17Z","published":"2025-05-20T12:05:17Z","title":"Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence\n  Encoders and Physics-Informed Neural Networks","summary":"  In this work, we explore the integration of Sequence Encoding for Online\nParameter Identification with Physics-Informed Neural Networks to create a\nmodel that, once trained, can be utilized for real time applications with\nvariable parameters, boundary conditions, and initial conditions. Recently, the\ncombination of PINNs with Sparse Regression has emerged as a method for\nperforming dynamical system identification through supervised learning and\nsparse regression optimization, while also solving the dynamics using PINNs.\nHowever, this approach can be limited by variations in parameters or boundary\nand initial conditions, requiring retraining of the model whenever changes\noccur. In this work, we introduce an architecture that employs Deep Sets or\nSequence Encoders to encode dynamic parameters, boundary conditions, and\ninitial conditions, using these encoded features as inputs for the PINN,\nenabling the model to adapt to changes in parameters, BCs, and ICs. We apply\nthis approach to three different problems. First, we analyze the Rossler ODE\nsystem, demonstrating the robustness of the model with respect to noise and its\nability to generalize. Next, we explore the model's capability in a 2D\nNavier-Stokes PDE problem involving flow past a cylinder with a parametric\nsinusoidal inlet velocity function, showing that the model can encode pressure\ndata from a few points to identify the inlet velocity profile and utilize\nphysics to compute velocity and pressure throughout the domain. Finally, we\naddress a 1D heat monitoring problem using real data from the heating of glass\nfiber and thermoplastic composite plates.\n","authors":["Mouad Elaarabi","Domenico Borzacchiello","Philippe Le Bot","Nathan Lauzeral","Sebastien Comas-Cardona"],"pdf_url":"https://arxiv.org/pdf/2505.14252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14246v1","updated":"2025-05-20T11:59:25Z","published":"2025-05-20T11:59:25Z","title":"Visual Agentic Reinforcement Fine-Tuning","summary":"  A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.\n","authors":["Ziyu Liu","Yuhang Zang","Yushan Zou","Zijian Liang","Xiaoyi Dong","Yuhang Cao","Haodong Duan","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14246v1.pdf","comment":"project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT"},{"id":"http://arxiv.org/abs/2504.12324v2","updated":"2025-05-20T11:52:01Z","published":"2025-04-11T13:18:26Z","title":"Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and\n  Interpretability Prediction","summary":"  Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer\nreview.\n","authors":["Mengying Yuan","Wenhao Wang","Zixuan Wang","Yujie Huang","Kangli Wei","Fei Li","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2504.12324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12442v2","updated":"2025-05-20T11:48:36Z","published":"2025-05-18T14:31:45Z","title":"IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems","summary":"  The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.\n","authors":["Liwen Wang","Wenxuan Wang","Shuai Wang","Zongjie Li","Zhenlan Ji","Zongyi Lyu","Daoyuan Wu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2505.12442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11537v3","updated":"2025-05-20T11:45:24Z","published":"2025-02-17T08:06:10Z","title":"Uncovering Untapped Potential in Sample-Efficient World Model Agents","summary":"  World model (WM) agents enable sample-efficient reinforcement learning by\nlearning policies entirely from simulated experience. However, existing\ntoken-based world models (TBWMs) are limited to visual inputs and discrete\nactions, restricting their adoption and applicability. Moreover, although both\nintrinsic motivation and prioritized WM replay have shown promise in improving\nWM performance and generalization, they remain underexplored in this setting,\nparticularly in combination. We introduce Simulus, a highly modular TBWM agent\nthat integrates (1) a modular multi-modality tokenization framework, (2)\nintrinsic motivation, (3) prioritized WM replay, and (4)\nregression-as-classification for reward and return prediction. Simulus achieves\nstate-of-the-art sample efficiency for planning-free WMs across three diverse\nbenchmarks. Ablation studies reveal the individual contribution of each\ncomponent while highlighting their synergy. Our code and model weights are\npublicly available at https://github.com/leor-c/Simulus.\n","authors":["Lior Cohen","Kaixin Wang","Bingyi Kang","Uri Gadot","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.11537v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14238v1","updated":"2025-05-20T11:43:25Z","published":"2025-05-20T11:43:25Z","title":"ABBA: Highly Expressive Hadamard Product Adaptation for Large Language\n  Models","summary":"  Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.\n","authors":["Raghav Singhal","Kaustubh Ponkshe","Rohit Vartak","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2505.14238v1.pdf","comment":"Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2410.21673v2","updated":"2025-05-20T11:43:05Z","published":"2024-10-29T02:48:41Z","title":"Knowledge-Guided Prompt Learning for Request Quality Assurance in Public\n  Code Review","summary":"  Public Code Review (PCR) is developed in the Software Question Answering\n(SQA) community, assisting developers in exploring high-quality and efficient\nreview services. Current methods on PCR mainly focus on the reviewer's\nperspective, including finding a capable reviewer, predicting comment quality,\nand recommending/generating review comments. However, it is not well studied\nthat how to satisfy the review necessity requests posted by developers which\ncan increase their visibility, which in turn acts as a prerequisite for better\nreview responses. To this end, we propose Knowledge-guided Prompt learning for\nPublic Code Review (KP-PCR) to achieve developer-based code review request\nquality assurance (i.e., predicting request necessity and recommending tags\nsubtask). Specifically, we reformulate the two subtasks via 1) text prompt\ntuning which converts both of them into a Masked Language Model (MLM) by\nconstructing prompt templates using hard prompt; and 2) knowledge and code\nprefix tuning which introduces knowledge guidance from fine-tuned large\nlanguage models by soft prompt, and uses program dependence graph to\ncharacterize code snippets. Finally, both of the request necessity prediction\nand tag recommendation subtasks output predicted results through an answer\nengineering module. In addition, we further analysis the time complexity of our\nKP-PCR that has lightweight prefix based the operation of introducing knowledge\nguidance. Experimental results on the PCR dataset for the period 2011-2023\ndemonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request\nnecessity prediction and by 1.4%-6.9% in the tag recommendation. The code\nimplementation is released at https://github.com/WUT-IDEA/KP-PCR.\n","authors":["Lin Li","Xinchun Yu","Xinyu Chen","Peng Liang"],"pdf_url":"https://arxiv.org/pdf/2410.21673v2.pdf","comment":"27 pages, 5 images, 12 tables, Manuscript revision submitted to a\n  journal (2025)"},{"id":"http://arxiv.org/abs/2505.14235v1","updated":"2025-05-20T11:42:26Z","published":"2025-05-20T11:42:26Z","title":"Toward Embodied AGI: A Review of Embodied AI and the Road Ahead","summary":"  Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.\n","authors":["Yequan Wang","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2505.14235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14234v1","updated":"2025-05-20T11:41:26Z","published":"2025-05-20T11:41:26Z","title":"Fast and close Shannon entropy approximation","summary":"  Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy\nare key components in many tools used in physics, information theory, machine\nlearning (ML) and quantum computing. Besides of the significant amounts of SE\ncomputations required in these fields, the singularity of the SE gradient is\none of the central mathematical reason inducing the high cost, frequently low\nrobustness and slow convergence of such tools. Here we propose the Fast Entropy\nApproximation (FEA) - a non-singular rational approximation of Shannon entropy\nand its gradient that achieves a mean absolute error of $10^{-3}$, which is\napproximately $20$ times lower than comparable state-of-the-art methods. FEA\nallows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary\ncomputational operations, as compared to tens of elementary operations behind\nthe fastest entropy computation algorithms with table look-ups, bitshifts, or\nseries approximations. On a set of common benchmarks for the feature selection\nproblem in machine learning, we show that the combined effect of fewer\nelementary operations, low approximation error, and a non-singular gradient\nallows significantly better model quality and enables ML feature extraction\nthat is two to three orders of magnitude faster and computationally cheaper\nwhen incorporating FEA into AI tools.\n","authors":["Illia Horenko","Davide Bassetti","Lukáš Pospíšil"],"pdf_url":"https://arxiv.org/pdf/2505.14234v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2505.14233v1","updated":"2025-05-20T11:41:21Z","published":"2025-05-20T11:41:21Z","title":"Mechanistic Fine-tuning for In-context Learning","summary":"  In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.\n","authors":["Hakaze Cho","Peng Luo","Mariko Kato","Rin Kaenbyou","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2505.14233v1.pdf","comment":"28 pages, 31 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.14227v1","updated":"2025-05-20T11:37:49Z","published":"2025-05-20T11:37:49Z","title":"VoQA: Visual-only Question Answering","summary":"  We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.\n","authors":["Luyang Jiang","Jianing An","Jie Luo","Wenjun Wu","Lei Huang"],"pdf_url":"https://arxiv.org/pdf/2505.14227v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2502.07693v4","updated":"2025-05-20T11:35:47Z","published":"2025-02-11T16:46:56Z","title":"AI-driven Personalized Privacy Assistants: a Systematic Literature\n  Review","summary":"  In recent years, several personalized assistants based on AI have been\nresearched and developed to help users make privacy-related decisions. These\nAI-driven Personalized Privacy Assistants (AI-driven PPAs) can provide\nsignificant benefits for users, who might otherwise struggle with making\ndecisions about their personal data in online environments that often overload\nthem with different privacy decision requests. So far, no studies have\nsystematically investigated the emerging topic of AI-driven PPAs, classifying\ntheir underlying technologies, architecture and features, including decision\ntypes or the accuracy of their decisions. To fill this gap, we present a\nSystematic Literature Review (SLR) to map the existing solutions found in the\nscientific literature, which allows reasoning about existing approaches and\nopen challenges for this research field. We screened several hundred unique\nresearch papers over the recent years (2013-2025), constructing a\nclassification from 41 included papers. As a result, this SLR reviews several\naspects of existing research on AI-driven PPAs in terms of types of\npublications, contributions, methodological quality, and other quantitative\ninsights. Furthermore, we provide a comprehensive classification for AI-driven\nPPAs, delving into their architectural choices, system contexts, types of AI\nused, data sources, types of decisions, and control over decisions, among other\nfacets. Based on our SLR, we further underline the research gaps and challenges\nand formulate recommendations for the design and development of AI-driven PPAs\nas well as avenues for future research.\n","authors":["Victor Morel","Leonardo Iwaya","Simone Fischer-Hübner"],"pdf_url":"https://arxiv.org/pdf/2502.07693v4.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2505.14226v1","updated":"2025-05-20T11:35:25Z","published":"2025-05-20T11:35:25Z","title":"\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed\n  Hinglish to Red-Team LLMs","summary":"  Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.\n","authors":["Darpan Aswal","Siddharth D Jaiswal"],"pdf_url":"https://arxiv.org/pdf/2505.14226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12767v5","updated":"2025-05-20T11:24:19Z","published":"2025-02-18T11:31:52Z","title":"R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs","summary":"  Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.\n","authors":["Sumin Jo","Junseong Choi","Jiho Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2502.12767v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14217v1","updated":"2025-05-20T11:23:52Z","published":"2025-05-20T11:23:52Z","title":"Federated learning in low-resource settings: A chest imaging study in\n  Africa -- Challenges and lessons learned","summary":"  This study explores the use of Federated Learning (FL) for tuberculosis (TB)\ndiagnosis using chest X-rays in low-resource settings across Africa. FL allows\nhospitals to collaboratively train AI models without sharing raw patient data,\naddressing privacy concerns and data scarcity that hinder traditional\ncentralized models. The research involved hospitals and research centers in\neight African countries. Most sites used local datasets, while Ghana and The\nGambia used public ones. The study compared locally trained models with a\nfederated model built across all institutions to evaluate FL's real-world\nfeasibility. Despite its promise, implementing FL in sub-Saharan Africa faces\nchallenges such as poor infrastructure, unreliable internet, limited digital\nliteracy, and weak AI regulations. Some institutions were also reluctant to\nshare model updates due to data control concerns. In conclusion, FL shows\nstrong potential for enabling AI-driven healthcare in underserved regions, but\nbroader adoption will require improvements in infrastructure, education, and\nregulatory support.\n","authors":["Jorge Fabila","Lidia Garrucho","Víctor M. Campello","Carlos Martín-Isla","Karim Lekadir"],"pdf_url":"https://arxiv.org/pdf/2505.14217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14216v1","updated":"2025-05-20T11:22:34Z","published":"2025-05-20T11:22:34Z","title":"Reinforcement Learning vs. Distillation: Understanding Accuracy and\n  Capability in LLM Reasoning","summary":"  Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.\n","authors":["Minwu Kim","Anubhav Shrestha","Safal Shrestha","Aadim Nepal","Keith Ross"],"pdf_url":"https://arxiv.org/pdf/2505.14216v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2503.12908v3","updated":"2025-05-20T11:19:52Z","published":"2025-03-17T08:17:28Z","title":"HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models","summary":"  Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.\n","authors":["Xinyan Jiang","Hang Ye","Yongxin Zhu","Xiaoying Zheng","Zikang Chen","Jun Gong"],"pdf_url":"https://arxiv.org/pdf/2503.12908v3.pdf","comment":"Accepted by ACL2025 findings"},{"id":"http://arxiv.org/abs/2505.14212v1","updated":"2025-05-20T11:16:29Z","published":"2025-05-20T11:16:29Z","title":"Automatic Dataset Generation for Knowledge Intensive Question Answering\n  Tasks","summary":"  A question-answering (QA) system is to search suitable answers within a\nknowledge base. Current QA systems struggle with queries requiring complex\nreasoning or real-time knowledge integration. They are often supplemented with\nretrieval techniques on a data source such as Retrieval-Augmented Generation\n(RAG). However, RAG continues to face challenges in handling complex reasoning\nand logical connections between multiple sources of information. A novel\napproach for enhancing Large Language Models (LLMs) in knowledge-intensive QA\ntasks is presented through the automated generation of context-based QA pairs.\nThis methodology leverages LLMs to create fine-tuning data, reducing reliance\non human labelling and improving model comprehension and reasoning\ncapabilities. The proposed system includes an automated QA generator and a\nmodel fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.\nComprehensive experiments demonstrate improvements in logical coherence and\nfactual accuracy, with implications for developing adaptable Artificial\nIntelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,\nBLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA\npairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA\npairs.\n","authors":["Sizhe Yuen","Ting Su","Ziyang Wang","Yali Du","Adam J. Sobey"],"pdf_url":"https://arxiv.org/pdf/2505.14212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02429v3","updated":"2025-05-20T11:12:15Z","published":"2024-10-03T12:24:18Z","title":"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models","summary":"  Large Language Models (LLMs) excel in textual and visual tasks but often\nproduce outputs that defy physical laws when dealing with physical-world\nreasoning tasks. Inspired by human cognition, where perception is fundamental\nto reasoning, we explore augmenting LLMs with enhanced perception abilities\nusing Internet of Things (IoT) sensor data and pertinent knowledge for\nIoT-sensory task reasoning in the physical world. In this work, we\nsystematically study LLMs' capability to address real-world IoT-sensory tasks\nby augmenting their perception and knowledge base, and then propose a unified\nframework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three\nsteps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning and activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions. We design a new\nbenchmark comprising five real-world tasks with varying data types and\nreasoning complexities to evaluate the performance of IoT-LLM. Experimental\nresults on six LLMs reveal that IoT-LLM significantly improves the performance\nof IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a\n49.4% average improvement over previous methods.\n","authors":["Tuo An","Yunjiao Zhou","Han Zou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02429v3.pdf","comment":"21 pages, 11 figures, under review"},{"id":"http://arxiv.org/abs/2505.14209v1","updated":"2025-05-20T11:11:46Z","published":"2025-05-20T11:11:46Z","title":"Embedded Mean Field Reinforcement Learning for Perimeter-defense Game","summary":"  With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios.\n","authors":["Li Wang","Xin Yu","Xuxin Lv","Gangzheng Ai","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14206v1","updated":"2025-05-20T11:05:06Z","published":"2025-05-20T11:05:06Z","title":"Challenges and Limitations in the Synthetic Generation of mHealth Sensor\n  Data","summary":"  The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.\n","authors":["Flavio Di Martino","Franca Delmastro"],"pdf_url":"https://arxiv.org/pdf/2505.14206v1.pdf","comment":"Submitted to ACM Transactions on Computing for Healthcare (ACM\n  HEALTH)"}]},"2025-05-18T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2505.12567v1","updated":"2025-05-18T22:55:16Z","published":"2025-05-18T22:55:16Z","title":"A Survey of Attacks on Large Language Models","summary":"  Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats.\n","authors":["Wenrui Xu","Keshab K. Parhi"],"pdf_url":"https://arxiv.org/pdf/2505.12567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12541v1","updated":"2025-05-18T20:38:38Z","published":"2025-05-18T20:38:38Z","title":"Private Statistical Estimation via Truncation","summary":"  We introduce a novel framework for differentially private (DP) statistical\nestimation via data truncation, addressing a key challenge in DP estimation\nwhen the data support is unbounded. Traditional approaches rely on\nproblem-specific sensitivity analysis, limiting their applicability. By\nleveraging techniques from truncated statistics, we develop computationally\nefficient DP estimators for exponential family distributions, including\nGaussian mean and covariance estimation, achieving near-optimal sample\ncomplexity. Previous works on exponential families only consider bounded or\none-dimensional families. Our approach mitigates sensitivity through truncation\nwhile carefully correcting for the introduced bias using maximum likelihood\nestimation and DP stochastic gradient descent. Along the way, we establish\nimproved uniform convergence guarantees for the log-likelihood function of\nexponential families, which may be of independent interest. Our results provide\na general blueprint for DP algorithm design via truncated statistics.\n","authors":["Manolis Zampetakis","Felix Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.12541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06498v2","updated":"2025-05-18T16:54:13Z","published":"2025-05-10T03:40:17Z","title":"An In-kernel Forensics Engine for Investigating Evasive Attacks","summary":"  Over the years, adversarial attempts against critical services have become\nmore effective and sophisticated in launching low-profile attacks. This trend\nhas always been concerning. However, an even more alarming trend is the\nincreasing difficulty of collecting relevant evidence about these attacks and\nthe involved threat actors in the early stages before significant damage is\ndone. This issue puts defenders at a significant disadvantage, as it becomes\nexceedingly difficult to understand the attack details and formulate an\nappropriate response. Developing robust forensics tools to collect evidence\nabout modern threats has never been easy. One main challenge is to provide a\nrobust trade-off between achieving sufficient visibility while leaving minimal\ndetectable artifacts. This paper will introduce LASE, an open-source\nLow-Artifact Forensics Engine to perform threat analysis and forensics in\nWindows operating system. LASE augments current analysis tools by providing\ndetailed, system-wide monitoring capabilities while minimizing detectable\nartifacts. We designed multiple deployment scenarios, showing LASE's potential\nin evidence gathering and threat reasoning in a real-world setting. By making\nLASE and its execution trace data available to the broader research community,\nthis work encourages further exploration in the field by reducing the\nengineering costs for threat analysis and building a longitudinal behavioral\nanalysis catalog for diverse security domains.\n","authors":["Javad Zandi","Lalchandra Rampersaud","Amin Kharraz"],"pdf_url":"https://arxiv.org/pdf/2505.06498v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12490v1","updated":"2025-05-18T16:25:21Z","published":"2025-05-18T16:25:21Z","title":"Proposal for Improving Google A2A Protocol: Safeguarding Sensitive Data\n  in Multi-Agent Systems","summary":"  A2A, a protocol for AI agent communication, offers a robust foundation for\nsecure AI agent communication. However, it has several critical issues in\nhandling sensitive data, such as payment details, identification documents, and\npersonal information. This paper reviews the existing protocol, identifies its\nlimitations, and proposes specific enhancements to improve security, privacy,\nand trust. It includes a concrete example to illustrate the problem and\nsolution, research-backed rationales, and implementation considerations,\ndrawing on prior studies to strengthen the arguments and proposed solutions.\nThis proposal includes seven enhancements: short-lived tokens, customer\nauthentication (SCA), granular scopes, explicit consent, direct data transfer,\nmulti-transaction approval, and payment standard compliance. The vacation\nbooking example illustrates how these enhancements reduce risks and enhance\nuser experience.\n","authors":["Yedidel Louck","Ariel Stulman","Amit Dvir"],"pdf_url":"https://arxiv.org/pdf/2505.12490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12453v1","updated":"2025-05-18T15:05:11Z","published":"2025-05-18T15:05:11Z","title":"SecEmb: Sparsity-Aware Secure Federated Learning of On-Device\n  Recommender System with Large Embedding","summary":"  Federated recommender system (FedRec) has emerged as a solution to protect\nuser data through collaborative training techniques. A typical FedRec involves\ntransmitting the full model and entire weight updates between edge devices and\nthe server, causing significant burdens to devices with limited bandwidth and\ncomputational power. While the sparsity of embedding updates provides\nopportunity for payload optimization, existing sparsity-aware federated\nprotocols generally sacrifice privacy for efficiency. A key challenge in\ndesigning a secure sparsity-aware efficient protocol is to protect the rated\nitem indices from the server. In this paper, we propose a lossless secure\nrecommender systems on sparse embedding updates (SecEmb). SecEmb reduces user\npayload while ensuring that the server learns no information about both rated\nitem indices and individual updates except the aggregated model. The protocol\nconsists of two correlated modules: (1) a privacy-preserving embedding\nretrieval module that allows users to download relevant embeddings from the\nserver, and (2) an update aggregation module that securely aggregates updates\nat the server. Empirical analysis demonstrates that SecEmb reduces both\ndownload and upload communication costs by up to 90x and decreases user-side\ncomputation time by up to 70x compared with secure FedRec protocols.\nAdditionally, it offers non-negligible utility advantages compared with lossy\nmessage compression methods.\n","authors":["Peihua Mai","Youlong Ding","Ziyan Lyu","Minxin Du","Yan Pang"],"pdf_url":"https://arxiv.org/pdf/2505.12453v1.pdf","comment":"26 pages, accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2503.22330v2","updated":"2025-05-18T14:52:04Z","published":"2025-03-28T11:11:19Z","title":"WMCopier: Forging Invisible Image Watermarks on Arbitrary Images","summary":"  Invisible Image Watermarking is crucial for ensuring content provenance and\naccountability in generative AI. While Gen-AI providers are increasingly\nintegrating invisible watermarking systems, the robustness of these schemes\nagainst forgery attacks remains poorly characterized. This is critical, as\nforging traceable watermarks onto illicit content leads to false attribution,\npotentially harming the reputation and legal standing of Gen-AI service\nproviders who are not responsible for the content. In this work, we propose\nWMCopier, an effective watermark forgery attack that operates without requiring\nany prior knowledge of or access to the target watermarking algorithm. Our\napproach first models the target watermark distribution using an unconditional\ndiffusion model, and then seamlessly embeds the target watermark into a\nnon-watermarked image via a shallow inversion process. We also incorporate an\niterative optimization procedure that refines the reconstructed image to\nfurther trade off the fidelity and forgery efficiency. Experimental results\ndemonstrate that WMCopier effectively deceives both open-source and\nclosed-source watermark systems (e.g., Amazon's system), achieving a\nsignificantly higher success rate than existing methods. Additionally, we\nevaluate the robustness of forged samples and discuss the potential defenses\nagainst our attack.\n","authors":["Ziping Dong","Chao Shuai","Zhongjie Ba","Peng Cheng","Zhan Qin","Qinglong Wang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2503.22330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12402v1","updated":"2025-05-18T13:05:17Z","published":"2025-05-18T13:05:17Z","title":"Automated Profile Inference with Language Model Agents","summary":"  Impressive progress has been made in automated problem-solving by the\ncollaboration of large language models (LLMs) based agents. However, these\nautomated capabilities also open avenues for malicious applications. In this\npaper, we study a new threat that LLMs pose to online pseudonymity, called\nautomated profile inference, where an adversary can instruct LLMs to\nautomatically scrape and extract sensitive personal attributes from publicly\nvisible user activities on pseudonymous platforms. We also introduce an\nautomated profiling framework called AutoProfiler to assess the feasibility of\nsuch threats in real-world scenarios. AutoProfiler consists of four specialized\nLLM agents, who work collaboratively to collect and process user online\nactivities and generate a profile with extracted personal information.\nExperimental results on two real-world datasets and one synthetic dataset\ndemonstrate that AutoProfiler is highly effective and efficient, and can be\neasily deployed on a web scale. We demonstrate that the inferred attributes are\nboth sensitive and identifiable, posing significant risks of privacy breaches,\nsuch as de-anonymization and sensitive information leakage. Additionally, we\nexplore mitigation strategies from different perspectives and advocate for\nincreased public awareness of this emerging privacy threat to online\npseudonymity.\n","authors":["Yuntao Du","Zitao Li","Bolin Ding","Yaliang Li","Hanshen Xiao","Jingren Zhou","Ninghui Li"],"pdf_url":"https://arxiv.org/pdf/2505.12402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12393v1","updated":"2025-05-18T12:43:10Z","published":"2025-05-18T12:43:10Z","title":"Protocol as Poetry: Case Study on Pak's Protocol Arts","summary":"  Protocol art emerges at the confluence of blockchain-based smart contracts\nand a century-long lineage of conceptual art, participatory art, and\nalgorithmic generative art practices. Yet existing definitions-most notably\nPrimavera De Filippi's \"protocolism\"-struggle to demarcate this nascent genre\nfrom other art forms in practice. Addressing this definition-to-practice gap,\nthis paper offers a focused case study of pioneering protocol artworks by Pak,\nan early and influential pseudonymous protocol artist who treats smart\ncontracts as medium and protocol participation as message. Tracing the\nevolution from early open-edition releases of The Fungible and the dynamic\nmechanics of Merge to the soul-bound messaging of Censored and the reflective\nabsence of Not Found, we examine how Pak choreographs distributed agency across\ncollectors and autonomous contracts, showing how programmable protocols become\na social fabric in artistic meaning-making. Through thematic analysis of Pak's\nworks, we identify seven core characteristics that distinguish protocol art:\n(1) system-centric rather than object-centric composition, (2) autonomous\ngovernance for open-ended control, (3) distributed agency and communal\nauthorship, (4) temporal dynamism and lifecycle aesthetics, (5) economic-driven\nengagement, (6) poetic message embedding in interaction rituals, and (7)\ninteroperability enabling composability for emergence. We then discuss how\nthese features set protocol art apart from adjacent artistic movements. By\ndeveloping a theoretical framework grounded in Pak's practice, we contribute to\nthe emerging literature on protocolism while offering design implications for\nartists shaping this evolving art form.\n","authors":["Botao Amber Hu"],"pdf_url":"https://arxiv.org/pdf/2505.12393v1.pdf","comment":"Submitted to Ars Electronica Expanded Conference 2025 Research Paper"},{"id":"http://arxiv.org/abs/2505.12335v1","updated":"2025-05-18T10:00:39Z","published":"2025-05-18T10:00:39Z","title":"Is Artificial Intelligence Generated Image Detection a Solved Problem?","summary":"  The rapid advancement of generative models, such as GANs and Diffusion\nmodels, has enabled the creation of highly realistic synthetic images, raising\nserious concerns about misinformation, deepfakes, and copyright infringement.\nAlthough numerous Artificial Intelligence Generated Image (AIGI) detectors have\nbeen proposed, often reporting high accuracy, their effectiveness in real-world\nscenarios remains questionable. To bridge this gap, we introduce AIGIBench, a\ncomprehensive benchmark designed to rigorously evaluate the robustness and\ngeneralization capabilities of state-of-the-art AIGI detectors. AIGIBench\nsimulates real-world challenges through four core tasks: multi-source\ngeneralization, robustness to image degradation, sensitivity to data\naugmentation, and impact of test-time pre-processing. It includes 23 diverse\nfake image subsets that span both advanced and widely adopted image generation\ntechniques, along with real-world samples collected from social media and AI\nart platforms. Extensive experiments on 11 advanced detectors demonstrate that,\ndespite their high reported accuracy in controlled settings, these detectors\nsuffer significant performance drops on real-world data, limited benefits from\ncommon augmentations, and nuanced effects of pre-processing, highlighting the\nneed for more robust detection strategies. By providing a unified and realistic\nevaluation framework, AIGIBench offers valuable insights to guide future\nresearch toward dependable and generalizable AIGI detection.\n","authors":["Ziqiang Li","Jiazhen Yan","Ziwen He","Kai Zeng","Weiwei Jiang","Lizhi Xiong","Zhangjie Fu"],"pdf_url":"https://arxiv.org/pdf/2505.12335v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.12296v1","updated":"2025-05-18T08:19:18Z","published":"2025-05-18T08:19:18Z","title":"PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained\n  Watermarking","summary":"  Machine learning models are increasingly shared and outsourced, raising\nrequirements of verifying training effort (Proof-of-Learning, PoL) to ensure\nclaimed performance and establishing ownership (Proof-of-Ownership, PoO) for\ntransactions. When models are trained by untrusted parties, PoL and PoO must be\nenforced together to enable protection, attribution, and compensation. However,\nexisting studies typically address them separately, which not only weakens\nprotection against forgery and privacy breaches but also leads to high\nverification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO\nusing chained watermarks. PoLO splits the training process into fine-grained\ntraining shards and embeds a dedicated watermark in each shard. Each watermark\nis generated using the hash of the preceding shard, certifying the training\nprocess of the preceding shard. The chained structure makes it computationally\ndifficult to forge any individual part of the whole training process. The\ncomplete set of watermarks serves as the PoL, while the final watermark\nprovides the PoO. PoLO offers more efficient and privacy-preserving\nverification compared to the vanilla PoL solutions that rely on gradient-based\ntrajectory tracing and inadvertently expose training data during verification,\nwhile maintaining the same level of ownership assurance of watermark-based PoO\nschemes. Our evaluation shows that PoLO achieves 99% watermark detection\naccuracy for ownership verification, while preserving data privacy and cutting\nverification costs to just 1.5-10% of traditional methods. Forging PoLO demands\n1.1-4x more resources than honest proof generation, with the original proof\nretaining over 90% detection accuracy even after attacks.\n","authors":["Haiyu Deng","Yanna Jiang","Guangsheng Yu","Qin Wang","Xu Wang","Baihe Ma","Wei Ni","Ren Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2505.12296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00735v3","updated":"2025-05-18T07:29:41Z","published":"2025-02-02T10:05:08Z","title":"`Do as I say not as I do': A Semi-Automated Approach for Jailbreak\n  Prompt Attack against Multimodal LLMs","summary":"  Large Language Models (LLMs) have seen widespread applications across various\ndomains due to their growing ability to process diverse types of input data,\nincluding text, audio, image and video. While LLMs have demonstrated\noutstanding performance in understanding and generating contexts for different\nscenarios, they are vulnerable to prompt-based attacks, which are mostly via\ntext input. In this paper, we introduce the first voice-based jailbreak attack\nagainst multimodal LLMs, termed as Flanking Attack, which can process different\ntypes of input simultaneously towards the multimodal LLMs. Our work is\nmotivated by recent advancements in monolingual voice-driven large language\nmodels, which have introduced new attack surfaces beyond traditional text-based\nvulnerabilities for LLMs. To investigate these risks, we examine the\nstate-of-the-art multimodal LLMs, which can be accessed via different types of\ninputs such as audio input, focusing on how adversarial prompts can bypass its\ndefense mechanisms. We propose a novel strategy, in which the disallowed prompt\nis flanked by benign, narrative-driven prompts. It is integrated in the\nFlanking Attack which attempts to humanizes the interaction context and execute\nthe attack through a fictional setting. Further, to better evaluate the attack\nperformance, we present a semi-automated self-assessment framework for policy\nviolation detection. We demonstrate that Flanking Attack is capable of\nmanipulating state-of-the-art LLMs into generating misaligned and forbidden\noutputs, which achieves an average attack success rate ranging from 0.67 to\n0.93 across seven forbidden scenarios.\n","authors":["Chun Wai Chiu","Linghan Huang","Bo Li","Huaming Chen","Kim-Kwang Raymond Choo"],"pdf_url":"https://arxiv.org/pdf/2502.00735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15631v2","updated":"2025-05-18T06:57:37Z","published":"2024-10-21T04:27:41Z","title":"Security of Language Models for Code: A Systematic Literature Review","summary":"  Language models for code (CodeLMs) have emerged as powerful tools for\ncode-related tasks, outperforming traditional methods and standard machine\nlearning approaches. However, these models are susceptible to security\nvulnerabilities, drawing increasing research attention from domains such as\nsoftware engineering, artificial intelligence, and cybersecurity. Despite the\ngrowing body of research focused on the security of CodeLMs, a comprehensive\nsurvey in this area remains absent. To address this gap, we systematically\nreview 67 relevant papers, organizing them based on attack and defense\nstrategies. Furthermore, we provide an overview of commonly used language\nmodels, datasets, and evaluation metrics, and highlight open-source tools and\npromising directions for future research in securing CodeLMs.\n","authors":["Yuchen Chen","Weisong Sun","Chunrong Fang","Zhenpeng Chen","Yifei Ge","Tingxu Han","Quanjun Zhang","Yang Liu","Zhenyu Chen","Baowen Xu"],"pdf_url":"https://arxiv.org/pdf/2410.15631v2.pdf","comment":"Accepted to ACM Transactions on Software Engineering and Methodology\n  (TOSEM)"},{"id":"http://arxiv.org/abs/2505.12256v1","updated":"2025-05-18T06:31:45Z","published":"2025-05-18T06:31:45Z","title":"TPM2.0-Supported Runtime Customizable TEE on FPGA-SoC with\n  User-Controllable vTPM","summary":"  Constructing a Trusted Execution Environment (TEE) on Field Programmable Gate\nArray System on Chip (FPGA-SoC) in Cloud can effectively protect users' private\nintel-lectual Property (IP) cores. In order to facilitate the wide-spread\ndeployment of FPGA-SoC TEE, this paper proposes an approach for constructing a\nTPM 2.0-compatible runtime customizable TEE on FPGA-SoC. This approach\nleverages a user-controllable virtual Trusted Platform Module (vTPM) that\nintegrates sensitive operations specific to FPGA-SoC TEE. It provides TPM 2.0\nsupport for a customizable FPGA-SoC TEE to dynamically measure, deploy, and\ninvoke IP during runtime. Our main contributions include: (i) Propose an\nFPGA-vTPM architecture that enables the TPM 2.0 specification support for\nFPGA-SoC TEE; (ii) Explore the utilization of FPGA-vTPM to dynamically measure,\ndeploy, and invoke users' IPs on FPGA-SoC TEE; (iii) Extend the TPM command set\nto accommodate the sensitive operations of FPGA-SoC TEE, enabling users to\nperform sensitive tasks in a secure and verifiable manner according to the TPM\n2.0 specification. We implement a prototype of TRCTEE on the Xilinx Zynq\nUltraScale+ MPSoC platform and conducted security analysis and performance\nevaluations to prove the practicality and enhanced security features of this\napproach.\n","authors":["Jingkai Mao","Xiaolin Chang"],"pdf_url":"https://arxiv.org/pdf/2505.12256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10281v3","updated":"2025-05-18T05:31:35Z","published":"2024-06-12T05:13:09Z","title":"Watermarking Language Models with Error Correcting Codes","summary":"  Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating $p$-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.\n","authors":["Patrick Chao","Yan Sun","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2406.10281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12239v1","updated":"2025-05-18T05:28:18Z","published":"2025-05-18T05:28:18Z","title":"ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting\n  with Privacy Preservation","summary":"  The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency.\n","authors":["Jianheng Tang","Huiping Zhuang","Di Fang","Jiaxu Li","Feijiang Han","Yajiang Huang","Kejia Fan","Leye Wang","Zhanxing Zhu","Shanghang Zhang","Houbing Herbert Song","Yunhuai Liu"],"pdf_url":"https://arxiv.org/pdf/2505.12239v1.pdf","comment":"21 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.12210v1","updated":"2025-05-18T03:02:30Z","published":"2025-05-18T03:02:30Z","title":"Nonmalleable Progress Leakage","summary":"  Information-flow control systems often enforce progress-insensitive\nnoninterference, as it is simple to understand and enforce. Unfortunately, real\nprograms need to declassify results and endorse inputs, which noninterference\ndisallows, while preventing attackers from controlling leakage, including\nthrough progress channels, which progress-insensitivity ignores.\n  This work combines ideas for progress-sensitive security with secure\ndowngrading (declassification and endorsement) to identify a notion of securely\ndowngrading progress information. We use hyperproperties to distill the\nseparation between progress-sensitive and progress-insensitive noninterference\nand combine it with nonmalleable information flow, an existing\n(progress-insensitive) definition of secure downgrading, to define nonmalleable\nprogress leakage (NMPL). We present the first information-flow type system to\nallow some progress leakage while enforcing NMPL, and we show how to infer the\nlocation of secure progress downgrades. All theorems are verified in Rocq.\n","authors":["Ethan Cecchetti"],"pdf_url":"https://arxiv.org/pdf/2505.12210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12186v1","updated":"2025-05-18T01:08:18Z","published":"2025-05-18T01:08:18Z","title":"Self-Destructive Language Model","summary":"  Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)\n","authors":["Yuhui Wang","Rongyi Zhu","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12186v1.pdf","comment":null}]},"2025-05-17T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2505.12167v1","updated":"2025-05-17T22:51:52Z","published":"2025-05-17T22:51:52Z","title":"FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting\n  Models","summary":"  Deep learning-based weather forecasting models have recently demonstrated\nsignificant performance improvements over gold-standard physics-based\nsimulation tools. However, these models are vulnerable to adversarial attacks,\nwhich raises concerns about their trustworthiness. In this paper, we first\ninvestigate the feasibility of applying existing adversarial attack methods to\nweather forecasting models. We argue that a successful attack should (1) not\nmodify significantly its original inputs, (2) be faithful, i.e., achieve the\ndesired forecast at targeted locations with minimal changes to non-targeted\nlocations, and (3) be geospatio-temporally realistic. However, balancing these\ncriteria is a challenge as existing methods are not designed to preserve the\ngeospatio-temporal dependencies of the original samples. To address this\nchallenge, we propose a novel framework called FABLE (Forecast Alteration By\nLocalized targeted advErsarial attack), which employs a 3D discrete wavelet\ndecomposition to extract the varying components of the geospatio-temporal data.\nBy regulating the magnitude of adversarial perturbations across different\ncomponents, FABLE can generate adversarial inputs that maintain\ngeospatio-temporal coherence while remaining faithful and closely aligned with\nthe original inputs. Experimental results on multiple real-world datasets\ndemonstrate the effectiveness of our framework over baseline methods across\nvarious metrics.\n","authors":["Yue Deng","Asadullah Hill Galib","Xin Lan","Pang-Ning Tan","Lifeng Luo"],"pdf_url":"https://arxiv.org/pdf/2505.12167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12144v1","updated":"2025-05-17T21:28:56Z","published":"2025-05-17T21:28:56Z","title":"Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing\n  Stake for Social Capital","summary":"  Consensus protocols used today in blockchains often rely on computational\npower or financial stakes - scarce resources. We propose a novel protocol using\nsocial capital - trust and influence from social interactions - as a\nnon-transferable staking mechanism to ensure fairness and decentralization. The\nmethodology integrates zero-knowledge proofs, verifiable credentials, a\nWhisk-like leader election, and an incentive scheme to prevent Sybil attacks\nand encourage engagement. The theoretical framework would enhance privacy and\nequity, though unresolved issues like off-chain bribery require further\nresearch. This work offers a new model aligned with modern social media\nbehavior and lifestyle, with applications in finance, providing a practical\ninsight for decentralized system development.\n","authors":["Juraj Mariani","Ivan Homoliak"],"pdf_url":"https://arxiv.org/pdf/2505.12144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12128v1","updated":"2025-05-17T19:41:44Z","published":"2025-05-17T19:41:44Z","title":"Back to Square Roots: An Optimal Bound on the Matrix Factorization Error\n  for Multi-Epoch Differentially Private SGD","summary":"  Matrix factorization mechanisms for differentially private training have\nemerged as a promising approach to improve model utility under privacy\nconstraints. In practical settings, models are typically trained over multiple\nepochs, requiring matrix factorizations that account for repeated\nparticipation. Existing theoretical upper and lower bounds on multi-epoch\nfactorization error leave a significant gap. In this work, we introduce a new\nexplicit factorization method, Banded Inverse Square Root (BISR), which imposes\na banded structure on the inverse correlation matrix. This factorization\nenables us to derive an explicit and tight characterization of the multi-epoch\nerror. We further prove that BISR achieves asymptotically optimal error by\nmatching the upper and lower bounds. Empirically, BISR performs on par with\nstate-of-the-art factorization methods, while being simpler to implement,\ncomputationally efficient, and easier to analyze.\n","authors":["Nikita P. Kalinin","Ryan McKenna","Jalaj Upadhyay","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2505.12128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12106v1","updated":"2025-05-17T18:19:35Z","published":"2025-05-17T18:19:35Z","title":"MalVis: A Large-Scale Image-Based Framework and Dataset for Advancing\n  Android Malware Classification","summary":"  As technology advances, Android malware continues to pose significant threats\nto devices and sensitive data. The open-source nature of the Android OS and the\navailability of its SDK contribute to this rapid growth. Traditional malware\ndetection techniques, such as signature-based, static, and dynamic analysis,\nstruggle to detect obfuscated threats that use encryption, packing, or\ncompression. While deep learning (DL)-based visualization methods have been\nproposed, they often fail to highlight the critical malicious features\neffectively. This research introduces MalVis, a unified visualization framework\nthat integrates entropy and N-gram analysis to emphasize structural and\nanomalous patterns in malware bytecode. MalVis addresses key limitations of\nprior methods, including insufficient feature representation, poor\ninterpretability, and limited data accessibility. The framework leverages a\nnewly introduced large-scale dataset, the MalVis dataset, containing over 1.3\nmillion visual samples across nine malware classes and one benign class. We\nevaluate MalVis against state-of-the-art visualization techniques using leading\nCNN models: MobileNet-V2, DenseNet201, ResNet50, and Inception-V3. To enhance\nperformance and reduce overfitting, we implement eight ensemble learning\nstrategies. Additionally, an undersampling technique mitigates class imbalance\nin the multiclass setting. MalVis achieves strong results: 95.19% accuracy,\n90.81% F1-score, 92.58% precision, 89.10% recall, 87.58% MCC, and 98.06%\nROC-AUC. These findings demonstrate the effectiveness of MalVis in enabling\naccurate, interpretable malware detection and providing a valuable resource for\nsecurity research and applications.\n","authors":["Saleh J. Makkawy","Michael J. De Lucia","Kenneth E. Barner"],"pdf_url":"https://arxiv.org/pdf/2505.12106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12104v1","updated":"2025-05-17T18:14:57Z","published":"2025-05-17T18:14:57Z","title":"The Impact of Emerging Phishing Threats: Assessing Quishing and\n  LLM-generated Phishing Emails against Organizations","summary":"  Modern organizations are persistently targeted by phishing emails. Despite\nadvances in detection systems and widespread employee training, attackers\ncontinue to innovate, posing ongoing threats. Two emerging vectors stand out in\nthe current landscape: QR-code baits and LLM-enabled pretexting. Yet, little is\nknown about the effectiveness of current defenses against these attacks,\nparticularly when it comes to real-world impact on employees. This gap leaves\nuncertainty around to what extent related countermeasures are justified or\nneeded. Our work addresses this issue.\n  We conduct three phishing simulations across organizations of varying sizes\n-- from small-medium businesses to a multinational enterprise. In total, we\nsend over 71k emails targeting employees, including: a \"traditional\" phishing\nemail with a click-through button; a nearly-identical \"quishing\" email with a\nQR code instead; and a phishing email written with the assistance of an LLM and\nopen-source intelligence. Our results show that quishing emails have the same\neffectiveness as traditional phishing emails at luring users to the landing\nwebpage -- which is worrying, given that quishing emails are much harder to\nidentify even by operational detectors. We also find that LLMs can be very good\n\"social engineers\": in one company, over 30% of the emails opened led to\nvisiting the landing webpage -- a rate exceeding some prior benchmarks.\nFinally, we complement our study by conducting a survey across the\norganizations' employees, measuring their \"perceived\" phishing awareness. Our\nfindings suggest a correlation between higher self-reported awareness and\norganizational resilience to phishing attempts.\n","authors":["Marie Weinz","Nicola Zannone","Luca Allodi","Giovanni Apruzzese"],"pdf_url":"https://arxiv.org/pdf/2505.12104v1.pdf","comment":"Accepted to AsiaCCS'25"},{"id":"http://arxiv.org/abs/2411.19939v3","updated":"2025-05-17T15:14:14Z","published":"2024-11-29T18:56:37Z","title":"VLSBench: Unveiling Visual Leakage in Multimodal Safety","summary":"  Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counterintuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs aligned with\nimage text pairs. To explain such a phenomenon, we discover a Visual Safety\nInformation Leakage (VSIL) problem in existing multimodal safety benchmarks,\ni.e., the potentially risky content in the image has been revealed in the\ntextual query. Thus, MLLMs can easily refuse these sensitive image-text pairs\naccording to textual queries only, leading to unreliable cross-modality safety\nevaluation of MLLMs. We also conduct a further comparison experiment between\ntextual alignment and multimodal alignment to highlight this drawback. To this\nend, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k\nimage-text pairs through an automated data pipeline. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically\ncompare textual and multimodal alignment methods on VLSBench and find that\ntextual alignment is effective enough for multimodal safety scenarios with\nVSIL, while multimodal alignment is preferable for safety scenarios without\nVSIL. Code and data are released under https://github.com/AI45Lab/VLSBench\n","authors":["Xuhao Hu","Dongrui Liu","Hao Li","Xuanjing Huang","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2411.19939v3.pdf","comment":"ACL2025 Main"},{"id":"http://arxiv.org/abs/2305.15560v4","updated":"2025-05-17T15:12:18Z","published":"2023-05-24T23:47:26Z","title":"Differentially Private Synthetic Data via Foundation Model APIs 1:\n  Images","summary":"  Generating differentially private (DP) synthetic data that closely resembles\nthe original private data is a scalable way to mitigate privacy concerns in the\ncurrent data-driven world. In contrast to current practices that train\ncustomized models for this task, we aim to generate DP Synthetic Data via APIs\n(DPSDA), where we treat foundation models as blackboxes and only utilize their\ninference APIs. Such API-based, training-free approaches are easier to deploy\nas exemplified by the recent surge in the number of API-based apps. These\napproaches can also leverage the power of large foundation models which are\nonly accessible via their inference APIs. However, this comes with greater\nchallenges due to strictly more restrictive model access and the need to\nprotect privacy from the API provider.\n  In this paper, we present a new framework called Private Evolution (PE) to\nsolve this problem and show its initial promise on synthetic images.\nSurprisingly, PE can match or even outperform state-of-the-art (SOTA) methods\nwithout any model training. For example, on CIFAR10 (with ImageNet as the\npublic data), we achieve FID <= 7.9 with privacy cost {\\epsilon} = 0.67,\nsignificantly improving the previous SOTA from {\\epsilon} = 32. We further\ndemonstrate the promise of applying PE on large foundation models such as\nStable Diffusion to tackle challenging private datasets with a small number of\nhigh-resolution images. The code and data are released at\nhttps://github.com/microsoft/DPSDA.\n","authors":["Zinan Lin","Sivakanth Gopi","Janardhan Kulkarni","Harsha Nori","Sergey Yekhanin"],"pdf_url":"https://arxiv.org/pdf/2305.15560v4.pdf","comment":"Published in ICLR 2024"},{"id":"http://arxiv.org/abs/2505.12038v1","updated":"2025-05-17T15:01:07Z","published":"2025-05-17T15:01:07Z","title":"Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on\n  Diverse Datasets","summary":"  Large language models (LLMs) have shown great potential as general-purpose AI\nassistants across various domains. To fully leverage this potential in specific\napplications, many companies provide fine-tuning API services, enabling users\nto upload their own data for LLM customization. However, fine-tuning services\nintroduce a new safety threat: user-uploaded data, whether harmful or benign,\ncan break the model's alignment, leading to unsafe outputs. Moreover, existing\ndefense methods struggle to address the diversity of fine-tuning datasets\n(e.g., varying sizes, tasks), often sacrificing utility for safety or vice\nversa. To address this issue, we propose Safe Delta, a safety-aware\npost-training defense method that adjusts the delta parameters (i.e., the\nparameter change before and after fine-tuning). Specifically, Safe Delta\nestimates the safety degradation, selects delta parameters to maximize utility\nwhile limiting overall safety loss, and applies a safety compensation vector to\nmitigate residual safety loss. Through extensive experiments on four diverse\ndatasets with varying settings, our approach consistently preserves safety\nwhile ensuring that the utility gain from benign datasets remains unaffected.\n","authors":["Ning Lu","Shengcai Liu","Jiahao Wu","Weiyu Chen","Zhirui Zhang","Yew-Soon Ong","Qi Wang","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2505.12038v1.pdf","comment":"ICML 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2505.12019v1","updated":"2025-05-17T14:16:47Z","published":"2025-05-17T14:16:47Z","title":"FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor\n  Defense Against High-Ratio Malicious Clients","summary":"  Federated learning (FL) is gaining increasing attention as an emerging\ncollaborative machine learning approach, particularly in the context of\nlarge-scale computing and data systems. However, the fundamental algorithm of\nFL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although\nresearchers have proposed numerous defense algorithms, two significant\nchallenges remain. The attack is becoming more stealthy and harder to detect,\nand current defense methods are unable to handle 50\\% or more malicious users\nor assume an auxiliary server dataset.\n  To address these challenges, we propose a novel defense algorithm, FL-PLAS,\n\\textbf{F}ederated \\textbf{L}earning based on \\textbf{P}artial\\textbf{ L}ayer\n\\textbf{A}ggregation \\textbf{S}trategy. In particular, we divide the local\nmodel into a feature extractor and a classifier. In each iteration, the clients\nonly upload the parameters of a feature extractor after local training. The\nserver then aggregates these local parameters and returns the results to the\nclients.\n  Each client retains its own classifier layer, ensuring that the backdoor\nlabels do not impact other clients. We assess the effectiveness of FL-PLAS\nagainst state-of-the-art (SOTA) backdoor attacks on three image datasets and\ncompare our approach to six defense strategies. The results of the experiment\ndemonstrate that our methods can effectively protect local models from backdoor\nattacks. Without requiring any auxiliary dataset for the server, our method\nachieves a high main-task accuracy with a lower backdoor accuracy even under\nthe condition of 90\\% malicious users with the attacks of trigger, semantic and\nedge-case.\n","authors":["Jianyi Zhang","Ziyin Zhou","Yilong Li","Qichao Jin"],"pdf_url":"https://arxiv.org/pdf/2505.12019v1.pdf","comment":"20pages"},{"id":"http://arxiv.org/abs/2505.12018v1","updated":"2025-05-17T14:16:16Z","published":"2025-05-17T14:16:16Z","title":"A Human Study of Cognitive Biases in CTF Challenges","summary":"  Cybersecurity training has become a crucial part of computer science\neducation and industrial onboarding. Capture the Flag (CTF) competitions have\nemerged as a valuable, gamified approach for developing and refining the skills\nof cybersecurity and software engineering professionals. However, while CTFs\nprovide a controlled environment for tackling real world challenges, the\nparticipants' decision making and problem solving processes remain under\nexplored. Recognizing that psychology may play a role in a cyber attacker's\nbehavior, we investigate how cognitive biases could be used to improve CTF\neducation and security. In this paper, we present an approach to control\ncognitive biases, specifically Satisfaction of Search and Loss Aversion, to\ninfluence and potentially hinder attackers' effectiveness against web\napplication vulnerabilities in a CTF style challenge. We employ a rigorous\nquantitative and qualitative analysis through a controlled human study of CTF\ntasks. CTF exercises are widely used in cybersecurity education and research to\nsimulate real world attack scenarios and help participants develop critical\nskills by solving security challenges in controlled environments. In our study,\nparticipants interact with a web application containing deliberately embedded\nvulnerabilities while being subjected to tasks designed to trigger cognitive\nbiases. Our study reveals that many participants exhibit the Satisfaction of\nSearch bias and that this bias has a significant effect on their success. On\naverage, participants found 25% fewer flags compared to those who did not\nexhibit this bias. Our findings provide valuable insights into how cognitive\nbiases can be strategically employed to enhance cybersecurity outcomes,\neducation, and measurements through the lens of CTF challenges.\n","authors":["Yuwei Yang","Skyler Grandel","Daniel Balasubramanian","Yu Huang","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2505.12018v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.11988v1","updated":"2025-05-17T12:46:10Z","published":"2025-05-17T12:46:10Z","title":"TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text","summary":"  Accurately identifying adversarial techniques in security texts is critical\nfor effective cyber defense. However, existing methods face a fundamental\ntrade-off: they either rely on generic models with limited domain precision or\nrequire resource-intensive pipelines that depend on large labeled datasets and\ntask-specific optimizations, such as custom hard-negative mining and denoising,\nresources rarely available in specialized domains.\n  We propose TechniqueRAG, a domain-specific retrieval-augmented generation\n(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,\ninstruction-tuned LLMs, and minimal text-technique pairs. Our approach\naddresses data scarcity by fine-tuning only the generation component on limited\nin-domain examples, circumventing the need for resource-intensive retrieval\ntraining. While conventional RAG mitigates hallucination by coupling retrieval\nand generation, its reliance on generic retrievers often introduces noisy\ncandidates, limiting domain-specific precision. To address this, we enhance\nretrieval quality and domain specificity through zero-shot LLM re-ranking,\nwhich explicitly aligns retrieved candidates with adversarial techniques.\n  Experiments on multiple security benchmarks demonstrate that TechniqueRAG\nachieves state-of-the-art performance without extensive task-specific\noptimizations or labeled data, while comprehensive analysis provides further\ninsights.\n","authors":["Ahmed Lekssays","Utsav Shukla","Husrev Taha Sencar","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2505.11988v1.pdf","comment":"Accepted at ACL (Findings) 2025"},{"id":"http://arxiv.org/abs/2503.09712v3","updated":"2025-05-17T11:45:29Z","published":"2025-03-12T18:05:32Z","title":"Revisiting Backdoor Attacks on Time Series Classification in the\n  Frequency Domain","summary":"  Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.\n","authors":["Yuanmin Huang","Mi Zhang","Zhaoxiang Wang","Wenxuan Li","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2503.09712v3.pdf","comment":"WWW 2025 (Oral)"},{"id":"http://arxiv.org/abs/2505.11963v1","updated":"2025-05-17T11:31:24Z","published":"2025-05-17T11:31:24Z","title":"MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language\n  Models","summary":"  Hardware security verification is a challenging and time-consuming task. For\nthis purpose, design engineers may utilize tools such as formal verification,\nlinters, and functional simulation tests, coupled with analysis and a deep\nunderstanding of the hardware design being inspected. Large Language Models\n(LLMs) have been used to assist during this task, either directly or in\nconjunction with existing tools. We improve the state of the art by proposing\nMARVEL, a multi-agent LLM framework for a unified approach to decision-making,\ntool use, and reasoning. MARVEL mimics the cognitive process of a designer\nlooking for security vulnerabilities in RTL code. It consists of a supervisor\nagent that devises the security policy of the system-on-chips (SoCs) using its\nsecurity documentation. It delegates tasks to validate the security policy to\nindividual executor agents. Each executor agent carries out its assigned task\nusing a particular strategy. Each executor agent may use one or more tools to\nidentify potential security bugs in the design and send the results back to the\nsupervisor agent for further analysis and confirmation. MARVEL includes\nexecutor agents that leverage formal tools, linters, simulation tests,\nLLM-based detection schemes, and static analysis-based checks. We test our\napproach on a known buggy SoC based on OpenTitan from the Hack@DATE\ncompetition. We find that 20 of the 48 issues reported by MARVEL pose security\nvulnerabilities.\n","authors":["Luca Collini","Baleegh Ahmad","Joey Ah-kiow","Ramesh Karri"],"pdf_url":"https://arxiv.org/pdf/2505.11963v1.pdf","comment":"Submitted for Peer Review"},{"id":"http://arxiv.org/abs/2502.03052v2","updated":"2025-05-17T10:47:24Z","published":"2025-02-05T10:29:54Z","title":"Understanding and Enhancing the Transferability of Jailbreaking Attacks","summary":"  Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs.\n","authors":["Runqi Lin","Bo Han","Fengwang Li","Tongling Liu"],"pdf_url":"https://arxiv.org/pdf/2502.03052v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2411.05335v3","updated":"2025-05-17T10:04:20Z","published":"2024-11-08T05:14:46Z","title":"A Quality-Centric Framework for Generic Deepfake Detection","summary":"  Detecting AI-generated images, particularly deepfakes, has become\nincreasingly crucial, with the primary challenge being the generalization to\npreviously unseen manipulation methods. This paper tackles this issue by\nleveraging the forgery quality of training data to improve the generalization\nperformance of existing deepfake detectors. Generally, the forgery quality of\ndifferent deepfakes varies: some have easily recognizable forgery clues, while\nothers are highly realistic. Existing works often train detectors on a mix of\ndeepfakes with varying forgery qualities, potentially leading detectors to\nshort-cut the easy-to-spot artifacts from low-quality forgery samples, thereby\nhurting generalization performance. To tackle this issue, we propose a novel\nquality-centric framework for generic deepfake detection, which is composed of\na Quality Evaluator, a low-quality data enhancement module, and a learning\npacing strategy that explicitly incorporates forgery quality into the training\nprocess. Our framework is inspired by curriculum learning, which is designed to\ngradually enable the detector to learn more challenging deepfake samples,\nstarting with easier samples and progressing to more realistic ones. We employ\nboth static and dynamic assessments to assess the forgery quality, combining\ntheir scores to produce a final rating for each training sample. The rating\nscore guides the selection of deepfake samples for training, with higher-rated\nsamples having a higher probability of being chosen. Furthermore, we propose a\nnovel frequency data augmentation method specifically designed for low-quality\nforgery samples, which helps to reduce obvious forgery traces and improve their\noverall realism. Extensive experiments demonstrate that our proposed framework\ncan be applied plug-and-play to existing detection models and significantly\nenhance their generalization performance in detection.\n","authors":["Wentang Song","Zhiyuan Yan","Yuzhen Lin","Taiping Yao","Changsheng Chen","Shen Chen","Yandan Zhao","Shouhong Ding","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2411.05335v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11928v1","updated":"2025-05-17T09:25:47Z","published":"2025-05-17T09:25:47Z","title":"Efficient Implementations of Residue Generators Mod 2n + 1 Providing\n  Diminished-1 Representation","summary":"  The moduli of the form 2n + 1 belong to a class of low-cost odd moduli, which\nhave been frequently selected to form the basis of various residue number\nsystems (RNS). The most efficient computations modulo (mod) 2n + 1 are\nperformed using the so-called diminished-1 (D1) representation. Therefore, it\nis desirable that the input converter from the positional number system to RNS\n(composed of a set of residue generators) could generate the residues mod 2n +\n1 in D1 form. In this paper, we propose the basic architecture of the residue\ngenerator mod 2n + 1 with D1 output. It is universal, because its initial part\ncan be easily designed for an arbitrary p >= 4n, whereas its final block-the\n4-operand adder mod 2n + 1-preserves the same structure for any p. If a pair of\nconjugate moduli 2n +/- 1 belongs to the RNS moduli set, the latter\narchitecture can be easily extended to build p-input bi-residue generators mod\n2n+/-1, which not only save hardware by sharing p - 4n full-adders, but also\ngenerate the residue mod 2n + 1 directly in D1 form.\n","authors":["Stanisław J. Piestrak","Piotr Patronik"],"pdf_url":"https://arxiv.org/pdf/2505.11928v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.11901v1","updated":"2025-05-17T08:33:50Z","published":"2025-05-17T08:33:50Z","title":"Benchmarking LLMs in an Embodied Environment for Blue Team Threat\n  Hunting","summary":"  As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. In this paper, we\npresent CYBERTEAM, a benchmark designed to guide LLMs in blue teaming practice.\nCYBERTEAM constructs an embodied environment in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of embodied functions tailored to its specific\nanalytical requirements. This transforms the overall threat-hunting process\ninto a structured sequence of function-driven operations, where each node\nrepresents a discrete function and edges define the execution order. Guided by\nthis framework, LLMs are directed to perform threat-hunting tasks through\nmodular steps. Overall, CYBERTEAM integrates 30 tasks and 9 embodied functions,\nguiding LLMs through pipelined threat analysis. We evaluate leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CYBERTEAM's embodied\nfunction-calling against fundamental elicitation strategies. Our results offer\nvaluable insights into the current capabilities and limitations of LLMs in\nthreat hunting, laying the foundation for the practical adoption in real-world\ncybersecurity applications.\n","authors":["Xiaoqun Liu","Feiyang Yu","Xi Li","Guanhua Yan","Ping Yang","Zhaohan Xi"],"pdf_url":"https://arxiv.org/pdf/2505.11901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02132v3","updated":"2025-05-17T08:09:21Z","published":"2023-12-04T18:54:34Z","title":"Hot PATE: Private Aggregation of Distributions for Diverse Task","summary":"  The Private Aggregation of Teacher Ensembles (PATE) framework enables\nprivacy-preserving machine learning by aggregating responses from disjoint\nsubsets of sensitive data. Adaptations of PATE to tasks with inherent output\ndiversity such as text generation face a core tension: preserving output\ndiversity reduces teacher agreement, which in turn increases the noise required\nfor differential privacy, degrading utility. Yet suppressing diversity is\ncounterproductive, as modern large language models encapsulate knowledge in\ntheir output distributions.\n  We propose Hot PATE, a variant tailored to settings where outputs are\ndistributions. We formally define what it means to preserve diversity and\nintroduce an efficient aggregation mechanism that transfers diversity to the\nrandomized output without incurring additional privacy cost. Our method can be\nimplemented with only API access to proprietary models and serves as a drop-in\nreplacement for existing \"cold\" PATE aggregators. Empirically, Hot PATE\nachieves orders-of-magnitude improvement on in-context learning tasks.\n","authors":["Edith Cohen","Benjamin Cohen-Wang","Xin Lyu","Jelani Nelson","Tamas Sarlos","Uri Stemmer"],"pdf_url":"https://arxiv.org/pdf/2312.02132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11884v1","updated":"2025-05-17T07:26:12Z","published":"2025-05-17T07:26:12Z","title":"Facial Recognition Leveraging Generative Adversarial Networks","summary":"  Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples.\n","authors":["Zhongwen Li","Zongwei Li","Xiaoqi Li"],"pdf_url":"https://arxiv.org/pdf/2505.11884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11880v1","updated":"2025-05-17T07:15:36Z","published":"2025-05-17T07:15:36Z","title":"AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES\n  Instruction Extension for IoT Security","summary":"  The Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm essential for securing embedded systems and IoT platforms. However,\nexisting AES hardware accelerators often face limitations in performance,\nenergy efficiency, and flexibility. This paper presents AES-RV, a\nhardware-efficient RISC-V accelerator featuring low-latency AES instruction\nextensions optimized for real-time processing across all AES modes and key\nsizes. AES-RV integrates three key innovations: high-bandwidth internal buffers\nfor continuous data processing, a specialized AES unit with custom low-latency\ninstructions, and a pipelined system supported by a ping-pong memory transfer\nmechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to\n255.97 times speedup and up to 453.04 times higher energy efficiency compared\nto baseline and conventional CPU/GPU platforms. It also demonstrates superior\nthroughput and area efficiency against state-of-the-art AES accelerators,\nmaking it a strong candidate for secure and high-performance embedded systems.\n","authors":["Van Tinh Nguyen","Phuc Hung Pham","Vu Trung Duong Le","Hoai Luan Pham","Tuan Hai Vu","Thi Diem Tran"],"pdf_url":"https://arxiv.org/pdf/2505.11880v1.pdf","comment":"6 pages, 5 figures. Submitted to IEICE Electronics Express"},{"id":"http://arxiv.org/abs/2505.11837v1","updated":"2025-05-17T04:54:26Z","published":"2025-05-17T04:54:26Z","title":"On Membership Inference Attacks in Knowledge Distillation","summary":"  Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD.\n","authors":["Ziyao Cui","Minxing Zhang","Jian Pei"],"pdf_url":"https://arxiv.org/pdf/2505.11837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21042v2","updated":"2025-05-17T04:44:18Z","published":"2025-04-28T13:30:48Z","title":"What's Pulling the Strings? Evaluating Integrity and Attribution in AI\n  Training and Inference through Concept Shift","summary":"  The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.\n","authors":["Jiamin Chang","Haoyang Li","Hammond Pearce","Ruoxi Sun","Bo Li","Minhui Xue"],"pdf_url":"https://arxiv.org/pdf/2504.21042v2.pdf","comment":"Accepted to The ACM Conference on Computer and Communications\n  Security (CCS) 2025"},{"id":"http://arxiv.org/abs/2504.18812v2","updated":"2025-05-17T03:27:52Z","published":"2025-04-26T05:51:29Z","title":"SynFuzz: Leveraging Fuzzing of Netlist to Detect Synthesis Bugs","summary":"  In the evolving landscape of integrated circuit (IC) design, the increasing\ncomplexity of modern processors and intellectual property (IP) cores has\nintroduced new challenges in ensuring design correctness and security. The\nrecent advancements in hardware fuzzing techniques have shown their efficacy in\ndetecting hardware bugs and vulnerabilities at the RTL abstraction level of\nhardware. However, they suffer from several limitations, including an inability\nto address vulnerabilities introduced during synthesis and gate-level\ntransformations. These methods often fail to detect issues arising from library\nadversaries, where compromised or malicious library components can introduce\nbackdoors or unintended behaviors into the design. In this paper, we present a\nnovel hardware fuzzer, SynFuzz, designed to overcome the limitations of\nexisting hardware fuzzing frameworks. SynFuzz focuses on fuzzing hardware at\nthe gate-level netlist to identify synthesis bugs and vulnerabilities that\narise during the transition from RTL to the gate-level. We analyze the\nintrinsic hardware behaviors using coverage metrics specifically tailored for\nthe gate-level. Furthermore, SynFuzz implements differential fuzzing to uncover\nbugs associated with EDA libraries. We evaluated SynFuzz on popular open-source\nprocessors and IP designs, successfully identifying 7 new synthesis bugs.\nAdditionally, by exploiting the optimization settings of EDA tools, we\nperformed a compromised library mapping attack (CLiMA), creating a malicious\nversion of hardware designs that remains undetectable by traditional\nverification methods. We also demonstrate how SynFuzz overcomes the limitations\nof the industry-standard formal verification tool, Cadence Conformal, providing\na more robust and comprehensive approach to hardware verification.\n","authors":["Raghul Saravanan","Sudipta Paria","Aritra Dasgupta","Venkat Nitin Patnala","Swarup Bhunia","Sai Manoj P D"],"pdf_url":"https://arxiv.org/pdf/2504.18812v2.pdf","comment":"15 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.23687v2","updated":"2025-05-17T03:16:08Z","published":"2024-10-31T07:22:51Z","title":"Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey","summary":"  With the advent of Large Vision-Language Models (LVLMs), new attack vectors,\nsuch as cognitive bias, prompt injection, and jailbreaking, have emerged.\nUnderstanding these attacks promotes system robustness improvement and neural\nnetworks demystification. However, existing surveys often target attack\ntaxonomy and lack in-depth analysis like 1) unified insights into\nadversariality, transferability, and generalization; 2) detailed evaluations\nframework; 3) motivation-driven attack categorizations; and 4) an integrated\nperspective on both traditional and LVLM attacks. This article addresses these\ngaps by offering a thorough summary of traditional and LVLM adversarial\nattacks, emphasizing their connections and distinctions, and providing\nactionable insights for future research.\n","authors":["Chiyu Zhang","Lu Zhou","Xiaogang Xu","Jiafei Wu","Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08956v3","updated":"2025-05-17T01:19:58Z","published":"2024-07-12T03:18:38Z","title":"Defending Code Language Models against Backdoor Attacks with Deceptive\n  Cross-Entropy Loss","summary":"  Code Language Models (CLMs), particularly those leveraging deep learning,\nhave achieved significant success in code intelligence domain. However, the\nissue of security, particularly backdoor attacks, is often overlooked in this\nprocess. The previous research has focused on designing backdoor attacks for\nCLMs, but effective defenses have not been adequately addressed. In particular,\nexisting defense methods from natural language processing, when directly\napplied to CLMs, are not effective enough and lack generality, working well in\nsome models and scenarios but failing in others, thus fall short in\nconsistently mitigating backdoor attacks. To bridge this gap, we first confirm\nthe phenomenon of \"early learning\" as a general occurrence during the training\nof CLMs. This phenomenon refers to that a model initially focuses on the main\nfeatures of training data but may become more sensitive to backdoor triggers\nover time, leading to overfitting and susceptibility to backdoor attacks. We\nthen analyze that overfitting to backdoor triggers results from the use of the\ncross-entropy loss function, where the unboundedness of cross-entropy leads the\nmodel to increasingly concentrate on the features of the poisoned data. Based\non this insight, we propose a general and effective loss function DeCE\n(Deceptive Cross-Entropy) by blending deceptive distributions and applying\nlabel smoothing to limit the gradient to bounded, which prevents the model from\noverfitting to backdoor triggers and then enhances the security of CLMs against\nbackdoor attacks.\n","authors":["Guang Yang","Yu Zhou","Xiang Chen","Xiangyu Zhang","Terry Yue Zhuo","David Lo","Taolue Chen"],"pdf_url":"https://arxiv.org/pdf/2407.08956v3.pdf","comment":"TOSEM"},{"id":"http://arxiv.org/abs/2310.04055v5","updated":"2025-05-17T00:30:15Z","published":"2023-10-06T07:09:05Z","title":"Kick Bad Guys Out! Conditionally Activated Anomaly Detection in\n  Federated Learning with Zero-Knowledge Proof Verification","summary":"  Federated Learning (FL) systems are vulnerable to adversarial attacks, such\nas model poisoning and backdoor attacks. However, existing defense mechanisms\noften fall short in real-world settings due to key limitations: they may rely\non impractical assumptions, introduce distortions by modifying aggregation\nfunctions, or degrade model performance even in benign scenarios. To address\nthese issues, we propose a novel anomaly detection method designed specifically\nfor practical FL scenarios. Our approach employs a two-stage, conditionally\nactivated detection mechanism: cross-round check first detects whether\nsuspicious activity has occurred, and, if warranted, a cross-client check\nfilters out malicious participants. This mechanism preserves utility while\navoiding unrealistic assumptions. Moreover, to ensure the transparency and\nintegrity of the defense mechanism, we incorporate zero-knowledge proofs,\nenabling clients to verify the detection without relying solely on the server's\ngoodwill. To the best of our knowledge, this is the first method to bridge the\ngap between theoretical advances in FL security and the demands of real-world\ndeployment. Extensive experiments across diverse tasks and real-world edge\ndevices demonstrate the effectiveness of our method over state-of-the-art\ndefenses.\n","authors":["Shanshan Han","Wenxuan Wu","Baturalp Buyukates","Weizhao Jin","Qifan Zhang","Yuhang Yao","Salman Avestimehr","Chaoyang He"],"pdf_url":"https://arxiv.org/pdf/2310.04055v5.pdf","comment":null}]},"2025-05-21T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2505.14162v2","updated":"2025-05-21T07:54:58Z","published":"2025-05-20T10:16:06Z","title":"Versatile Quantum-Safe Hybrid Key Exchange and Its Application to MACsec","summary":"  Advancements in quantum computing pose a significant threat to most of the\ncryptography currently deployed. Fortunately, cryptographic building blocks to\nmitigate the threat are already available; mostly based on post-quantum and\nquantum cryptography, but also on symmetric cryptography techniques. Notably,\nquantum-safe building blocks must be deployed as soon as possible due to the\n``harvest-now decrypt-later'' attack scenario, which is already challenging our\nsensitive and encrypted data today.\n  Following an agile defense-in-depth approach, Hybrid Authenticated Key\nExchange (HAKE) protocols have recently been gaining significant attention.\nSuch protocols modularly combine conventional, post-quantum, and quantum\ncryptography to achieve confidentiality, authenticity, and integrity guarantees\nfor network channels. Unfortunately, only a few protocols have yet been\nproposed (mainly Muckle and Muckle+) with different flexibility guarantees.\n  Looking at available standards in the network domain (especially at the Media\nAccess Control Security (MACsec) standard), we believe that HAKE protocols\ncould already bring strong security benefits to MACsec today. MACsec is a\nstandard designed to secure communication at the data link layer in Ethernet\nnetworks by providing security for all traffic between adjacent entities. In\naddition, MACsec establishes secure channels within a Local Area Network (LAN),\nensuring that data remain protected from eavesdropping, tampering, and\nunauthorized access, while operating transparently to higher layer protocols.\nCurrently, MACsec does not offer enough protection in the event of\ncryptographically relevant quantum computers.\n  In this work, we tackle the challenge and propose a new versatile HAKE\nprotocol, dubbed VMuckle, which is sufficiently flexible for the use in MACsec\nto provide LAN participants with hybrid key material ensuring secure\ncommunication.\n","authors":["Jaime S. Buruaga","Augustine Bugler","Juan P. Brito","Vicente Martin","Christoph Striecks"],"pdf_url":"https://arxiv.org/pdf/2505.14162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14103v2","updated":"2025-05-21T03:36:20Z","published":"2025-05-20T09:10:45Z","title":"AudioJailbreak: Jailbreak Attacks against End-to-End Large\n  Audio-Language Models","summary":"  Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.\n","authors":["Guangke Chen","Fu Song","Zhe Zhao","Xiaojun Jia","Yang Liu","Yanchen Qiao","Weizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12640v2","updated":"2025-05-21T07:24:34Z","published":"2025-05-19T02:47:44Z","title":"GDPRShield: AI-Powered GDPR Support for Software Developers in Small and\n  Medium-Sized Enterprises","summary":"  With the rapid increase in privacy violations in modern software development,\nregulatory frameworks such as the General Data Protection Regulation (GDPR)\nhave been established to enforce strict data protection practices. However,\ninsufficient privacy awareness among SME software developers contributes to\nfailure in GDPR compliance. For instance, a developer unfamiliar with data\nminimization may build a system that collects excessive data, violating GDPR\nand risking fines. One reason for this lack of awareness is that developers in\nSMEs often take on multidisciplinary roles (e.g., front-end, back-end, database\nmanagement, and privacy compliance), which limits specialization in privacy.\nThis lack of awareness may lead to poor privacy attitudes, ultimately hindering\nthe development of a strong organizational privacy culture. However, SMEs that\nachieve GDPR compliance may gain competitive advantages, such as increased user\ntrust and marketing value, compared to others that do not.\n  Therefore, in this paper, we introduce a novel AI-powered framework called\n\"GDPRShield,\" specifically designed to enhance the GDPR awareness of SME\nsoftware developers and, through this, improve their privacy attitudes.\nSimultaneously, GDPRShield boosts developers' motivation to comply with GDPR\nfrom the early stages of software development. It leverages functional\nrequirements written as user stories to provide comprehensive GDPR-based\nprivacy descriptions tailored to each requirement. Alongside improving\nawareness, GDPRShield strengthens motivation by presenting real-world\nconsequences of noncompliance, such as heavy fines, reputational damage, and\nloss of user trust, aligned with each requirement. This dual focus on awareness\nand motivation leads developers to engage with GDPRShield, improving their GDPR\ncompliance and privacy attitudes, which will help SMEs build a stronger privacy\nculture over time.\n","authors":["Tharaka Wijesundara","Mathew Warren","Nalin Arachchilage"],"pdf_url":"https://arxiv.org/pdf/2505.12640v2.pdf","comment":"10 pages (This work has been submitted to the euroUSEC for possible\n  publication.)"},{"id":"http://arxiv.org/abs/2408.13247v2","updated":"2025-05-21T17:58:04Z","published":"2024-08-23T17:42:06Z","title":"An In-Depth Investigation of Data Collection in LLM App Ecosystems","summary":"  LLM app (tool) ecosystems are rapidly evolving to support sophisticated use\ncases that often require extensive user data collection. Given that LLM apps\nare developed by third parties and anecdotal evidence indicating inconsistent\nenforcement of policies by LLM platforms, sharing user data with these apps\npresents significant privacy risks. In this paper, we aim to bring transparency\nin data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem\nas a case study. We propose an LLM-based framework to analyze the natural\nlanguage specifications of GPT Actions (custom tools) and assess their data\ncollection practices. Our analysis reveals that Actions collect excessive data\nacross 24 categories and 145 data types, with third-party Actions collecting\n6.03% more data on average. We find that several Actions violate OpenAI's\npolicies by collecting sensitive information, such as passwords, which is\nexplicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy\nanalysis framework to automatically check the consistency of data collection by\nActions with disclosures in their privacy policies. Our measurements indicate\nthat the disclosures for most of the collected data types are omitted, with\nonly 5.8% of Actions clearly disclosing their data collection practices.\n","authors":["Yuhao Wu","Evin Jaff","Ke Yang","Ning Zhang","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2408.13247v2.pdf","comment":"Accepted by the ACM Internet Measurement Conference (IMC) 2025"},{"id":"http://arxiv.org/abs/2505.15797v1","updated":"2025-05-21T17:50:18Z","published":"2025-05-21T17:50:18Z","title":"VoteMate: A Decentralized Application for Scalable Electronic Voting on\n  EVM-Based Blockchain","summary":"  Voting is a cornerstone of democracy, allowing citizens to express their will\nand make collective decisions. With advancing technology, online voting is\ngaining popularity as it enables voting from anywhere with Internet access,\neliminating the need for printed ballots or polling stations. However, despite\nits benefits, online voting carries significant risks. A single vulnerability\ncould be exploited to manipulate elections on a large scale. Centralized\nsystems can be secure but may lack transparency and confidentiality, especially\nif those in power manipulate them. Blockchain-based voting offers a\ntransparent, tamper-resistant alternative with end-to-end verifiability and\nstrong security. Adding cryptographic layers can also ensure voter\nconfidentiality.\n","authors":["Ivan Homoliak","Tomáš Švondr"],"pdf_url":"https://arxiv.org/pdf/2505.15797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15753v1","updated":"2025-05-21T16:58:14Z","published":"2025-05-21T16:58:14Z","title":"Scalable Defense against In-the-wild Jailbreaking Attacks with Safety\n  Context Retrieval","summary":"  Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.\n","authors":["Taiye Chen","Zeming Wei","Ang Li","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15738v1","updated":"2025-05-21T16:43:17Z","published":"2025-05-21T16:43:17Z","title":"Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses","summary":"  Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.\n","authors":["Xiaoxue Yang","Bozhidar Stevanoski","Matthieu Meeus","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2505.15738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01565v6","updated":"2025-05-21T16:29:45Z","published":"2024-11-03T13:36:34Z","title":"SQL Injection Jailbreak: A Structural Disaster of Large Language Models","summary":"  Large Language Models (LLMs) are susceptible to jailbreak attacks that can\ninduce them to generate harmful content. Previous jailbreak methods primarily\nexploited the internal properties or capabilities of LLMs, such as\noptimization-based jailbreak methods and methods that leveraged the model's\ncontext-learning abilities. In this paper, we introduce a novel jailbreak\nmethod, SQL Injection Jailbreak (SIJ), which targets the external properties of\nLLMs, specifically, the way LLMs construct input prompts. By injecting\njailbreak information into user prompts, SIJ successfully induces the model to\noutput harmful content. For open-source models, SIJ achieves near 100% attack\nsuccess rates on five well-known LLMs on the AdvBench and HEx-PHI, while\nincurring lower time costs compared to previous methods. For closed-source\nmodels, SIJ achieves an average attack success rate over 85% across five models\nin the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in\nLLMs that urgently requires mitigation. To address this, we propose a simple\nadaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate\nits effectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak.\n","authors":["Jiawei Zhao","Kejiang Chen","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2411.01565v6.pdf","comment":"Accepted by findings of ACL 2025"},{"id":"http://arxiv.org/abs/2502.12562v2","updated":"2025-05-21T15:31:15Z","published":"2025-02-18T05:57:35Z","title":"SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings","summary":"  Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.\n","authors":["Weikai Lu","Hao Peng","Huiping Zhuang","Cen Chen","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12562v2.pdf","comment":"Accepted in ACL 2025 Main Track"},{"id":"http://arxiv.org/abs/2505.15644v1","updated":"2025-05-21T15:22:45Z","published":"2025-05-21T15:22:45Z","title":"FragFake: A Dataset for Fine-Grained Detection of Edited Images with\n  Vision Language Models","summary":"  Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.\n","authors":["Zhen Sun","Ziyi Zhang","Zeren Luo","Zeyang Sha","Tianshuo Cong","Zheng Li","Shiwen Cui","Weiqiang Wang","Jiaheng Wei","Xinlei He","Qi Li","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15644v1.pdf","comment":"14pages,15 figures"},{"id":"http://arxiv.org/abs/2501.11091v5","updated":"2025-05-21T15:22:29Z","published":"2025-01-19T16:04:03Z","title":"Bitcoin: A Non-Continuous Time System","summary":"  This paper examines Bitcoin as a non-continuous time system shaped by\nprobabilistic block generation, the occurrence of forks, and the non-linear\nconfirmation of transactions. It introduces an entropy-based interpretation in\nwhich each block represents the resolution of uncertainty into an economically\nvalidated history. Bitcoin does not measure time through synchronized clocks or\ntrusted authorities; instead, it constructs time through decentralized\nconsensus. This mechanism enables permissionless coordination by ensuring\ntemporal order emerges from the progressive collapse of competing\npossibilities.\n","authors":["Bin Chen"],"pdf_url":"https://arxiv.org/pdf/2501.11091v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15568v1","updated":"2025-05-21T14:22:34Z","published":"2025-05-21T14:22:34Z","title":"Model Checking the Security of the Lightning Network","summary":"  Payment channel networks are an approach to improve the scalability of\nblockchain-based cryptocurrencies. The Lightning Network is a payment channel\nnetwork built for Bitcoin that is already used in practice. Because the\nLightning Network is used for transfer of financial value, its security in the\npresence of adversarial participants should be verified. The Lightning\nprotocol's complexity makes it hard to assess whether the protocol is secure.\nTo enable computer-aided security verification of Lightning, we formalize the\nprotocol in TLA+ and formally specify the security property that honest users\nare guaranteed to retrieve their correct balance. While model checking provides\na fully automated verification of the security property, the state space of the\nprotocol's specification is so large that model checking becomes unfeasible. We\nmake model checking the Lightning Network possible using two refinement steps\nthat we verify using proofs. In a first step, we prove that the model of time\nused in the protocol can be abstracted using ideas from the research of timed\nautomata. In a second step, we prove that it suffices to model check the\nprotocol for single payment channels and the protocol for multi-hop payments\nseparately. These refinements reduce the state space sufficiently to allow for\nmodel checking Lightning with models with payments over up to four hops and two\nconcurrent payments. These results indicate that the current specification of\nLightning is secure.\n","authors":["Matthias Grundmann","Hannes Hartenstein"],"pdf_url":"https://arxiv.org/pdf/2505.15568v1.pdf","comment":"Supersedes arXiv:2307.02342"},{"id":"http://arxiv.org/abs/2505.06520v2","updated":"2025-05-21T14:04:41Z","published":"2025-05-10T05:35:08Z","title":"PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of\n  Neural Networks","summary":"  It is often desirable to remove (a.k.a. unlearn) a specific part of the\ntraining data from a trained neural network model. A typical application\nscenario is to protect the data holder's right to be forgotten, which has been\npromoted by many recent regulation rules. Existing unlearning methods involve\ntraining alternative models with remaining data, which may be costly and\nchallenging to verify from the data holder or a thirdparty auditor's\nperspective. In this work, we provide a new angle and propose a novel\nunlearning approach by imposing carefully crafted \"patch\" on the original\nneural network to achieve targeted \"forgetting\" of the requested data to\ndelete. Specifically, inspired by the research line of neural network repair,\nwe propose to strategically seek a lightweight minimum \"patch\" for unlearning a\ngiven data point with certifiable guarantee. Furthermore, to unlearn a\nconsiderable amount of data points (or an entire class), we propose to\niteratively select a small subset of representative data points to unlearn,\nwhich achieves the effect of unlearning the whole set. Extensive experiments on\nmultiple categorical datasets demonstrates our approach's effectiveness,\nachieving measurable unlearning while preserving the model's performance and\nbeing competitive in efficiency and memory consumption compared to various\nbaseline methods.\n","authors":["Xuran Li","Jingyi Wang","Xiaohan Yuan","Peixin Zhang","Zhan Qin","Zhibo Wang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2505.06520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04964v4","updated":"2025-05-21T13:35:45Z","published":"2023-08-09T13:58:03Z","title":"ModSec-AdvLearn: Countering Adversarial SQL Injections with Robust\n  Machine Learning","summary":"  Many Web Application Firewalls (WAFs) leverage the OWASP CRS to block\nincoming malicious requests. The CRS consists of different sets of rules\ndesigned by domain experts to detect well-known web attack patterns. Both the\nset of rules and the weights used to combine them are manually defined,\nyielding four different default configurations of the CRS. In this work, we\nfocus on the detection of SQLi attacks, and show that the manual configurations\nof the CRS typically yield a suboptimal trade-off between detection and false\nalarm rates. Furthermore, we show that these configurations are not robust to\nadversarial SQLi attacks, i.e., carefully-crafted attacks that iteratively\nrefine the malicious SQLi payload by querying the target WAF to bypass\ndetection. To overcome these limitations, we propose (i) using machine learning\nto automate the selection of the set of rules to be combined along with their\nweights, i.e., customizing the CRS configuration based on the monitored web\nservices; and (ii) leveraging adversarial training to significantly improve its\nrobustness to adversarial SQLi manipulations. Our experiments, conducted using\nthe well-known open-source ModSecurity WAF equipped with the CRS rules, show\nthat our approach, named ModSec-AdvLearn, can (i) increase the detection rate\nup to 30%, while retaining negligible false alarm rates and discarding up to\n50% of the CRS rules; and (ii) improve robustness against adversarial SQLi\nattacks up to 85%, marking a significant stride toward designing more effective\nand robust WAFs. We release our open-source code at\nhttps://github.com/pralab/modsec-advlearn.\n","authors":["Giuseppe Floris","Christian Scano","Biagio Montaruli","Luca Demetrio","Andrea Valenza","Luca Compagna","Davide Ariu","Luca Piras","Davide Balzarotti","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2308.04964v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15483v1","updated":"2025-05-21T13:01:41Z","published":"2025-05-21T13:01:41Z","title":"Optimal Piecewise-based Mechanism for Collecting Bounded Numerical Data\n  under Local Differential Privacy","summary":"  Numerical data with bounded domains is a common data type in personal\ndevices, such as wearable sensors. While the collection of such data is\nessential for third-party platforms, it raises significant privacy concerns.\nLocal differential privacy (LDP) has been shown as a framework providing\nprovable individual privacy, even when the third-party platform is untrusted.\nFor numerical data with bounded domains, existing state-of-the-art LDP\nmechanisms are piecewise-based mechanisms, which are not optimal, leading to\nreduced data utility.\n  This paper investigates the optimal design of piecewise-based mechanisms to\nmaximize data utility under LDP. We demonstrate that existing piecewise-based\nmechanisms are heuristic forms of the $3$-piecewise mechanism, which is far\nfrom enough to study optimality. We generalize the $3$-piecewise mechanism to\nits most general form, i.e. $m$-piecewise mechanism with no pre-defined form of\neach piece. Under this form, we derive the closed-form optimal mechanism by\ncombining analytical proofs and off-the-shelf optimization solvers. Next, we\nextend the generalized piecewise-based mechanism to the circular domain (along\nwith the classical domain), defined on a cyclic range where the distance\nbetween the two endpoints is zero. By incorporating this property, we design\nthe optimal mechanism for the circular domain, achieving significantly improved\ndata utility compared with existing mechanisms.\n  Our proposed mechanisms guarantee optimal data utility under LDP among all\ngeneralized piecewise-based mechanisms. We show that they also achieve optimal\ndata utility in two common applications of LDP: distribution estimation and\nmean estimation. Theoretical analyses and experimental evaluations prove and\nvalidate the data utility advantages of our proposed mechanisms.\n","authors":["Ye Zheng","Sumita Mishra","Yidan Hu"],"pdf_url":"https://arxiv.org/pdf/2505.15483v1.pdf","comment":"Accepted by PET'25 (Issue 4)"},{"id":"http://arxiv.org/abs/2505.15476v1","updated":"2025-05-21T12:50:25Z","published":"2025-05-21T12:50:25Z","title":"Pura: An Efficient Privacy-Preserving Solution for Face Recognition","summary":"  Face recognition is an effective technology for identifying a target person\nby facial images. However, sensitive facial images raises privacy concerns.\nAlthough privacy-preserving face recognition is one of potential solutions,\nthis solution neither fully addresses the privacy concerns nor is efficient\nenough. To this end, we propose an efficient privacy-preserving solution for\nface recognition, named Pura, which sufficiently protects facial privacy and\nsupports face recognition over encrypted data efficiently. Specifically, we\npropose a privacy-preserving and non-interactive architecture for face\nrecognition through the threshold Paillier cryptosystem. Additionally, we\ncarefully design a suite of underlying secure computing protocols to enable\nefficient operations of face recognition over encrypted data directly.\nFurthermore, we introduce a parallel computing mechanism to enhance the\nperformance of the proposed secure computing protocols. Privacy analysis\ndemonstrates that Pura fully safeguards personal facial privacy. Experimental\nevaluations demonstrate that Pura achieves recognition speeds up to 16 times\nfaster than the state-of-the-art.\n","authors":["Guotao Xu","Bowen Zhao","Yang Xiao","Yantao Zhong","Liang Zhai","Qingqi Pei"],"pdf_url":"https://arxiv.org/pdf/2505.15476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15420v1","updated":"2025-05-21T12:04:42Z","published":"2025-05-21T12:04:42Z","title":"Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems\n  through Benign Queries","summary":"  Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems.\n","authors":["Yuhao Wang","Wenjie Qu","Yanze Jiang","Zichen Liu","Yue Liu","Shengfang Zhai","Yinpeng Dong","Jiaheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12536v3","updated":"2025-05-21T11:35:06Z","published":"2024-07-17T13:18:16Z","title":"An Efficient TLS 1.3 Handshake Protocol with VC Certificate Type","summary":"  The paper presents a step forward in the design and implementation of a\nTransport Layer Security (TLS) handshake protocol that enables the use of\nVerifiable Credential (VC) while maintaining full compliance with RFC-8446 and\npreserving all the security features of TLS 1.3. The improvement over our\nprevious work lies in the handshake design, which now only uses messages\nalready defined for TLS 1.3. The design has an incredibly positive impact on\nthe implementation, as we made minimal changes to the OpenSSL library and\nrelied mostly on a novel external provider to handle VC and Decentralized\nIDentifier (DID) related operations. The experimental results prove the\nfeasibility of the design and show comparable performance to the original\nsolution based on Public Key Infrastructure (PKI) and X.509 certificates. These\nresults pave the way for the adoption of Self-Sovereign Identity in large-scale\nInternet of Things (IoT) systems, with a clear benefit in terms of reducing the\ncost of identity management.\n","authors":["Leonardo Perugini","Andrea Vesco"],"pdf_url":"https://arxiv.org/pdf/2407.12536v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.00386"},{"id":"http://arxiv.org/abs/2505.15389v1","updated":"2025-05-21T11:26:40Z","published":"2025-05-21T11:26:40Z","title":"Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study","summary":"  Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.\n","authors":["DongGeon Lee","Joonwon Jang","Jihae Jeong","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2505.15389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15383v1","updated":"2025-05-21T11:21:33Z","published":"2025-05-21T11:21:33Z","title":"Real-Time Detection of Insider Threats Using Behavioral Analytics and\n  Deep Evidential Clustering","summary":"  Insider threats represent one of the most critical challenges in modern\ncybersecurity. These threats arise from individuals within an organization who\nmisuse their legitimate access to harm the organization's assets, data, or\noperations. Traditional security mechanisms, primarily designed for external\nattackers, fall short in identifying these subtle and context-aware threats. In\nthis paper, we propose a novel framework for real-time detection of insider\nthreats using behavioral analytics combined with deep evidential clustering.\nOur system captures and analyzes user activities, applies context-rich\nbehavioral features, and classifies potential threats using a deep evidential\nclustering model that estimates both cluster assignment and epistemic\nuncertainty. The proposed model dynamically adapts to behavioral changes and\nsignificantly reduces false positives. We evaluate our framework on benchmark\ninsider threat datasets such as CERT and TWOS, achieving an average detection\naccuracy of 94.7% and a 38% reduction in false positives compared to\ntraditional clustering methods. Our results demonstrate the effectiveness of\nintegrating uncertainty modeling in threat detection pipelines. This research\nprovides actionable insights for deploying intelligent, adaptive, and robust\ninsider threat detection systems across various enterprise environments.\n","authors":["Anas Ali","Mubashar Husain","Peter Hans"],"pdf_url":"https://arxiv.org/pdf/2505.15383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15376v1","updated":"2025-05-21T11:11:44Z","published":"2025-05-21T11:11:44Z","title":"Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving\n  Intrusion Detection in Industrial IoT","summary":"  Industrial Internet of Things (IIoT) systems have become integral to smart\nmanufacturing, yet their growing connectivity has also exposed them to\nsignificant cybersecurity threats. Traditional intrusion detection systems\n(IDS) often rely on centralized architectures that raise concerns over data\nprivacy, latency, and single points of failure. In this work, we propose a\nnovel Federated Learning-Enhanced Blockchain Framework (FL-BCID) for\nprivacy-preserving intrusion detection tailored for IIoT environments. Our\narchitecture combines federated learning (FL) to ensure decentralized model\ntraining with blockchain technology to guarantee data integrity, trust, and\ntamper resistance across IIoT nodes. We design a lightweight intrusion\ndetection model collaboratively trained using FL across edge devices without\nexposing sensitive data. A smart contract-enabled blockchain system records\nmodel updates and anomaly scores to establish accountability. Experimental\nevaluations using the ToN-IoT and N-BaIoT datasets demonstrate the superior\nperformance of our framework, achieving 97.3% accuracy while reducing\ncommunication overhead by 41% compared to baseline centralized methods. Our\napproach ensures privacy, scalability, and robustness-critical for secure\nindustrial operations. The proposed FL-BCID system provides a promising\nsolution for enhancing trust and privacy in modern IIoT security architectures.\n","authors":["Anas Ali","Mubashar Husain","Peter Hans"],"pdf_url":"https://arxiv.org/pdf/2505.15376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08728v2","updated":"2025-05-21T09:45:04Z","published":"2025-05-13T16:39:00Z","title":"Securing RAG: A Risk Assessment and Mitigation Framework","summary":"  Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.\n","authors":["Lukas Ammann","Sara Ott","Christoph R. Landolt","Marco P. Lehmann"],"pdf_url":"https://arxiv.org/pdf/2505.08728v2.pdf","comment":"8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally.\n  This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2505.15265v1","updated":"2025-05-21T08:45:43Z","published":"2025-05-21T08:45:43Z","title":"Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic\n  Concepts for LVLMs","summary":"  Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.\n","authors":["Zihao Pan","Yu Tong","Weibin Wu","Jingyi Wang","Lifeng Chen","Zhe Zhao","Jiajia Wei","Yitong Qiao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.15265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15252v1","updated":"2025-05-21T08:28:56Z","published":"2025-05-21T08:28:56Z","title":"An Efficient Private GPT Never Autoregressively Decodes","summary":"  The wide deployment of the generative pre-trained transformer (GPT) has\nraised privacy concerns for both clients and servers. While cryptographic\nprimitives can be employed for secure GPT inference to protect the privacy of\nboth parties, they introduce considerable performance overhead.To accelerate\nsecure inference, this study proposes a public decoding and secure verification\napproach that utilizes public GPT models, motivated by the observation that\nsecurely decoding one and multiple tokens takes a similar latency. The client\nuses the public model to generate a set of tokens, which are then securely\nverified by the private model for acceptance. The efficiency of our approach\ndepends on the acceptance ratio of tokens proposed by the public model, which\nwe improve from two aspects: (1) a private sampling protocol optimized for\ncryptographic primitives and (2) model alignment using knowledge distillation.\nOur approach improves the efficiency of secure decoding while maintaining the\nsame level of privacy and generation quality as standard secure decoding.\nExperiments demonstrate a $2.1\\times \\sim 6.0\\times$ speedup compared to\nstandard decoding across three pairs of public-private models and different\nnetwork conditions.\n","authors":["Zhengyi Li","Yue Guan","Kang Yang","Yu Feng","Ning Liu","Yu Yu","Jingwen Leng","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2505.15252v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2503.18173v3","updated":"2025-05-21T08:19:44Z","published":"2025-03-23T19:06:25Z","title":"A Systematic Literature Review of Cyber Security Monitoring in Maritime","summary":"  In recent years, many cyber incidents have occurred in the maritime sector,\ntargeting the information technology (IT) and operational technology (OT)\ninfrastructure. One of the key approaches for handling cyber incidents is cyber\nsecurity monitoring, which aims at timely detection of cyber attacks with\nautomated methods. Although several literature review papers have been\npublished in the field of maritime cyber security, none of the previous studies\nhas focused on cyber security monitoring. The current paper addresses this\nresearch gap and surveys the methods, algorithms, tools and architectures used\nfor cyber security monitoring in the maritime sector. For the survey, a\nsystematic literature review of cyber security monitoring studies is conducted\nfollowing the Preferred Reporting Items for Systematic reviews and\nMeta-Analyses (PRISMA) protocol. The first contribution of this paper is the\nbibliometric analysis of related literature and the identification of the main\nresearch themes in previous works. For that purpose, the paper presents a\ntaxonomy for existing studies which highlights the main properties of maritime\ncyber security monitoring research. The second contribution of this paper is an\nin-depth analysis of previous works and the identification of research gaps and\nlimitations in existing literature. The gaps and limitations include several\ndataset and evaluation issues and a number of understudied research topics.\nBased on these findings, the paper outlines future research directions for\ncyber security monitoring in the maritime field.\n","authors":["Risto Vaarandi","Leonidas Tsiopoulos","Gabor Visky","Muaan Ur Rehman","Hayretdin Bahsi"],"pdf_url":"https://arxiv.org/pdf/2503.18173v3.pdf","comment":"Accepted for publication in IEEE Access"},{"id":"http://arxiv.org/abs/2505.15216v1","updated":"2025-05-21T07:44:52Z","published":"2025-05-21T07:44:52Z","title":"BountyBench: Dollar Impact of AI Agent Attackers and Defenders on\n  Real-World Cybersecurity Systems","summary":"  AI agents have the potential to significantly alter the cybersecurity\nlandscape. To help us understand this change, we introduce the first framework\nto capture offensive and defensive cyber-capabilities in evolving real-world\nsystems. Instantiating this framework with BountyBench, we set up 25 systems\nwith complex, real-world codebases. To capture the vulnerability lifecycle, we\ndefine three task types: Detect (detecting a new vulnerability), Exploit\n(exploiting a specific vulnerability), and Patch (patching a specific\nvulnerability). For Detect, we construct a new success indicator, which is\ngeneral across vulnerability types and provides localized evaluation. We\nmanually set up the environment for each system, including installing packages,\nsetting up server(s), and hydrating database(s). We add 40 bug bounties, which\nare vulnerabilities with monetary awards from \\$10 to \\$30,485, and cover 9 of\nthe OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy\nbased on information to guide detection, interpolating from identifying a zero\nday to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,\nOpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and\nClaude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing\nagents are Claude Code (5% on Detect, mapping to \\$1,350), Custom Agent with\nClaude 3.7 Sonnet Thinking (5% on Detect, mapping to \\$1,025; 67.5% on\nExploit), and OpenAI Codex CLI (5% on Detect, mapping to \\$2,400; 90% on Patch,\nmapping to \\$14,422). OpenAI Codex CLI and Claude Code are more capable at\ndefense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit\nscores of 32.5% and 57.5% respectively; in contrast, the custom agents are\nrelatively balanced between offense and defense, achieving Exploit scores of\n40-67.5% and Patch scores of 45-60%.\n","authors":["Andy K. Zhang","Joey Ji","Celeste Menders","Riya Dulepet","Thomas Qin","Ron Y. Wang","Junrong Wu","Kyleen Liao","Jiliang Li","Jinghan Hu","Sara Hong","Nardos Demilew","Shivatmica Murgai","Jason Tran","Nishka Kacheria","Ethan Ho","Denis Liu","Lauren McLane","Olivia Bruvik","Dai-Rong Han","Seungwoo Kim","Akhil Vyas","Cuiyuanxiu Chen","Ryan Li","Weiran Xu","Jonathan Z. Ye","Prerit Choudhary","Siddharth M. Bhatia","Vikram Sivashankar","Yuxuan Bao","Dawn Song","Dan Boneh","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2505.15216v1.pdf","comment":"78 pages"},{"id":"http://arxiv.org/abs/2505.04046v2","updated":"2025-05-21T07:05:11Z","published":"2025-05-07T01:12:00Z","title":"Reliable Disentanglement Multi-view Learning Against View Adversarial\n  Attacks","summary":"  Trustworthy multi-view learning has attracted extensive attention because\nevidence learning can provide reliable uncertainty estimation to enhance the\ncredibility of multi-view predictions. Existing trusted multi-view learning\nmethods implicitly assume that multi-view data is secure. However, in\nsafety-sensitive applications such as autonomous driving and security\nmonitoring, multi-view data often faces threats from adversarial perturbations,\nthereby deceiving or disrupting multi-view models. This inevitably leads to the\nadversarial unreliability problem (AUP) in trusted multi-view learning. To\novercome this tricky problem, we propose a novel multi-view learning framework,\nnamely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we\nfirst propose evidential disentanglement learning to decompose each view into\nclean and adversarial parts under the guidance of corresponding evidences,\nwhich is extracted by a pretrained evidence extractor. Then, we employ the\nfeature recalibration module to mitigate the negative impact of adversarial\nperturbations and extract potential informative features from them. Finally, to\nfurther ignore the irreparable adversarial interferences, a view-level\nevidential attention mechanism is designed. Extensive experiments on multi-view\nclassification tasks with adversarial attacks show that RDML outperforms the\nstate-of-the-art methods by a relatively large margin. Our code is available at\nhttps://github.com/Willy1005/2025-IJCAI-RDML.\n","authors":["Xuyang Wang","Siyuan Duan","Qizhi Li","Guiduo Duan","Yuan Sun","Dezhong Peng"],"pdf_url":"https://arxiv.org/pdf/2505.04046v2.pdf","comment":"11 pages, 11 figures, accepted by IJCAI 2025"},{"id":"http://arxiv.org/abs/2409.14052v3","updated":"2025-05-21T07:04:11Z","published":"2024-09-21T07:51:12Z","title":"An average case efficient algorithm for solving two-variable linear\n  Diophantine equations","summary":"  Solving two-variable linear Diophantine equations has application in many\ncryptographic protocols such as RSA and Elliptic curve cryptography. The\nExtended Euclid's algorithm is the most widely used algorithm to solve these\nequations. We revisit two algorithms to solve two-variable linear Diophantine\nequations. We write the iterative version of one of the revisited algorithms.\nFor another, we do a fine-grained analysis of the number of recursive calls and\narrive at a periodic function that represents the number of recursive calls. We\nfind the period and use it to derive an accurate closed-form expression for the\naverage number of recursive calls incurred by that algorithm. We find multiple\nloose upper bounds on the average number of recursive calls in different cases\nbased on whether a solution exists or not. If for a fixed value of $a,b$ and a\nvarying $c$, an equation $ax+by=c$ (where $a>b$) is solvable, then we can find\nthe solution in $O\\left(\\frac{\\log b}{gcd(a,b)}\\right)$ average number of\nrecursion or steps. We computationally evaluate this bound as well as one more\nupper bound and compare them with the average number of recursive calls in\nExtended Euclid's algorithm on a number of random $ n$-bit inputs. We observe\nthat the average number of iterations in the analyzed algorithm decreases with\nan increase in $gcd(a,b)$. We propose an iterative version of the algorithm. We\nimplement this algorithm and find that the average number of iterations by our\nalgorithm is less than that of two existing algorithms.\n","authors":["Mayank Deora","Pinakpani Pal"],"pdf_url":"https://arxiv.org/pdf/2409.14052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15156v1","updated":"2025-05-21T06:21:21Z","published":"2025-05-21T06:21:21Z","title":"Privacy-Preserving Socialized Recommendation based on Multi-View\n  Clustering in a Cloud Environment","summary":"  Recommendation as a service has improved the quality of our lives and plays a\nsignificant role in variant aspects. However, the preference of users may\nreveal some sensitive information, so that the protection of privacy is\nrequired. In this paper, we propose a privacy-preserving, socialized,\nrecommendation protocol that introduces information collected from online\nsocial networks to enhance the quality of the recommendation. The proposed\nscheme can calculate the similarity between users to determine their potential\nrelationships and interests, and it also can protect the users' privacy from\nleaking to an untrusted third party. The security analysis and experimental\nresults showed that our proposed scheme provides excellent performance and is\nfeasible for real-world applications.\n","authors":["Cheng Guo","Jing Jia","Peng Wang","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15156v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.15148v1","updated":"2025-05-21T06:03:39Z","published":"2025-05-21T06:03:39Z","title":"Dynamic Spectrum Sharing Based on the Rentable NFT Standard ERC4907","summary":"  Centralized Dynamic Spectrum Sharing (DSS) faces challenges like data\nsecurity, high management costs, and limited scalability. To address these\nissues, a blockchain-based DSS scheme has been proposed in this paper. First,\nwe utilize the ERC4907 standard to mint Non-Fungible Spectrum Tokens (NFSTs)\nthat serve as unique identifiers for spectrum resources and facilitate renting.\nNext, we develop a smart contract for NFST auctions, ensuring secure spectrum\ntransactions through the auction process. Lastly, we create a Web3 spectrum\nauction platform where users can access idle spectrum data and participate in\nauctions for NFST leases corresponding to the available spectrum. Experimental\nresults demonstrate that our NFST, designed according to the ERC4907 standard,\neffectively meets users' secure and efficient DSS requirements, making it a\nfeasible solution.\n","authors":["Litao Ye","Bin Chen","Shrivastava Shivanshu","Chen Sun","Shuo Wang","Siming Feng","Shengli Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15136v1","updated":"2025-05-21T05:43:41Z","published":"2025-05-21T05:43:41Z","title":"Hybrid Audio Detection Using Fine-Tuned Audio Spectrogram Transformers:\n  A Dataset-Driven Evaluation of Mixed AI-Human Speech","summary":"  The rapid advancement of artificial intelligence (AI) has enabled\nsophisticated audio generation and voice cloning technologies, posing\nsignificant security risks for applications reliant on voice authentication.\nWhile existing datasets and models primarily focus on distinguishing between\nhuman and fully synthetic speech, real-world attacks often involve audio that\ncombines both genuine and cloned segments. To address this gap, we construct a\nnovel hybrid audio dataset incorporating human, AI-generated, cloned, and mixed\naudio samples. We further propose fine-tuned Audio Spectrogram Transformer\n(AST)-based models tailored for detecting these complex acoustic patterns.\nExtensive experiments demonstrate that our approach significantly outperforms\nexisting baselines in mixed-audio detection, achieving 97\\% classification\naccuracy. Our findings highlight the importance of hybrid datasets and tailored\nmodels in advancing the robustness of speech-based authentication systems.\n","authors":["Kunyang Huang","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2505.15136v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.05849v2","updated":"2025-05-21T05:34:08Z","published":"2025-05-09T07:40:17Z","title":"AGENTFUZZER: Generic Black-Box Fuzzing for Indirect Prompt Injection\n  against LLM Agents","summary":"  The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites.\n","authors":["Zhun Wang","Vincent Siu","Zhe Ye","Tianneng Shi","Yuzhou Nie","Xuandong Zhao","Chenguang Wang","Wenbo Guo","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2505.05849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15124v1","updated":"2025-05-21T05:19:45Z","published":"2025-05-21T05:19:45Z","title":"A Survey On Secure Machine Learning","summary":"  In this survey, we will explore the interaction between secure multiparty\ncomputation and the area of machine learning. Recent advances in secure\nmultiparty computation (MPC) have significantly improved its applicability in\nthe realm of machine learning (ML), offering robust solutions for\nprivacy-preserving collaborative learning. This review explores key\ncontributions that leverage MPC to enable multiple parties to engage in ML\ntasks without compromising the privacy of their data. The integration of MPC\nwith ML frameworks facilitates the training and evaluation of models on\ncombined datasets from various sources, ensuring that sensitive information\nremains encrypted throughout the process. Innovations such as specialized\nsoftware frameworks and domain-specific languages streamline the adoption of\nMPC in ML, optimizing performance and broadening its usage. These frameworks\naddress both semi-honest and malicious threat models, incorporating features\nsuch as automated optimizations and cryptographic auditing to ensure compliance\nand data integrity. The collective insights from these studies highlight MPC's\npotential in fostering collaborative yet confidential data analysis, marking a\nsignificant stride towards the realization of secure and efficient\ncomputational solutions in privacy-sensitive industries. This paper\ninvestigates a spectrum of SecureML libraries that includes cryptographic\nprotocols, federated learning frameworks, and privacy-preserving algorithms. By\nsurveying the existing literature, this paper aims to examine the efficacy of\nthese libraries in preserving data privacy, ensuring model confidentiality, and\nfortifying ML systems against adversarial attacks. Additionally, the study\nexplores an innovative application domain for SecureML techniques: the\nintegration of these methodologies in gaming environments utilizing ML.\n","authors":["Taobo Liao","Taoran Li","Prathamesh Nadkarni"],"pdf_url":"https://arxiv.org/pdf/2505.15124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10619v4","updated":"2025-05-21T05:06:50Z","published":"2025-03-13T17:57:32Z","title":"Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models\n  with Tree Search","summary":"  We introduce Tempest, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Tempest expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Tempest reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Tempest achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models.\n","authors":["Andy Zhou","Ron Arel"],"pdf_url":"https://arxiv.org/pdf/2503.10619v4.pdf","comment":"Accepted to ACL 2025 Main"},{"id":"http://arxiv.org/abs/2410.02440v2","updated":"2025-05-21T04:37:27Z","published":"2024-10-03T12:37:39Z","title":"Optimizing Adaptive Attacks against Watermarks for Language Models","summary":"  Large Language Models (LLMs) can be misused to spread unwanted content at\nscale. Content watermarking deters misuse by hiding messages in content,\nenabling its detection using a secret watermarking key. Robustness is a core\nsecurity property, stating that evading detection requires (significant)\ndegradation of the content's quality. Many LLM watermarking methods have been\nproposed, but robustness is tested only against non-adaptive attackers who lack\nknowledge of the watermarking method and can find only suboptimal attacks. We\nformulate watermark robustness as an objective function and use\npreference-based optimization to tune adaptive attacks against the specific\nwatermarking method. Our evaluation shows that (i) adaptive attacks evade\ndetection against all surveyed watermarks, (ii) training against any watermark\nsucceeds in evading unseen watermarks, and (iii) optimization-based attacks are\ncost-effective. Our findings underscore the need to test robustness against\nadaptively tuned attacks. We release our adaptively optimized paraphrasers at\nhttps://github.com/nilslukas/ada-wm-evasion.\n","authors":["Abdulrahman Diaa","Toluwani Aremu","Nils Lukas"],"pdf_url":"https://arxiv.org/pdf/2410.02440v2.pdf","comment":"To appear at the International Conference on Machine Learning\n  (ICML'25)"},{"id":"http://arxiv.org/abs/2409.14781v6","updated":"2025-05-21T03:41:29Z","published":"2024-09-23T07:55:35Z","title":"Pretraining Data Detection for Large Language Models: A Divergence-based\n  Calibration Method","summary":"  As the scale of training corpora for large language models (LLMs) grows,\nmodel developers become increasingly reluctant to disclose details on their\ndata. This lack of transparency poses challenges to scientific evaluation and\nethical deployment. Recently, pretraining data detection approaches, which\ninfer whether a given text was part of an LLM's training data through black-box\naccess, have been explored. The Min-K\\% Prob method, which has achieved\nstate-of-the-art results, assumes that a non-training example tends to contain\na few outlier words with low token probabilities. However, the effectiveness\nmay be limited as it tends to misclassify non-training texts that contain many\ncommon words with high probabilities predicted by LLMs. To address this issue,\nwe introduce a divergence-based calibration method, inspired by the\ndivergence-from-randomness concept, to calibrate token probabilities for\npretraining data detection. We compute the cross-entropy (i.e., the divergence)\nbetween the token probability distribution and the token frequency distribution\nto derive a detection score. We have developed a Chinese-language benchmark,\nPatentMIA, to assess the performance of detection approaches for LLMs on\nChinese text. Experimental results on English-language benchmarks and PatentMIA\ndemonstrate that our proposed method significantly outperforms existing\nmethods. Our code and PatentMIA benchmark are available at\nhttps://github.com/zhang-wei-chao/DC-PDD.\n","authors":["Weichao Zhang","Ruqing Zhang","Jiafeng Guo","Maarten de Rijke","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2409.14781v6.pdf","comment":"Accepted by EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2505.15051v1","updated":"2025-05-21T03:16:07Z","published":"2025-05-21T03:16:07Z","title":"An Empirical Analysis of EOS Blockchain: Architecture, Contract, and\n  Security","summary":"  With the rapid development of blockchain technology, various blockchain\nsystems are exhibiting vitality and potential. As a representative of\nBlockchain 3.0, the EOS blockchain has been regarded as a strong competitor to\nEthereum. Nevertheless, compared with Bitcoin and Ethereum, academic research\nand in-depth analyses of EOS remain scarce. To address this gap, this study\nconducts a comprehensive investigation of the EOS blockchain from five key\ndimensions: system architecture, decentralization, performance, smart\ncontracts, and behavioral security. The architectural analysis focuses on six\ncore components of the EOS system, detailing their functionalities and\noperational workflows. The decentralization and performance evaluations, based\non data from the XBlock data-sharing platform, reveal several critical issues:\nlow account activity, limited participation in the supernode election process,\nminimal variation in the set of block producers, and a substantial gap between\nactual throughput and the claimed million-level performance. Five types of\ncontract vulnerabilities are identified in the smart contract dimension, and\nfour mainstream vulnerability detection platforms are introduced and\ncomparatively analyzed. In terms of behavioral security, four real-world\nattacks targeting the structural characteristics of EOS are summarized. This\nstudy contributes to the ongoing development of the EOS blockchain and provides\nvaluable insights for enhancing the security and regulatory mechanisms of\nblockchain ecosystems.\n","authors":["Haiyang Liu","Yingjie Mao","Xiaoqi Li"],"pdf_url":"https://arxiv.org/pdf/2505.15051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15017v1","updated":"2025-05-21T01:55:04Z","published":"2025-05-21T01:55:04Z","title":"PsyScam: A Benchmark for Psychological Techniques in Real-World Scams","summary":"  Online scams have become increasingly prevalent, with scammers using\npsychological techniques (PTs) to manipulate victims. While existing research\nhas developed benchmarks to study scammer behaviors, these benchmarks do not\nadequately reflect the PTs observed in real-world scams. To fill this gap, we\nintroduce PsyScam, a benchmark designed to systematically capture and evaluate\nPTs embedded in real-world scam reports. In particular, PsyScam bridges\npsychology and real-world cyber security analysis through collecting a wide\nrange of scam reports from six public platforms and grounding its annotations\nin well-established cognitive and psychological theories. We further\ndemonstrate PsyScam's utility through three downstream tasks: PT\nclassification, scam completion, and scam augmentation. Experimental results\nshow that PsyScam presents significant challenges to existing models in both\ndetecting and generating scam content based on the PTs used by real-world\nscammers. Our code and dataset are available at:\nhttps://anonymous.4open.science/r/PsyScam-66E4.\n","authors":["Shang Ma","Tianyi Ma","Jiahao Liu","Wei Song","Zhenkai Liang","Xusheng Xiao","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2505.15017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05310v2","updated":"2025-05-21T23:57:45Z","published":"2024-06-08T01:02:49Z","title":"COOKIEGUARD: Characterizing and Isolating the First-Party Cookie Jar","summary":"  As third-party cookies are being phased out or restricted by major browsers,\nfirst-party cookies are increasingly repurposed for tracking. Prior work has\nshown that third-party scripts embedded in the main frame can access and\nexfiltrate first-party cookies, including those set by other third-party\nscripts. However, existing browser security mechanisms, such as the Same-Origin\nPolicy, Content Security Policy, and third-party storage partitioning, do not\nprevent this type of cross-domain interaction within the main frame. While\nrecent studies have begun to highlight this issue, there remains a lack of\ncomprehensive measurement and practical defenses.\n  In this work, we conduct the first large-scale measurement of cross-domain\naccess to first-party cookies across 20,000 websites. We find that 56 percent\nof websites include third-party scripts that exfiltrate cookies they did not\nset, and 32 percent allow unauthorized overwriting or deletion, revealing\nsignificant confidentiality and integrity risks.\n  To mitigate this, we propose CookieGuard, a browser-based runtime enforcement\nmechanism that isolates first-party cookies on a per-script-origin basis.\nCookieGuard blocks all unauthorized cross-domain cookie operations while\npreserving site functionality in most cases, with Single Sign-On disruption\nobserved on 11 percent of sites.\n  Our results expose critical flaws in current browser models and offer a\ndeployable path toward stronger cookie isolation.\n","authors":["Pouneh Nikkhah Bahrami","Aurore Fass","Zubair Shafiq"],"pdf_url":"https://arxiv.org/pdf/2406.05310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08951v3","updated":"2025-05-21T21:21:20Z","published":"2023-10-13T08:49:25Z","title":"Unsupervised Log Anomaly Detection with Few Unique Tokens","summary":"  This article introduces a novel method for detecting anomalies within log\ndata from control system nodes at the European XFEL accelerator. Effective\nanomaly detection is crucial for providing operators with a clear understanding\nof each node's availability, status, and potential problems, thereby ensuring\nsmooth accelerator operation. Traditional and learning-based anomaly detection\nmethods face significant limitations due to the sequential nature of these logs\nand the lack of a rich, node-specific text corpus. To address this, we propose\nan approach utilizing word embeddings to represent log entries and a Hidden\nMarkov Model (HMM) to model the typical sequential patterns of these embeddings\nfor individual nodes. Anomalies are identified by scoring individual log\nentries based on a probability ratio: this ratio compares the likelihood of the\nlog sequence including the new entry against its likelihood without it,\neffectively measuring how well the new entry fits the established pattern. High\nscores indicate potential anomalies that deviate from the node's routine\nbehavior. This method functions as a warning system, alerting operators to\nirregular log events that may signify underlying issues, thereby facilitating\nproactive intervention.\n","authors":["Antonin Sulc","Annika Eichler","Tim Wilksen"],"pdf_url":"https://arxiv.org/pdf/2310.08951v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16008v1","updated":"2025-05-21T20:48:24Z","published":"2025-05-21T20:48:24Z","title":"LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language\n  Similarity-Aware Graph Optimization","summary":"  We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings.\n","authors":["Wenrui Yu","Yiyi Chen","Johannes Bjerva","Sokol Kosta","Qiongxiu Li"],"pdf_url":"https://arxiv.org/pdf/2505.16008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04564v3","updated":"2025-05-21T20:35:06Z","published":"2025-03-06T15:53:37Z","title":"Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User\n  Association","summary":"  Secure aggregation is motivated by federated learning (FL) where a cloud\nserver aims to compute an averaged model (i.e., weights of deep neural\nnetworks) of the locally-trained models of numerous clients, while adhering to\ndata security requirements. Hierarchical secure aggregation (HSA) extends this\nconcept to a three-layer network, where clustered users communicate with the\nserver through an intermediate layer of relays. In HSA, beyond conventional\nserver security, relay security is also enforced to ensure that the relays\nremain oblivious to the users' inputs (an abstraction of the local models in\nFL). Existing study on HSA assumes that each user is associated with only one\nrelay, limiting opportunities for coding across inter-cluster users to achieve\nefficient communication and key generation. In this paper, we consider HSA with\na cyclic association pattern where each user is connected to $B$ consecutive\nrelays in a wrap-around manner. We propose an efficient aggregation scheme\nwhich includes a message design for the inputs inspired by gradient coding-a\nwell-known technique for efficient communication in distributed computing-along\nwith a highly nontrivial security key design. We also derive novel converse\nbounds on the minimum achievable communication and key rates using\ninformation-theoretic arguments.\n","authors":["Xiang Zhang","Zhou Li","Kai Wan","Hua Sun","Mingyue Ji","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2503.04564v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15954v1","updated":"2025-05-21T19:11:36Z","published":"2025-05-21T19:11:36Z","title":"Integrating Robotic Navigation with Blockchain: A Novel PoS-Based\n  Approach for Heterogeneous Robotic Teams","summary":"  This work explores a novel integration of blockchain methodologies with Wide\nArea Visual Navigation (WAVN) to address challenges in visual navigation for a\nheterogeneous team of mobile robots deployed for unstructured applications in\nagriculture, forestry, etc. Focusing on overcoming challenges such as GPS\nindependence, environmental changes, and computational limitations, the study\nintroduces the Proof of Stake (PoS) mechanism, commonly used in blockchain\nsystems, into the WAVN framework \\cite{Lyons_2022}. This integration aims to\nenhance the cooperative navigation capabilities of robotic teams by\nprioritizing robot contributions based on their navigation reliability. The\nmethodology involves a stake weight function, consensus score with PoS, and a\nnavigability function, addressing the computational complexities of robotic\ncooperation and data validation. This innovative approach promises to optimize\nrobotic teamwork by leveraging blockchain principles, offering insights into\nthe scalability, efficiency, and overall system performance. The project\nanticipates significant advancements in autonomous navigation and the broader\napplication of blockchain technology beyond its traditional financial context.\n","authors":["Nasim Paykari","Ali Alfatemi","Damian M. Lyons","Mohamed Rahouti"],"pdf_url":"https://arxiv.org/pdf/2505.15954v1.pdf","comment":"4 pages, 4 figures, presented at 2024 21st International Conference\n  on Ubiquitous Robots (UR) June 24 - 27, 2024, New York University, USA"},{"id":"http://arxiv.org/abs/2505.15935v1","updated":"2025-05-21T18:42:00Z","published":"2025-05-21T18:42:00Z","title":"MAPS: A Multilingual Benchmark for Global Agent Performance and Security","summary":"  Agentic AI systems, which build on Large Language Models (LLMs) and interact\nwith tools and memory, have rapidly advanced in capability and scope. Yet,\nsince LLMs have been shown to struggle in multilingual settings, typically\nresulting in lower performance and reduced safety, agentic systems risk\ninheriting these limitations. This raises concerns about the global\naccessibility of such systems, as users interacting in languages other than\nEnglish may encounter unreliable or security-critical agent behavior. Despite\ngrowing interest in evaluating agentic AI, existing benchmarks focus\nexclusively on English, leaving multilingual settings unexplored. To address\nthis gap, we propose MAPS, a multilingual benchmark suite designed to evaluate\nagentic AI systems across diverse languages and tasks. MAPS builds on four\nwidely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code\ngeneration), MATH (mathematical reasoning), and the Agent Security Benchmark\n(security). We translate each dataset into ten diverse languages, resulting in\n805 unique tasks and 8,855 total language-specific instances. Our benchmark\nsuite enables a systematic analysis of how multilingual contexts affect agent\nperformance and robustness. Empirically, we observe consistent degradation in\nboth performance and security when transitioning from English to other\nlanguages, with severity varying by task and correlating with the amount of\ntranslated input. Building on these findings, we provide actionable\nrecommendations to guide agentic AI systems development and assessment under\nmultilingual settings. This work establishes a standardized evaluation\nframework, encouraging future research towards equitable, reliable, and\nglobally accessible agentic AI. MAPS benchmark suite is publicly available at\nhttps://huggingface.co/datasets/Fujitsu-FRE/MAPS\n","authors":["Omer Hofman","Oren Rachmil","Shamik Bose","Vikas Pahuja","Jonathan Brokman","Toshiya Shimizu","Trisha Starostina","Kelly Marchisio","Seraphina Goldfarb-Tarrant","Roman Vainshtein"],"pdf_url":"https://arxiv.org/pdf/2505.15935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15921v1","updated":"2025-05-21T18:19:24Z","published":"2025-05-21T18:19:24Z","title":"Defining Atomicity (and Integrity) for Snapshots of Storage in Forensic\n  Computing","summary":"  The acquisition of data from main memory or from hard disk storage is usually\none of the first steps in a forensic investigation. We revisit the discussion\non quality criteria for \"forensically sound\" acquisition of such storage and\npropose a new way to capture the intent to acquire an instantaneous snapshot\nfrom a single target system. The idea of our definition is to allow a certain\nflexibility into when individual portions of memory are acquired, but at the\nsame time require being consistent with causality (i.e., cause/effect\nrelations). Our concept is much stronger than the original notion of atomicity\ndefined by Vomel and Freiling (2012) but still attainable using copy-on-write\nmechanisms. As a minor result, we also fix a conceptual problem within the\noriginal definition of integrity.\n","authors":["Jenny Ottmann","Frank Breitinger","Felix Freiling"],"pdf_url":"https://arxiv.org/pdf/2505.15921v1.pdf","comment":"Proceedings of the Digital Forensics Research Conference Europe\n  (DFRWS EU), March 29-April 1, 2022"},{"id":"http://arxiv.org/abs/2505.15175v1","updated":"2025-05-21T06:45:06Z","published":"2025-05-21T06:45:06Z","title":"A Linear Approach to Data Poisoning","summary":"  We investigate the theoretical foundations of data poisoning attacks in\nmachine learning models. Our analysis reveals that the Hessian with respect to\nthe input serves as a diagnostic tool for detecting poisoning, exhibiting\nspectral signatures that characterize compromised datasets. We use random\nmatrix theory (RMT) to develop a theory for the impact of poisoning proportion\nand regularisation on attack efficacy in linear regression. Through QR stepwise\nregression, we study the spectral signatures of the Hessian in multi-output\nregression. We perform experiments on deep networks to show experimentally that\nthis theory extends to modern convolutional and transformer networks under the\ncross-entropy loss. Based on these insights we develop preliminary algorithms\nto determine if a network has been poisoned and remedies which do not require\nfurther training.\n","authors":["Diego Granziol","Donald Flynn"],"pdf_url":"https://arxiv.org/pdf/2505.15175v1.pdf","comment":"9 pages, 9 Figures"},{"id":"http://arxiv.org/abs/2505.15140v1","updated":"2025-05-21T05:54:37Z","published":"2025-05-21T05:54:37Z","title":"EC-LDA : Label Distribution Inference Attack against Federated Graph\n  Learning with Embedding Compression","summary":"  Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. However, since clients\nare required to upload model parameters to the server in each round, this\nprovides the server with an opportunity to infer each client's data privacy. In\nthis paper, we focus on label distribution attacks(LDAs) that aim to infer the\nlabel distributions of the clients' local data. We take the first step to\nattack client's label distributions in FGL. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and we propose a new attack\nnamed EC-LDA, which significantly improves the attack effectiveness by\ncompressing node embeddings. Thirdly, extensive experiments on node\nclassification and link prediction tasks across six widely used graph datasets\nshow that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal\nvalues under both Cos-sim and JS-div evaluation metrics in the CoraFull and\nLastFM datasets. Finally, we explore the robustness of EC-LDA under\ndifferential privacy protection.\n","authors":["Tong Cheng","Fu Jie","Xinpeng Ling","Huifa Li","Zhili Chen"],"pdf_url":"https://arxiv.org/pdf/2505.15140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15088v1","updated":"2025-05-21T04:14:35Z","published":"2025-05-21T04:14:35Z","title":"Leveraging Large Language Models for Command Injection Vulnerability\n  Analysis in Python: An Empirical Study on Popular Open-Source Projects","summary":"  Command injection vulnerabilities are a significant security threat in\ndynamic languages like Python, particularly in widely used open-source projects\nwhere security issues can have extensive impact. With the proven effectiveness\nof Large Language Models(LLMs) in code-related tasks, such as testing,\nresearchers have explored their potential for vulnerabilities analysis. This\nstudy evaluates the potential of large language models (LLMs), such as GPT-4,\nas an alternative approach for automated testing for vulnerability detection.\nIn particular, LLMs have demonstrated advanced contextual understanding and\nadaptability, making them promising candidates for identifying nuanced security\nvulnerabilities within code. To evaluate this potential, we applied LLM-based\nanalysis to six high-profile GitHub projects-Django, Flask, TensorFlow,\nScikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive\nadoption across software development and academic research. Our analysis\nassesses both the strengths and limitations of LLMs in detecting command\ninjection vulnerabilities, evaluating factors such as detection accuracy,\nefficiency, and practical integration into development workflows. In addition,\nwe provide a comparative analysis of different LLM tools to identify those most\nsuitable for security applications. Our findings offer guidance for developers\nand security researchers on leveraging LLMs as innovative and automated\napproaches to enhance software security.\n","authors":["Yuxuan Wang","Jingshu Chen","Qingyang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15088v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.14684v2","updated":"2025-05-21T17:02:34Z","published":"2025-05-20T17:59:31Z","title":"Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning","summary":"  Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.\n","authors":["Haolei Xu","Yuchen Yan","Yongliang Shen","Wenqi Zhang","Guiyang Hou","Shengpei Jiang","Kaitao Song","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14684v2.pdf","comment":"Project: https://zju-real.github.io/CoT-Bridge/"},{"id":"http://arxiv.org/abs/2505.14604v2","updated":"2025-05-21T16:45:44Z","published":"2025-05-20T16:53:40Z","title":"Let LLMs Break Free from Overthinking via Self-Braking Tuning","summary":"  Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.\n","authors":["Haoran Zhao","Yuchen Yan","Yongliang Shen","Haolei Xu","Wenqi Zhang","Kaitao Song","Jian Shao","Weiming Lu","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.14604v2.pdf","comment":"Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page:\n  https://ZJU-REAL.github.io/SBT"},{"id":"http://arxiv.org/abs/2505.14552v2","updated":"2025-05-21T07:43:57Z","published":"2025-05-20T16:06:32Z","title":"KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation","summary":"  Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.\n","authors":["Jiajun Shi","Jian Yang","Jiaheng Liu","Xingyuan Bu","Jiangjie Chen","Junting Zhou","Kaijing Ma","Zhoufutu Wen","Bingli Wang","Yancheng He","Liang Song","Hualei Zhu","Shilong Li","Xingjian Wang","Wei Zhang","Ruibin Yuan","Yifan Yao","Wenjun Yang","Yunli Wang","Siyuan Fang","Siyu Yuan","Qianyu He","Xiangru Tang","Yingshui Tan","Wangchunshu Zhou","Zhaoxiang Zhang","Zhoujun Li","Wenhao Huang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14552v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.14452v2","updated":"2025-05-21T16:59:56Z","published":"2025-05-20T14:51:27Z","title":"How Managers Perceive AI-Assisted Conversational Training for Workplace\n  Communication","summary":"  Effective workplace communication is essential for managerial success, yet\nmany managers lack access to tailored and sustained training. Although\nAI-assisted communication systems may offer scalable training solutions, little\nis known about how managers envision the role of AI in helping them improve\ntheir communication skills. To investigate this, we designed a conversational\nrole-play system, CommCoach, as a functional probe to understand how managers\nanticipate using AI to practice their communication skills. Through\nsemi-structured interviews, participants emphasized the value of adaptive,\nlow-risk simulations for practicing difficult workplace conversations. They\nalso highlighted opportunities, including human-AI teaming, transparent and\ncontext-aware feedback, and greater control over AI-generated personas.\nAI-assisted communication training should balance personalization, structured\nlearning objectives, and adaptability to different user styles and contexts.\nHowever, achieving this requires carefully navigating tensions between adaptive\nand consistent AI feedback, realism and potential bias, and the open-ended\nnature of AI conversations versus structured workplace discourse.\n","authors":["Lance T. Wilhelm","Xiaohan Ding","Kirk McInnis Knutsen","Buse Carik","Eugenia H. Rho"],"pdf_url":"https://arxiv.org/pdf/2505.14452v2.pdf","comment":"accepted to CUI '25"},{"id":"http://arxiv.org/abs/2505.15808v1","updated":"2025-05-21T17:59:02Z","published":"2025-05-21T17:59:02Z","title":"Neural Conditional Transport Maps","summary":"  We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability.\n","authors":["Carlos Rodriguez-Pardo","Leonardo Chiani","Emanuele Borgonovo","Massimo Tavoni"],"pdf_url":"https://arxiv.org/pdf/2505.15808v1.pdf","comment":"Under Review. Supplementary material included in the pdf"},{"id":"http://arxiv.org/abs/2502.01636v2","updated":"2025-05-21T17:58:23Z","published":"2025-02-03T18:59:14Z","title":"Lifelong Knowledge Editing requires Better Regularization","summary":"  Knowledge editing is a promising way to improve factuality in large language\nmodels, but recent studies have shown significant model degradation during\nsequential editing. In this paper, we formalize the popular locate-then-edit\nmethods as a two-step fine-tuning process, allowing us to precisely identify\nthe root cause of this degradation. We show that model degradation occurs due\nto (1) over-optimization of internal activations and (2) continuous norm-growth\nof edited matrices. To mitigate these issues, we introduce two regularization\ntechniques: (1) Most-Probable Early Stopping (MPES) and (2) explicit Frobenius\nnorm-constraint. We demonstrate that applying these simple yet effective\nregularization techniques at key points in the editing process can\nsubstantially mitigate model degradation. Combining these regularization\nmethods enables scaling locate-then-edit methods to 10,000 edits while reducing\nediting time by 42-61%. These results show that targeted regularization is\nessential for lifelong knowledge editing.\n","authors":["Akshat Gupta","Phudish Prateepamornkul","Maochuan Lu","Ahmed Alaa","Thomas Hartvigsen","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2502.01636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13247v2","updated":"2025-05-21T17:58:04Z","published":"2024-08-23T17:42:06Z","title":"An In-Depth Investigation of Data Collection in LLM App Ecosystems","summary":"  LLM app (tool) ecosystems are rapidly evolving to support sophisticated use\ncases that often require extensive user data collection. Given that LLM apps\nare developed by third parties and anecdotal evidence indicating inconsistent\nenforcement of policies by LLM platforms, sharing user data with these apps\npresents significant privacy risks. In this paper, we aim to bring transparency\nin data practices of LLM app ecosystems. We examine OpenAI's GPT app ecosystem\nas a case study. We propose an LLM-based framework to analyze the natural\nlanguage specifications of GPT Actions (custom tools) and assess their data\ncollection practices. Our analysis reveals that Actions collect excessive data\nacross 24 categories and 145 data types, with third-party Actions collecting\n6.03% more data on average. We find that several Actions violate OpenAI's\npolicies by collecting sensitive information, such as passwords, which is\nexplicitly prohibited by OpenAI. Lastly, we develop an LLM-based privacy policy\nanalysis framework to automatically check the consistency of data collection by\nActions with disclosures in their privacy policies. Our measurements indicate\nthat the disclosures for most of the collected data types are omitted, with\nonly 5.8% of Actions clearly disclosing their data collection practices.\n","authors":["Yuhao Wu","Evin Jaff","Ke Yang","Ning Zhang","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2408.13247v2.pdf","comment":"Accepted by the ACM Internet Measurement Conference (IMC) 2025"},{"id":"http://arxiv.org/abs/2505.15801v1","updated":"2025-05-21T17:54:43Z","published":"2025-05-21T17:54:43Z","title":"VerifyBench: Benchmarking Reference-based Reward Systems for Large\n  Language Models","summary":"  Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.\n","authors":["Yuchen Yan","Jin Jiang","Zhenbang Ren","Yijun Li","Xudong Cai","Yang Liu","Xin Xu","Mengdi Zhang","Jian Shao","Yongliang Shen","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2505.15801v1.pdf","comment":"Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench"},{"id":"http://arxiv.org/abs/2503.24370v3","updated":"2025-05-21T17:51:27Z","published":"2025-03-31T17:50:13Z","title":"Effectively Controlling Reasoning Models through Thinking Intervention","summary":"  Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We find that the\nThinking Intervention paradigm enhances the capabilities of reasoning models\nacross a wide range of tasks, including instruction following on IFEval and\nOverthinking, instruction hierarchy on SEP, and safety alignment on XSTest and\nSorryBench. Our results demonstrate that Thinking Intervention significantly\noutperforms baseline prompting approaches, achieving up to 6.7% accuracy gains\nin instruction-following scenarios, 15.4% improvements in reasoning about\ninstruction hierarchies, and a 40.0% increase in refusal rates for unsafe\nprompts using open-source DeepSeek R1 models. Overall, our work opens a\npromising new research avenue for controlling reasoning LLMs.\n","authors":["Tong Wu","Chong Xiang","Jiachen T. Wang","G. Edward Suh","Prateek Mittal"],"pdf_url":"https://arxiv.org/pdf/2503.24370v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01697v3","updated":"2025-05-21T17:50:43Z","published":"2025-02-03T00:12:40Z","title":"BARE: Leveraging Base Language Models for Few-Shot Synthetic Data\n  Generation","summary":"  As the demand for high-quality data in model training grows, researchers and\ndevelopers are increasingly generating synthetic data to tune and train LLMs.\nHowever, current data generation methods rely on seed sets containing tens of\nthousands of examples to prompt instruction-tuned models. This reliance can be\nespecially problematic when the curation of high-quality examples is expensive\nor difficult. In this paper we explore the novel few-shot synthetic data\ngeneration setting -- generating a high-quality dataset from a few examples. We\nshow that when working with only a few seed examples, instruction-tuned models\nused in current synthetic data methods produce insufficient diversity for\ndownstream tasks. In contrast, we show that base models without post-training,\nlargely untapped for synthetic data generation, offer substantially greater\noutput diversity, albeit with lower instruction following abilities. Leveraging\nthis insight, we propose Base-Refine (BARE), a novel two-stage method that\ncombines the diversity of base models with the quality assurance of\ninstruction-tuned models. BARE excels in few-shot synthetic data generation:\nusing only 3 seed examples it generates diverse, high-quality datasets that\nsignificantly improve downstream task performance. We show that fine-tuning\nLlama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable\nto state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore,\ndata generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2\n1B on GSM8K over data generated by only instruction-models, and an 18.4%\nimprovement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method\nfor RAG data generation.\n","authors":["Alan Zhu","Parth Asawa","Jared Quincy Davis","Lingjiao Chen","Boris Hanin","Ion Stoica","Joseph E. Gonzalez","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2502.01697v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15792v1","updated":"2025-05-21T17:46:38Z","published":"2025-05-21T17:46:38Z","title":"Long-Form Information Alignment Evaluation Beyond Atomic Facts","summary":"  Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.\n","authors":["Danna Zheng","Mirella Lapata","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2505.15792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15790v1","updated":"2025-05-21T17:43:46Z","published":"2025-05-21T17:43:46Z","title":"Exploring the Innovation Opportunities for Pre-trained Models","summary":"  Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.\n","authors":["Minjung Park","Jodi Forlizzi","John Zimmerman"],"pdf_url":"https://arxiv.org/pdf/2505.15790v1.pdf","comment":"33 pages, 20 figures, 4 tables, DIS"},{"id":"http://arxiv.org/abs/2503.10918v2","updated":"2025-05-21T17:39:17Z","published":"2025-03-13T22:13:20Z","title":"Resource Heterogeneity-Aware and Utilization-Enhanced Scheduling for\n  Deep Learning Clusters","summary":"  Scheduling deep learning (DL) models to train on powerful clusters with\naccelerators like GPUs and TPUs, presently falls short, either lacking\nfine-grained heterogeneity awareness or leaving resources substantially\nunder-utilized. To fill this gap, we propose a novel design of a task-level\nheterogeneity-aware scheduler, Hadar, based on an optimization framework that\ncan boost resource utilization. Hadar leverages the performance traits of DL\njobs on a heterogeneous DL cluster, characterizes the task-level performance\nheterogeneity in the optimization problem, and makes scheduling decisions\nacross both spatial and temporal dimensions. It involves the primal-dual\nframework employing a dual subroutine, to solve the optimization problem and\nguide the scheduling design. Our trace-driven simulation with representative DL\nmodel training workloads demonstrates that Hadar accelerates the total time\nduration by 1.20x when compared with its state-of-the-art heterogeneity-aware\ncounterpart, Gavel. Further, our Hadar scheduler is enhanced to HadarE by\nforking each job into multiple copies to let a job train concurrently on\nheterogeneous GPUs resided on separate available nodes (i.e., machines or\nservers) for resource utilization enhancement. HadarE is evaluated extensively\non physical DL clusters for comparison with Hadar and Gavel. With substantial\nenhancement in cluster resource utilization (by 1.45x), HadarE exhibits\nconsiderable speed-ups in DL model training, reducing the total time duration\nby 50% (or 80%) on an Amazon's AWS (or our lab) cluster, while producing\ntrained DL models with consistently better inference quality than those trained\nby Hadar.\n","authors":["Abeda Sultana","Nabin Pakka","Fei Xu","Xu Yuan","Li Chen","Nian-Feng Tzeng"],"pdf_url":"https://arxiv.org/pdf/2503.10918v2.pdf","comment":"14 pages, 12 figures, IEEE Transactions on Computers"},{"id":"http://arxiv.org/abs/2505.11578v2","updated":"2025-05-21T17:36:58Z","published":"2025-05-16T14:40:56Z","title":"Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with\n  Physics-informed Fine-tuning","summary":"  This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation.\n","authors":["Peimian Du","Jiabin Liu","Xiaowei Jin","Mengwang Zuo","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2505.11578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10717v2","updated":"2025-05-21T17:36:21Z","published":"2025-05-15T21:40:21Z","title":"A Modular Approach for Clinical SLMs Driven by Synthetic Data with\n  Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment","summary":"  High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average.\n","authors":["Jean-Philippe Corbeil","Amin Dada","Jean-Michel Attendu","Asma Ben Abacha","Alessandro Sordoni","Lucas Caccia","François Beaulieu","Thomas Lin","Jens Kleesiek","Paul Vozila"],"pdf_url":"https://arxiv.org/pdf/2505.10717v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15784v1","updated":"2025-05-21T17:35:08Z","published":"2025-05-21T17:35:08Z","title":"Large Language Models as Computable Approximations to Solomonoff\n  Induction","summary":"  The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.\n","authors":["Jun Wan","Lingrui Mei"],"pdf_url":"https://arxiv.org/pdf/2505.15784v1.pdf","comment":"Both authors contributed equally"},{"id":"http://arxiv.org/abs/2505.15779v1","updated":"2025-05-21T17:31:49Z","published":"2025-05-21T17:31:49Z","title":"IA-T2I: Internet-Augmented Text-to-Image Generation","summary":"  Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.\n","authors":["Chuanhao Li","Jianwen Sun","Yukang Feng","Mingliang Zhai","Yifan Chang","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15779v1.pdf","comment":"12 pages, 7 figures, a framework that integrates reference images\n  from the Internet into T2I/TI2I models"},{"id":"http://arxiv.org/abs/2505.15778v1","updated":"2025-05-21T17:29:15Z","published":"2025-05-21T17:29:15Z","title":"Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous\n  Concept Space","summary":"  Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.\n","authors":["Zhen Zhang","Xuehai He","Weixiang Yan","Ao Shen","Chenyang Zhao","Shuohang Wang","Yelong Shen","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21798v2","updated":"2025-05-21T17:21:45Z","published":"2025-04-30T16:56:06Z","title":"SWE-smith: Scaling Data for Software Engineering Agents","summary":"  Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com.\n","authors":["John Yang","Kilian Leret","Carlos E. Jimenez","Alexander Wettig","Kabir Khandpur","Yanzhe Zhang","Binyuan Hui","Ofir Press","Ludwig Schmidt","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2504.21798v2.pdf","comment":"All assets available at https://swesmith.com"},{"id":"http://arxiv.org/abs/2505.15765v1","updated":"2025-05-21T17:10:47Z","published":"2025-05-21T17:10:47Z","title":"Constructing a 3D Town from a Single Image","summary":"  Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.\n","authors":["Kaizhi Zheng","Ruijian Zhang","Jing Gu","Jie Yang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07578v2","updated":"2025-05-21T17:09:44Z","published":"2025-03-10T17:44:46Z","title":"Denoising Score Distillation: From Noisy Diffusion Pretraining to\n  One-Step High-Quality Generation","summary":"  Diffusion models have achieved remarkable success in generating\nhigh-resolution, realistic images across diverse natural distributions.\nHowever, their performance heavily relies on high-quality training data, making\nit challenging to learn meaningful distributions from corrupted samples. This\nlimitation restricts their applicability in scientific domains where clean data\nis scarce or costly to obtain. In this work, we introduce denoising score\ndistillation (DSD), a surprisingly effective and novel approach for training\nhigh-quality generative models from low-quality data. DSD first pretrains a\ndiffusion model exclusively on noisy, corrupted samples and then distills it\ninto a one-step generator capable of producing refined, clean outputs. While\nscore distillation is traditionally viewed as a method to accelerate diffusion\nmodels, we show that it can also significantly enhance sample quality,\nparticularly when starting from a degraded teacher model. Across varying noise\nlevels and datasets, DSD consistently improves generative performancewe\nsummarize our empirical evidence in Fig. 1. Furthermore, we provide theoretical\ninsights showing that, in a linear model setting, DSD identifies the eigenspace\nof the clean data distributions covariance matrix, implicitly regularizing the\ngenerator. This perspective reframes score distillation as not only a tool for\nefficiency but also a mechanism for improving generative models, particularly\nin low-quality data settings.\n","authors":["Tianyu Chen","Yasi Zhang","Zhendong Wang","Ying Nian Wu","Oscar Leong","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.07578v2.pdf","comment":"First Author and Second Author contributed equally to this work. The\n  last two authors equally advised this work"},{"id":"http://arxiv.org/abs/2505.15754v1","updated":"2025-05-21T16:59:32Z","published":"2025-05-21T16:59:32Z","title":"Improving planning and MBRL with temporally-extended actions","summary":"  Continuous time systems are often modeled using discrete time dynamics but\nthis requires a small simulation step to maintain accuracy. In turn, this\nrequires a large planning horizon which leads to computationally demanding\nplanning problems and reduced performance. Previous work in model free\nreinforcement learning has partially addressed this issue using action repeats\nwhere a policy is learned to determine a discrete action duration. Instead we\npropose to control the continuous decision timescale directly by using\ntemporally-extended actions and letting the planner treat the duration of the\naction as an additional optimization variable along with the standard action\nvariables. This additional structure has multiple advantages. It speeds up\nsimulation time of trajectories and, importantly, it allows for deep horizon\nsearch in terms of primitive actions while using a shallow search depth in the\nplanner. In addition, in the model based reinforcement learning (MBRL) setting,\nit reduces compounding errors from model learning and improves training time\nfor models. We show that this idea is effective and that the range for action\ndurations can be automatically selected using a multi-armed bandit formulation\nand integrated into the MBRL framework. An extensive experimental evaluation\nboth in planning and in MBRL, shows that our approach yields faster planning,\nbetter solutions, and that it enables solutions to problems that are not solved\nin the standard formulation.\n","authors":["Palash Chatterjee","Roni Khardon"],"pdf_url":"https://arxiv.org/pdf/2505.15754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15753v1","updated":"2025-05-21T16:58:14Z","published":"2025-05-21T16:58:14Z","title":"Scalable Defense against In-the-wild Jailbreaking Attacks with Safety\n  Context Retrieval","summary":"  Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.\n","authors":["Taiye Chen","Zeming Wei","Ang Li","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15835v3","updated":"2025-05-21T16:55:34Z","published":"2024-07-22T17:51:53Z","title":"dMel: Speech Tokenization made Simple","summary":"  Large language models have revolutionized natural language processing by\nleveraging self-supervised pretraining on vast textual data. Inspired by this\nsuccess, researchers have investigated various compression-based speech\ntokenization methods to discretize continuous speech signals, enabling the\napplication of language modeling techniques to discrete tokens. However, audio\ncompressor introduces additional complexity and computational cost, and often\nfail on out-of-domain audio signals. In this work, we introduce a novel speech\nrepresentation (dmel) that discretizes mel-filterbank channels into intensity\nbins, creating a simpler yet more effective representation compared to existing\nspeech tokenization methods. Our approach demonstrates superior performance in\npreserving audio content, robustness to out-of-domain data, and offers a\ntraining-free, natural, and streamable representation. To address the\nhigh-dimensional nature of log-mel spectrograms, we propose an efficient\nparallel encoding and decoding method for high-dimensional tokens using an\nLM-style transformer architecture. This innovation enables us to develop\nRichTTS and RichASR, two models sharing the same architecture while achieving\ncomparable or better results than specialized existing methods. Our results\ndemonstrate the effectiveness of dmel in achieving high performance on both\nspeech synthesis and recognition tasks within a unified framework, paving the\nway for efficient and effective joint modeling of speech and text.\n","authors":["Richard He Bai","Tatiana Likhomanenko","Ruixiang Zhang","Zijin Gu","Zakaria Aldeneh","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2407.15835v3.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2505.15746v1","updated":"2025-05-21T16:51:44Z","published":"2025-05-21T16:51:44Z","title":"Higher-order Structure Boosts Link Prediction on Temporal Graphs","summary":"  Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods.\n","authors":["Jingzhe Liu","Zhigang Hua","Yan Xie","Bingheng Li","Harry Shomer","Yu Song","Kaveh Hassani","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2505.15746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15742v1","updated":"2025-05-21T16:49:47Z","published":"2025-05-21T16:49:47Z","title":"Neuro-Argumentative Learning with Case-Based Reasoning","summary":"  We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual\nAA-CBR), a data-driven, neurosymbolic classification model in which the outcome\nis determined by an argumentation debate structure that is learned\nsimultaneously with neural-based feature extractors. Each argument in the\ndebate is an observed case from the training data, favouring their labelling.\nCases attack or support those with opposing or agreeing labellings, with the\nstrength of each argument and relationship learned through gradient-based\nmethods. This argumentation debate structure provides human-aligned reasoning,\nimproving model interpretability compared to traditional neural networks (NNs).\nUnlike the existing purely symbolic variant, Abstract Argumentation for\nCase-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class\nclassification, automatic learning of feature and data point importance,\nassigning uncertainty values to outcomes, using all available data points, and\ndoes not require binary features. We show that Gradual AA-CBR performs\ncomparably to NNs whilst significantly outperforming existing AA-CBR\nformulations.\n","authors":["Adam Gould","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2505.15742v1.pdf","comment":"Accepted to NeSy25"},{"id":"http://arxiv.org/abs/2410.15764v3","updated":"2025-05-21T16:46:32Z","published":"2024-10-21T08:23:31Z","title":"LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec","summary":"  Although discrete speech tokens have exhibited strong potential for language\nmodel-based speech generation, their high bitrates and redundant timbre\ninformation restrict the development of such models. In this work, we propose\nLSCodec, a discrete speech codec that has both low bitrate and speaker\ndecoupling ability. LSCodec adopts a multi-stage unsupervised training\nframework with a speaker perturbation technique. A continuous information\nbottleneck is first established, followed by vector quantization that produces\na discrete speaker-decoupled space. A discrete token vocoder finally refines\nacoustic details from LSCodec. By reconstruction evaluations, LSCodec\ndemonstrates superior intelligibility and audio quality with only a single\ncodebook and smaller vocabulary size than baselines. Voice conversion and\nspeaker probing experiments prove the excellent speaker disentanglement of\nLSCodec, and ablation study verifies the effectiveness of the proposed training\nframework.\n","authors":["Yiwei Guo","Zhihan Li","Chenpeng Du","Hankun Wang","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15764v3.pdf","comment":"5 pages, 2 figures, 3 tables. Demo page:\n  https://cantabile-kwok.github.io/LSCodec/. Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.15740v1","updated":"2025-05-21T16:45:43Z","published":"2025-05-21T16:45:43Z","title":"HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis\n  and Refinement","summary":"  Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.\n","authors":["Jilin Hu","Jianyu Zhang","Yongwang Zhao","Talia Ringer"],"pdf_url":"https://arxiv.org/pdf/2505.15740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15738v1","updated":"2025-05-21T16:43:17Z","published":"2025-05-21T16:43:17Z","title":"Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses","summary":"  Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.\n","authors":["Xiaoxue Yang","Bozhidar Stevanoski","Matthieu Meeus","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2505.15738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15734v1","updated":"2025-05-21T16:40:12Z","published":"2025-05-21T16:40:12Z","title":"DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning","summary":"  Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.\n","authors":["Gaurav Srivastava","Zhenyu Bi","Meng Lu","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10928v2","updated":"2025-05-21T16:32:41Z","published":"2025-02-15T23:37:32Z","title":"Probing Semantic Routing in Large Mixture-of-Expert Models","summary":"  In the past year, large (>100B parameter) mixture-of-expert (MoE) models have\nbecome increasingly common in the open domain. While their advantages are often\nframed in terms of efficiency, prior work has also explored functional\ndifferentiation through routing behavior. We investigate whether expert routing\nin large MoE models is influenced by the semantics of the inputs. To test this,\nwe design two controlled experiments. First, we compare activations on sentence\npairs with a shared target word used in the same or different senses. Second,\nwe fix context and substitute the target word with semantically similar or\ndissimilar alternatives. Comparing expert overlap across these conditions\nreveals clear, statistically significant evidence of semantic routing in large\nMoE models.\n","authors":["Matthew Lyle Olson","Neale Ratzlaff","Musashi Hinck","Man Luo","Sungduk Yu","Chendi Xue","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2502.10928v2.pdf","comment":"16 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.15722v1","updated":"2025-05-21T16:30:18Z","published":"2025-05-21T16:30:18Z","title":"Shared Path: Unraveling Memorization in Multilingual LLMs through\n  Language Similarities","summary":"  We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.\n","authors":["Xiaoyu Luo","Yiyi Chen","Johannes Bjerva","Qiongxiu Li"],"pdf_url":"https://arxiv.org/pdf/2505.15722v1.pdf","comment":"17 pages, 14 tables, 10 figures"},{"id":"http://arxiv.org/abs/2505.15703v1","updated":"2025-05-21T16:16:52Z","published":"2025-05-21T16:16:52Z","title":"HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context\n  Understanding and Future Motion Representation Learning","summary":"  Motion forecasting represents a critical challenge in autonomous driving\nsystems, requiring accurate prediction of surrounding agents' future\ntrajectories. While existing approaches predict future motion states with the\nextracted scene context feature from historical agent trajectories and road\nlayouts, they suffer from the information degradation during the scene feature\nencoding. To address the limitation, we propose HAMF, a novel motion\nforecasting framework that learns future motion representations with the scene\ncontext encoding jointly, to coherently combine the scene understanding and\nfuture motion state prediction. We first embed the observed agent states and\nmap information into 1D token sequences, together with the target multi-modal\nfuture motion features as a set of learnable tokens. Then we design a unified\nAttention-based encoder, which synergistically combines self-attention and\ncross-attention mechanisms to model the scene context information and aggregate\nfuture motion features jointly. Complementing the encoder, we implement the\nMamba module in the decoding stage to further preserve the consistency and\ncorrelations among the learned future motion representations, to generate the\naccurate and diverse final trajectories. Extensive experiments on Argoverse 2\nbenchmark demonstrate that our hybrid Attention-Mamba model achieves\nstate-of-the-art motion forecasting performance with the simple and lightweight\narchitecture.\n","authors":["Xiaodong Mei","Sheng Wang","Jie Cheng","Yingbing Chen","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2505.15703v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2505.09847v2","updated":"2025-05-21T16:12:30Z","published":"2025-05-14T23:12:20Z","title":"Causal Predictive Optimization and Generation for Business AI","summary":"  The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.\n","authors":["Liyang Zhao","Olurotimi Seton","Himadeep Reddy Reddivari","Suvendu Jena","Shadow Zhao","Rachit Kumar","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08185v2","updated":"2025-05-21T16:10:06Z","published":"2024-09-12T16:20:57Z","title":"Fine-tuning Large Language Models for Entity Matching","summary":"  Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and ability to generalize to unseen entities. Existing research on\nusing LLMs for entity matching has focused on prompt engineering and in-context\nlearning. This paper explores the potential of fine-tuning LLMs for entity\nmatching. We analyze fine-tuning along two dimensions: 1) the representation of\ntraining examples, where we experiment with adding different types of\nLLM-generated explanations to the training set, and 2) the selection and\ngeneration of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodels ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods, only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o-mini.\n","authors":["Aaron Steiner","Ralph Peeters","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2409.08185v2.pdf","comment":"8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch"},{"id":"http://arxiv.org/abs/2505.15694v1","updated":"2025-05-21T16:07:47Z","published":"2025-05-21T16:07:47Z","title":"A Unified Theoretical Analysis of Private and Robust Offline Alignment:\n  from RLHF to DPO","summary":"  In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios.\n","authors":["Xingyu Zhou","Yulian Wu","Francesco Orabona"],"pdf_url":"https://arxiv.org/pdf/2505.15694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15693v1","updated":"2025-05-21T16:06:51Z","published":"2025-05-21T16:06:51Z","title":"Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff\n  Objectives","summary":"  Recent advances in reinforcement learning (RL) have renewed focus on the\ndesign of reward functions that shape agent behavior. Manually designing reward\nfunctions is tedious and error-prone. A principled alternative is to specify\nbehaviors in a formal language that can be automatically translated into\nrewards. Omega-regular languages are a natural choice for this purpose, given\ntheir established role in formal verification and synthesis. However, existing\nmethods using omega-regular specifications typically rely on discounted reward\nRL in episodic settings, with periodic resets. This setup misaligns with the\nsemantics of omega-regular specifications, which describe properties over\ninfinite behavior traces. In such cases, the average reward criterion and the\ncontinuing setting -- where the agent interacts with the environment over a\nsingle, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on\nabsolute liveness specifications -- a subclass of omega-regular languages that\ncannot be violated by any finite behavior prefix, making them well-suited to\nthe continuing setting. We present the first model-free RL framework that\ntranslates absolute liveness specifications to average-reward objectives. Our\napproach enables learning in communicating MDPs without episodic resetting. We\nalso introduce a reward structure for lexicographic multi-objective\noptimization, aiming to maximize an external average-reward objective among the\npolicies that also maximize the satisfaction probability of a given\nomega-regular specification. Our method guarantees convergence in unknown\ncommunicating MDPs and supports on-the-fly reductions that do not require full\nknowledge of the environment, thus enabling model-free RL. Empirical results\nshow our average-reward approach in continuing setting outperforms\ndiscount-based methods across benchmarks.\n","authors":["Milad Kazemi","Mateo Perez","Fabio Somenzi","Sadegh Soudjani","Ashutosh Trivedi","Alvaro Velasquez"],"pdf_url":"https://arxiv.org/pdf/2505.15693v1.pdf","comment":"29 pages, 6 figures and 2 tables"},{"id":"http://arxiv.org/abs/2502.17720v3","updated":"2025-05-21T16:04:15Z","published":"2025-02-24T23:23:27Z","title":"Spontaneous Giving and Calculated Greed in Language Models","summary":"  Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.\n","authors":["Yuxuan Li","Hirokazu Shirado"],"pdf_url":"https://arxiv.org/pdf/2502.17720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15687v1","updated":"2025-05-21T16:03:03Z","published":"2025-05-21T16:03:03Z","title":"Discovering Pathology Rationale and Token Allocation for Efficient\n  Multimodal Pathology Reasoning","summary":"  Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.\n","authors":["Zhe Xu","Cheng Jin","Yihui Wang","Ziyi Liu","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2505.15687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15683v1","updated":"2025-05-21T15:58:08Z","published":"2025-05-21T15:58:08Z","title":"A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability","summary":"  Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.\n","authors":["Zishuai Zhang","Hainan Zhang","Jiaying Zheng","Ziwei Wang","Yongxin Tong","Jin Dong","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.15683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15674v1","updated":"2025-05-21T15:53:28Z","published":"2025-05-21T15:53:28Z","title":"UniErase: Unlearning Token as a Universal Erasure Primitive for Language\n  Models","summary":"  Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain.\n","authors":["Miao Yu","Liang Lin","Guibin Zhang","Xinfeng Li","Junfeng Fang","Ningyu Zhang","Kun Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17216v2","updated":"2025-05-21T15:51:37Z","published":"2025-02-24T14:49:52Z","title":"Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM\n  Reasoning","summary":"  Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, it remains unclear what the contributing factors to the success\nof Neurosymbolic LLM reasoning are. This paper shows that one important factor\nis the choice of the formal language. By comparing 4 formal languages on 3\ndatasets over 6 LLMs, we show that the choice of formal language affects both\nthe syntactic and the semantic reasoning capability. Thereby, we introduce the\nintermediate language challenge, which is the challenge of picking a suitable\nformal language for neurosymbolic reasoning. Further, we compare the effects of\nusing different in-context-learning examples in an ablation study. We conclude\nthat on average, context-aware encodings help LLMs to reason, while there is no\napparent effect of using comments or markdown syntax.\n","authors":["Alexander Beiser","David Penz","Nysret Musliu"],"pdf_url":"https://arxiv.org/pdf/2502.17216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15671v1","updated":"2025-05-21T15:50:03Z","published":"2025-05-21T15:50:03Z","title":"Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification","summary":"  Knowing the uncertainty associated with the output of a deep neural network\nis of paramount importance in making trustworthy decisions, particularly in\nhigh-stakes fields like medical diagnosis and autonomous systems. Monte Carlo\nDropout (MCD) is a widely used method for uncertainty quantification, as it can\nbe easily integrated into various deep architectures. However, conventional MCD\noften struggles with providing well-calibrated uncertainty estimates. To\naddress this, we introduce innovative frameworks that enhances MCD by\nintegrating different search solutions namely Grey Wolf Optimizer (GWO),\nBayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an\nuncertainty-aware loss function, thereby improving the reliability of\nuncertainty quantification. We conduct comprehensive experiments using\ndifferent backbones, namely DenseNet121, ResNet50, and VGG16, on various\ndatasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic\ndataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%\non average in terms of both conventional accuracy and uncertainty accuracy\nwhile achieving significantly better calibration. These results highlight the\npotential of our approach to enhance the trustworthiness of deep learning\nmodels in safety-critical applications.\n","authors":["Hamzeh Asgharnezhad","Afshar Shamsi","Roohallah Alizadehsani","Arash Mohammadi","Hamid Alinejad-Rokny"],"pdf_url":"https://arxiv.org/pdf/2505.15671v1.pdf","comment":"22 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2404.05894v5","updated":"2025-05-21T15:44:53Z","published":"2024-04-08T22:40:57Z","title":"Learning Heuristics for Transit Network Design and Improvement with Deep\n  Reinforcement Learning","summary":"  Planning a network of public transit routes is a challenging optimization\nproblem. Metaheuristic algorithms search through the space of possible transit\nnetworks by applying heuristics that randomly alter routes in a network. The\ndesign of these heuristics has a major impact on the quality of the result. In\nthis paper, we use deep reinforcement learning to train a graph neural net to\nprovide heuristics for an evolutionary algorithm. These neural heuristics\nimprove the algorithm's results on benchmark synthetic cities with 70 nodes or\nmore, and achieve new state-of-the-art results on the challenging Mumford\nbenchmark. They also improve upon a simulation of the real transit network in\nthe city of Laval, Canada, by 52% and 25% on two key metrics, and offer cost\nsavings of up to 19% over the city's existing transit network.\n","authors":["Andrew Holliday","Ahmed El-Geneidy","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2404.05894v5.pdf","comment":"Under review at the journal Transportmetrica B"},{"id":"http://arxiv.org/abs/2505.11436v2","updated":"2025-05-21T15:41:51Z","published":"2025-05-16T16:56:40Z","title":"GODBench: A Benchmark for Multimodal Large Language Models in Video\n  Comment Art","summary":"  Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.\n","authors":["Yiming Lei","Chenkai Zhang","Zeming Liu","Haitao Leng","Shaoguo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11436v2.pdf","comment":"69 pages, 66 figures, accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2505.15662v1","updated":"2025-05-21T15:38:55Z","published":"2025-05-21T15:38:55Z","title":"Neural Quantum Digital Twins for Optimizing Quantum Annealing","summary":"  Quantum annealers have shown potential in addressing certain combinatorial\noptimization problems, though their performance is often limited by scalability\nand errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT)\nframework that reconstructs the energy landscape of quantum many-body systems\nrelevant to quantum annealing. The digital twin models both ground and excited\nstate dynamics, enabling detailed simulation of the adiabatic evolution\nprocess. We benchmark NQDT on systems with known analytical solutions and\ndemonstrate that it accurately captures key quantum phenomena, including\nquantum criticality and phase transitions. Leveraging this framework, one can\nidentify optimal annealing schedules that minimize excitation-related errors.\nThese findings highlight the utility of neural network-based digital twins as a\ndiagnostic and optimization tool for improving the performance of quantum\nannealers.\n","authors":["Jianlong Lu","Hanqiu Peng","Ying Chen"],"pdf_url":"https://arxiv.org/pdf/2505.15662v1.pdf","comment":"20 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.13997v2","updated":"2025-05-21T15:37:25Z","published":"2025-01-23T11:04:25Z","title":"Predictive Learning in Energy-based Models with Attractor Structures","summary":"  Predictive models are highly advanced in understanding the mechanisms of\nbrain function. Recent advances in machine learning further underscore the\npower of prediction for optimal representation in learning. However, there\nremains a gap in creating a biologically plausible model that explains how the\nneural system achieves prediction. In this paper, we introduce a framework that\nemploys an energy-based model (EBM) to capture the nuanced processes of\npredicting observation after action within the neural system, encompassing\nprediction, learning, and inference. We implement the EBM with a hierarchical\nstructure and integrate a continuous attractor neural network for memory,\nconstructing a biologically plausible model. In experimental evaluations, our\nmodel demonstrates efficacy across diverse scenarios. The range of actions\nincludes eye movement, motion in environments, head turning, and static\nobservation while the environment changes. Our model not only makes accurate\npredictions for environments it was trained on, but also provides reasonable\npredictions for unseen environments, matching the performances of machine\nlearning methods in multiple tasks. We hope that this study contributes to a\ndeep understanding of how the neural system performs prediction.\n","authors":["Xingsi Dong","Xiangyuan Peng","Si Wu"],"pdf_url":"https://arxiv.org/pdf/2501.13997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03654v2","updated":"2025-05-21T15:36:14Z","published":"2025-02-05T22:32:22Z","title":"Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning\n  Dynamics","summary":"  Activation functions are fundamental elements of deep learning architectures\nas they significantly influence training dynamics. ReLU, while widely used, is\nprone to the dying neuron problem, which has been mitigated by variants such as\nLeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently,\nself-gated activations like GELU and Swish have emerged as state-of-the-art\nalternatives, leveraging their smoothness to ensure stable gradient flow and\nprevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit\n(GoLU), a novel self-gated activation function defined as $\\mathrm{GoLU}(x) = x\n\\, \\mathrm{Gompertz}(x)$, where $\\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU\nactivation leverages the right-skewed asymmetry in the Gompertz function to\nreduce variance in the latent space more effectively compared to GELU and\nSwish, while preserving robust gradient flow. Extensive experiments across\ndiverse tasks, including Image Classification, Language Modeling, Semantic\nSegmentation, Object Detection, Instance Segmentation, and Diffusion, highlight\nGoLU's superior performance relative to state-of-the-art activation functions,\nestablishing GoLU as a robust alternative to existing activation functions.\n","authors":["Indrashis Das","Mahmoud Safari","Steven Adriaensen","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2502.03654v2.pdf","comment":"8 pages, excluding references and appendix; v2: slight improvement in\n  presentation. Equation (4) added, with proof in Appendix A. Appendices B\n  (Flipped Mish) and I (Machine Translation) added. Figure 9 added to Appendix\n  C. Appendix D extended with Heatmaps 12 and 13"},{"id":"http://arxiv.org/abs/2404.18922v4","updated":"2025-05-21T15:34:02Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v4.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2502.15975v2","updated":"2025-05-21T15:33:56Z","published":"2025-02-21T22:23:16Z","title":"Sparsity May Be All You Need: Sparse Random Parameter Adaptation","summary":"  Full fine-tuning of large language models for alignment and task adaptation\nhas become prohibitively expensive as models have grown in size.\nParameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing\nthe computational and memory resources needed for fine-tuning these models by\nonly training on a small number of parameters instead of all model parameters.\nCurrently, the most popular PEFT method is the Low-Rank Adaptation (LoRA),\nwhich freezes the parameters of the model to be fine-tuned and introduces a\nsmall set of trainable parameters in the form of low-rank matrices. We propose\nsimply reducing the number of trainable parameters by randomly selecting a\nsmall proportion of the model parameters to train on. In this paper, we compare\nthe efficiency and performance of our proposed approach with PEFT methods,\nincluding LoRA, as well as full parameter fine-tuning.\n","authors":["Jesus Rios","Pierre Dognin","Ronny Luss","Karthikeyan N. Ramamurthy"],"pdf_url":"https://arxiv.org/pdf/2502.15975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10900v2","updated":"2025-05-21T15:33:20Z","published":"2025-05-16T06:07:19Z","title":"Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With An LLM","summary":"  Interaction sparsity is a long-standing challenge in recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nis also found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this issue either enrich the connectivity\ndata by incorporating social networks or external knowledge graphs, or\nfine-tune LLMs into interaction augmenters or next-item recommenders. However,\nthese techniques tend to be resource demanding, requiring high computational\npower. They also have several limitations, including data availability, low\nquality, or synthetic noise issues. In this work, we propose LLM-based Intent\nKnowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR leverages latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets.\n","authors":["Wenqing Zheng","Noah Fatsi","Daniel Barcklow","Dmitri Kalaev","Steven Yao","Owen Reinert","C. Bayan Bruss","Daniele Rosa"],"pdf_url":"https://arxiv.org/pdf/2505.10900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15657v1","updated":"2025-05-21T15:32:42Z","published":"2025-05-21T15:32:42Z","title":"LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved\n  Than Previously Thought","summary":"  Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research.\n","authors":["Cheng Yan","Felix Mohr","Tom Viering"],"pdf_url":"https://arxiv.org/pdf/2505.15657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03234v2","updated":"2025-05-21T15:26:54Z","published":"2025-04-04T07:34:01Z","title":"Think When You Need: Self-Adaptive Chain-of-Thought Learning","summary":"  Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\"\n","authors":["Junjie Yang","Ke Lin","Xing Yu"],"pdf_url":"https://arxiv.org/pdf/2504.03234v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.15647v1","updated":"2025-05-21T15:25:23Z","published":"2025-05-21T15:25:23Z","title":"Second-Order Convergence in Private Stochastic Non-Convex Optimization","summary":"  We investigate the problem of finding second-order stationary points (SOSP)\nin differentially private (DP) stochastic non-convex optimization. Existing\nmethods suffer from two key limitations: (i) inaccurate convergence error rate\ndue to overlooking gradient variance in the saddle point escape analysis, and\n(ii) dependence on auxiliary private model selection procedures for identifying\nDP-SOSP, which can significantly impair utility, particularly in distributed\nsettings. To address these issues, we propose a generic perturbed stochastic\ngradient descent (PSGD) framework built upon Gaussian noise injection and\ngeneral gradient oracles. A core innovation of our framework is using model\ndrift distance to determine whether PSGD escapes saddle points, ensuring\nconvergence to approximate local minima without relying on second-order\ninformation or additional DP-SOSP identification. By leveraging the adaptive\nDP-SPIDER estimator as a specific gradient oracle, we develop a new DP\nalgorithm that rectifies the convergence error rates reported in prior work. We\nfurther extend this algorithm to distributed learning with arbitrarily\nheterogeneous data, providing the first formal guarantees for finding DP-SOSP\nin such settings. Our analysis also highlights the detrimental impacts of\nprivate selection procedures in distributed learning under high-dimensional\nmodels, underscoring the practical benefits of our design. Numerical\nexperiments on real-world datasets validate the efficacy of our approach.\n","authors":["Youming Tao","Zuyuan Zhang","Dongxiao Yu","Xiuzhen Cheng","Falko Dressler","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01475v2","updated":"2025-05-21T15:24:04Z","published":"2025-05-02T14:27:49Z","title":"CodeSSM: Towards State Space Models for Code Understanding","summary":"  Although transformers are widely used for various code-specific tasks, they\nhave some significant limitations. In this paper, we investigate State Space\nModels (SSMs) as a potential alternative to transformers for code understanding\ntasks, such as code retrieval, classification, and clone detection. Previous\nresearch has already demonstrated that SSMs are more compute-efficient than\ntransformers. In our work, we show that SSMs are also more sample-efficient and\ncan effectively extrapolate to longer contexts (beyond the pretraining context)\nduring fine-tuning. Through comprehensive experiments, we demonstrate that SSMs\ncould serve as a viable alternative to transformers for code understanding\ntasks, while addressing some of the major limitations associated with\ntransformers.\n","authors":["Shweta Verma","Abhinav Anand","Mira Mezini"],"pdf_url":"https://arxiv.org/pdf/2505.01475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15644v1","updated":"2025-05-21T15:22:45Z","published":"2025-05-21T15:22:45Z","title":"FragFake: A Dataset for Fine-Grained Detection of Edited Images with\n  Vision Language Models","summary":"  Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.\n","authors":["Zhen Sun","Ziyi Zhang","Zeren Luo","Zeyang Sha","Tianshuo Cong","Zheng Li","Shiwen Cui","Weiqiang Wang","Jiaheng Wei","Xinlei He","Qi Li","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15644v1.pdf","comment":"14pages,15 figures"},{"id":"http://arxiv.org/abs/2505.15633v1","updated":"2025-05-21T15:17:38Z","published":"2025-05-21T15:17:38Z","title":"Listen to the Context: Towards Faithful Large Language Models for\n  Retrieval Augmented Generation on Climate Questions","summary":"  Large language models that use retrieval augmented generation have the\npotential to unlock valuable knowledge for researchers, policymakers, and the\npublic by making long and technical climate-related documents more accessible.\nWhile this approach can help alleviate factual hallucinations by relying on\nretrieved passages as additional context, its effectiveness depends on whether\nthe model's output remains faithful to these passages. To address this, we\nexplore the automatic assessment of faithfulness of different models in this\nsetting. We then focus on ClimateGPT, a large language model specialised in\nclimate science, to examine which factors in its instruction fine-tuning impact\nthe model's faithfulness. By excluding unfaithful subsets of the model's\ntraining data, we develop ClimateGPT Faithful+, which achieves an improvement\nin faithfulness from 30% to 57% in supported atomic claims according to our\nautomatic metric.\n","authors":["David Thulke","Jakob Kemmler","Christian Dugast","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2505.15633v1.pdf","comment":"Accepted at the ClimateNLP 2025 Workshop at ACL"},{"id":"http://arxiv.org/abs/2501.17822v2","updated":"2025-05-21T15:05:27Z","published":"2025-01-29T18:14:51Z","title":"Aggregation Schemes for Single-Vector WSI Representation Learning in\n  Digital Pathology","summary":"  A crucial step to efficiently integrate Whole Slide Images (WSIs) in\ncomputational pathology is assigning a single high-quality feature vector,\ni.e., one embedding, to each WSI. With the existence of many pre-trained deep\nneural networks and the emergence of foundation models, extracting embeddings\nfor sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,\ngiven their high resolution and gigapixel nature, inputting them into existing\nGPUs as a single image is not feasible. As a result, WSIs are usually split\ninto many patches. Feeding each patch to a pre-trained model, each WSI can then\nbe represented by a set of patches, hence, a set of embeddings. Hence, in such\na setup, WSI representation learning reduces to set representation learning\nwhere for each WSI we have access to a set of patch embeddings. To obtain a\nsingle embedding from a set of patch embeddings for each WSI, multiple\nset-based learning schemes have been proposed in the literature. In this paper,\nwe evaluate the WSI search performance of multiple recently developed\naggregation techniques (mainly set representation learning techniques)\nincluding simple average or max pooling operations, Deep Sets, Memory networks,\nFocal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse\nand binary Fisher Vector on four different primary sites including bladder,\nbreast, kidney, and Colon from TCGA. Further, we benchmark the search\nperformance of these methods against the median of minimum distances of patch\nembeddings, a non-aggregating approach used for WSI retrieval.\n","authors":["Sobhan Hemati","Ghazal Alabtah","Saghir Alfasly","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2501.17822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15612v1","updated":"2025-05-21T15:03:26Z","published":"2025-05-21T15:03:26Z","title":"Learn to Reason Efficiently with Adaptive Length-based Reward Shaping","summary":"  Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.\n","authors":["Wei Liu","Ruochen Zhou","Yiyun Deng","Yuzhen Huang","Junteng Liu","Yuntian Deng","Yizhe Zhang","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2505.15612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11299v3","updated":"2025-05-21T15:02:30Z","published":"2025-03-14T11:08:30Z","title":"BriLLM: Brain-inspired Large Language Model","summary":"  This paper reports the first brain-inspired large language model (BriLLM).\nThis is a non-Transformer, non-GPT, non-traditional machine learning\ninput-output controlled generative language model. The model is based on the\nSignal Fully-connected flowing (SiFu) definition on the directed graph in terms\nof the neural network, and has the interpretability of all nodes on the graph\nof the whole model, instead of the traditional machine learning model that only\nhas limited interpretability at the input and output ends. In the language\nmodel scenario, the token is defined as a node in the graph. A randomly shaped\nor user-defined signal flow flows between nodes on the principle of \"least\nresistance\" along paths. The next token or node to be predicted or generated is\nthe target of the signal flow. As a language model, BriLLM theoretically\nsupports infinitely long $n$-gram models when the model size is independent of\nthe input and predicted length of the model. The model's working signal flow\nprovides the possibility of recall activation and innate multi-modal support\nsimilar to the cognitive patterns of the human brain. At present, we released\nthe first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node\nwidth, 16-token long sequence prediction ability, and language model prediction\nperformance comparable to GPT-1. More computing power will help us explore the\ninfinite possibilities depicted above.\n","authors":["Hai Zhao","Hongqiu Wu","Dongjie Yang","Anni Zou","Jiale Hong"],"pdf_url":"https://arxiv.org/pdf/2503.11299v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15607v1","updated":"2025-05-21T15:00:07Z","published":"2025-05-21T15:00:07Z","title":"From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with\n  Pedagogy using Reinforcement Learning","summary":"  Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.\n","authors":["David Dinucu-Jianu","Jakub Macina","Nico Daheim","Ido Hakimi","Iryna Gurevych","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2505.15607v1.pdf","comment":"David Dinucu-Jianu and Jakub Macina contributed equally. Code\n  available: https://github.com/eth-lre/PedagogicalRL"},{"id":"http://arxiv.org/abs/2505.15596v1","updated":"2025-05-21T14:50:30Z","published":"2025-05-21T14:50:30Z","title":"Exploring LLM-Generated Feedback for Economics Essays: How Teaching\n  Assistants Evaluate and Envision Its Use","summary":"  This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.\n","authors":["Xinyi Lu","Aditya Mahesh","Zejia Shen","Mitchell Dudley","Larissa Sano","Xu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15596v1.pdf","comment":"To be published in AIED'2025: In Proceedings of the 26th\n  International Conference on Artificial Intelligence in Education. The system\n  prompt and example feedback can be found through\n  http://github.com/UM-Lifelong-Learning-Lab/AIED2025-Exploring-LLM-Generated-Feedback-for-Economics-Essay"},{"id":"http://arxiv.org/abs/2505.15594v1","updated":"2025-05-21T14:49:24Z","published":"2025-05-21T14:49:24Z","title":"Beyond Classification: Evaluating Diffusion Denoised Smoothing for\n  Security-Utility Trade off","summary":"  While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.\n","authors":["Yury Belousov","Brian Pulfer","Vitaliy Kinakh","Slava Voloshynovskiy"],"pdf_url":"https://arxiv.org/pdf/2505.15594v1.pdf","comment":"Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)"},{"id":"http://arxiv.org/abs/2505.15589v1","updated":"2025-05-21T14:46:41Z","published":"2025-05-21T14:46:41Z","title":"World Models as Reference Trajectories for Rapid Motor Adaptation","summary":"  Deploying learned control policies in real-world environments poses a\nfundamental challenge. When system dynamics change unexpectedly, performance\ndegrades until models are retrained on new data. We introduce Reflexive World\nModels (RWM), a dual control framework that uses world model predictions as\nimplicit reference trajectories for rapid adaptation. Our method separates the\ncontrol problem into long-term reward maximization through reinforcement\nlearning and robust motor execution through rapid latent control. This dual\narchitecture achieves significantly faster adaptation with low online\ncomputational cost compared to model-based RL baselines, while maintaining\nnear-optimal performance. The approach combines the benefits of flexible policy\nlearning through reinforcement learning with rapid error correction\ncapabilities, providing a principled approach to maintaining performance in\nhigh-dimensional continuous control tasks under varying dynamics.\n","authors":["Carlos Stein Brito","Daniel McNamee"],"pdf_url":"https://arxiv.org/pdf/2505.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15889v4","updated":"2025-05-21T14:46:05Z","published":"2025-01-27T09:25:56Z","title":"Adaptive Width Neural Networks","summary":"  For almost 70 years, researchers have mostly relied on hyper-parameter tuning\nto select the width of neural networks' layers. This paper challenges the\nstatus quo by introducing an easy-to-use technique to learn an unbounded width\nof a neural network's layer during training. The technique does not rely on\nalternate optimization nor hand-crafted gradient heuristics; rather, it jointly\noptimizes the width and the parameters of each layer via simple\nbackpropagation. We apply the technique to a broad range of data domains such\nas tables, images, text, sequences, and graphs, showing how the width adapts to\nthe task's difficulty. The method imposes a soft ordering of importance among\nneurons, by which it also is possible to truncate the trained network at\nvirtually zero cost, achieving a smooth trade-off between performance and\ncompute resources in a structured way. Alternatively, one can dynamically\ncompress the network with no performance degradation. In light of recent\nfoundation models trained on large datasets, believed to require billions of\nparameters and where hyper-parameter tuning is unfeasible due to humongous\ntraining costs, our approach stands as a viable alternative for width learning.\n","authors":["Federico Errica","Henrik Christiansen","Viktor Zaverkin","Mathias Niepert","Francesco Alesiani"],"pdf_url":"https://arxiv.org/pdf/2501.15889v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12719v3","updated":"2025-05-21T14:45:38Z","published":"2024-06-18T15:41:15Z","title":"Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis","summary":"  Large Language Models (LLMs), already shown to ace various text comprehension\ntasks, have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. Building on earlier studies of LLMs for tabular\ntasks, we probe how in-context learning (ICL), model scale, instruction tuning,\nand domain bias affect Tabular QA (TQA) robustness by testing LLMs, under\ndiverse augmentations and perturbations, on diverse domains: Wikipedia-based\n$\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$, and scientific $\\textbf{SCITAB}$.\nAlthough instruction tuning and larger, newer LLMs deliver stronger, more\nrobust TQA performance, data contamination and reliability issues, especially\non $\\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and the drops in performance, with sensitivity peaking in the\nmodel's middle layers. We highlight the need for improved interpretable\nmethodologies to develop more reliable LLMs for table comprehension.\n","authors":["Kushal Raj Bhandari","Sixue Xing","Soham Dan","Jianxi Gao"],"pdf_url":"https://arxiv.org/pdf/2406.12719v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07098v2","updated":"2025-05-21T14:42:41Z","published":"2024-09-11T08:36:49Z","title":"Diversity-Driven View Subset Selection for Indoor Novel View Synthesis","summary":"  Novel view synthesis of indoor scenes can be achieved by capturing a\nmonocular video sequence of the environment. However, redundant information\ncaused by artificial movements in the input video data reduces the efficiency\nof scene modeling. To address this, we formulate the problem as a combinatorial\noptimization task for view subset selection. In this work, we propose a novel\nsubset selection framework that integrates a comprehensive diversity-based\nmeasurement with well-designed utility functions. We provide a theoretical\nanalysis of these utility functions and validate their effectiveness through\nextensive experiments. Furthermore, we introduce IndoorTraj, a novel dataset\ndesigned for indoor novel view synthesis, featuring complex and extended\ntrajectories that simulate intricate human behaviors. Experiments on IndoorTraj\nshow that our framework consistently outperforms baseline strategies while\nusing only 5-20% of the data, highlighting its remarkable efficiency and\neffectiveness. The code is available at:\nhttps://github.com/zehao-wang/IndoorTraj\n","authors":["Zehao Wang","Han Zhou","Matthew B. Blaschko","Tinne Tuytelaars","Minye Wu"],"pdf_url":"https://arxiv.org/pdf/2409.07098v2.pdf","comment":"12 pages, TMLR 2025"},{"id":"http://arxiv.org/abs/2505.15581v1","updated":"2025-05-21T14:36:01Z","published":"2025-05-21T14:36:01Z","title":"UWSAM: Segment Anything Model Guided Underwater Instance Segmentation\n  and A Large-scale Benchmark Dataset","summary":"  With recent breakthroughs in large-scale modeling, the Segment Anything Model\n(SAM) has demonstrated significant potential in a variety of visual\napplications. However, due to the lack of underwater domain expertise, SAM and\nits variants face performance limitations in end-to-end underwater instance\nsegmentation tasks, while their higher computational requirements further\nhinder their application in underwater scenarios. To address this challenge, we\npropose a large-scale underwater instance segmentation dataset, UIIS10K, which\nincludes 10,048 images with pixel-level annotations for 10 categories. Then, we\nintroduce UWSAM, an efficient model designed for automatic and accurate\nsegmentation of underwater instances. UWSAM efficiently distills knowledge from\nthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the\nMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective\nvisual representation learning. Furthermore, we design an End-to-end Underwater\nPrompt Generator (EUPG) for UWSAM, which automatically generates underwater\nprompts instead of explicitly providing foreground points or boxes as prompts,\nthus enabling the network to locate underwater instances accurately for\nefficient segmentation. Comprehensive experimental results show that our model\nis effective, achieving significant performance improvements over\nstate-of-the-art methods on multiple underwater instance datasets. Datasets and\ncodes are available at https://github.com/LiamLian0727/UIIS10K.\n","authors":["Hua Li","Shijie Lian","Zhiyuan Li","Runmin Cong","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2505.15581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15572v1","updated":"2025-05-21T14:25:41Z","published":"2025-05-21T14:25:41Z","title":"Bridging the Domain Gap in Equation Distillation with Reinforcement\n  Feedback","summary":"  The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.\n","authors":["Wangyang Ying","Haoyue Bai","Nanxu Gong","Xinyuan Wang","Sixun Dong","Haifeng Chen","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2505.15572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15559v1","updated":"2025-05-21T14:17:25Z","published":"2025-05-21T14:17:25Z","title":"Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music\n  Attributes","summary":"  Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.\n","authors":["Zixun Guo","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2505.15559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15558v1","updated":"2025-05-21T14:17:06Z","published":"2025-05-21T14:17:06Z","title":"Robo-DM: Data Management For Large Robot Datasets","summary":"  Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.\n","authors":["Kaiyuan Chen","Letian Fu","David Huang","Yanxiang Zhang","Lawrence Yunliang Chen","Huang Huang","Kush Hari","Ashwin Balakrishna","Ted Xiao","Pannag R Sanketi","John Kubiatowicz","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2505.15558v1.pdf","comment":"Best paper finalist of IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2505.15554v1","updated":"2025-05-21T14:15:49Z","published":"2025-05-21T14:15:49Z","title":"DayDreamer at CQs-Gen 2025: Generating Critical Questions through\n  Argument Scheme Completion","summary":"  Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.\n","authors":["Wendi Zhou","Ameer Saadat-Yazdi","Nadin Kökciyan"],"pdf_url":"https://arxiv.org/pdf/2505.15554v1.pdf","comment":"ArgMining 2025 CQs-Gen shared task"},{"id":"http://arxiv.org/abs/2505.15547v1","updated":"2025-05-21T14:11:59Z","published":"2025-05-21T14:11:59Z","title":"Oversmoothing, \"Oversquashing\", Heterophily, Long-Range, and more:\n  Demystifying Common Beliefs in Graph Machine Learning","summary":"  After a renaissance phase in which researchers revisited the message-passing\nparadigm through the lens of deep learning, the graph machine learning\ncommunity shifted its attention towards a deeper and practical understanding of\nmessage-passing's benefits and limitations. In this position paper, we notice\nhow the fast pace of progress around the topics of oversmoothing and\noversquashing, the homophily-heterophily dichotomy, and long-range tasks, came\nwith the consolidation of commonly accepted beliefs and assumptions that are\nnot always true nor easy to distinguish from each other. We argue that this has\nled to ambiguities around the investigated problems, preventing researchers\nfrom focusing on and addressing precise research questions while causing a good\namount of misunderstandings. Our contribution wants to make such common beliefs\nexplicit and encourage critical thinking around these topics, supported by\nsimple but noteworthy counterexamples. The hope is to clarify the distinction\nbetween the different issues and promote separate but intertwined research\ndirections to address them.\n","authors":["Adrian Arnaiz-Rodriguez","Federico Errica"],"pdf_url":"https://arxiv.org/pdf/2505.15547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13178v4","updated":"2025-05-21T14:11:19Z","published":"2025-02-18T07:35:35Z","title":"Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy,\n  Unified Evaluation, and Comparative Analysis","summary":"  Post-training Quantization (PTQ) technique has been extensively adopted for\nlarge language models (LLMs) compression owing to its efficiency and low\nresource requirement. However, current research lacks a in-depth analysis of\nthe superior and applicable scenarios of each PTQ strategy. In addition,\nexisting algorithms focus primarily on performance, overlooking the trade-off\namong model size, performance, and quantization bitwidth. To mitigate these\nconfusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly,\nin order to support our benchmark, we propose a comprehensive taxonomy for\nexisting mainstream methods by scrutinizing their computational strategies\n(e.g., optimization-based, compensation-based, etc.). Then, we conduct\nextensive experiments with the baseline within each class, covering models with\nvarious sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1),\narchitectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and\nVILA1.5) on a wide range of evaluation metrics.Through comparative analysis on\nthe results, we summarize the superior of each PTQ strategy and\nmodelsize-bitwidth trade-off considering the performance. For example, our\nbenchmark reveals that compensation-based technique demonstrates outstanding\ncross-architecture robustness and extremely low-bit PTQ for ultra large models\nshould be reexamined. Finally, we further accordingly claim that a practical\ncombination of compensation and other PTQ strategy can achieve SOTA various\nrobustness. We believe that our benchmark will provide valuable recommendations\nfor the deployment of LLMs and future research on PTQ approaches.We conduct an\nrepository for our benchmark at https://github.com/zjq0455/PTQ_Benchmark.\n","authors":["Jiaqi Zhao","Ming Wang","Miao Zhang","Yuzhang Shang","Xuebo Liu","Yaowei Wang","Min Zhang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.13178v4.pdf","comment":"17 pages, 3 fugures"},{"id":"http://arxiv.org/abs/2505.06520v2","updated":"2025-05-21T14:04:41Z","published":"2025-05-10T05:35:08Z","title":"PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of\n  Neural Networks","summary":"  It is often desirable to remove (a.k.a. unlearn) a specific part of the\ntraining data from a trained neural network model. A typical application\nscenario is to protect the data holder's right to be forgotten, which has been\npromoted by many recent regulation rules. Existing unlearning methods involve\ntraining alternative models with remaining data, which may be costly and\nchallenging to verify from the data holder or a thirdparty auditor's\nperspective. In this work, we provide a new angle and propose a novel\nunlearning approach by imposing carefully crafted \"patch\" on the original\nneural network to achieve targeted \"forgetting\" of the requested data to\ndelete. Specifically, inspired by the research line of neural network repair,\nwe propose to strategically seek a lightweight minimum \"patch\" for unlearning a\ngiven data point with certifiable guarantee. Furthermore, to unlearn a\nconsiderable amount of data points (or an entire class), we propose to\niteratively select a small subset of representative data points to unlearn,\nwhich achieves the effect of unlearning the whole set. Extensive experiments on\nmultiple categorical datasets demonstrates our approach's effectiveness,\nachieving measurable unlearning while preserving the model's performance and\nbeing competitive in efficiency and memory consumption compared to various\nbaseline methods.\n","authors":["Xuran Li","Jingyi Wang","Xiaohan Yuan","Peixin Zhang","Zhan Qin","Zhibo Wang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2505.06520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01701v2","updated":"2025-05-21T14:01:30Z","published":"2025-02-03T09:14:26Z","title":"Learning with Differentially Private (Sliced) Wasserstein Gradients","summary":"  In this work, we introduce a novel framework for privately optimizing\nobjectives that rely on Wasserstein distances between data-dependent empirical\nmeasures. Our main theoretical contribution is, based on an explicit\nformulation of the Wasserstein gradient in a fully discrete setting, a control\non the sensitivity of this gradient to individual data points, allowing strong\nprivacy guarantees at minimal utility cost. Building on these insights, we\ndevelop a deep learning approach that incorporates gradient and activations\nclipping, originally designed for DP training of problems with a finite-sum\nstructure. We further demonstrate that privacy accounting methods extend to\nWasserstein-based objectives, facilitating large-scale private training.\nEmpirical results confirm that our framework effectively balances accuracy and\nprivacy, offering a theoretically sound solution for privacy-preserving machine\nlearning tasks relying on optimal transport distances such as Wasserstein\ndistance or sliced-Wasserstein distance.\n","authors":["David Rodríguez-Vítores","Clément Lalanne","Jean-Michel Loubes"],"pdf_url":"https://arxiv.org/pdf/2502.01701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20808v5","updated":"2025-05-21T14:00:34Z","published":"2025-02-28T07:50:36Z","title":"MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts","summary":"  Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.\n","authors":["Peijie Wang","Zhong-Zhi Li","Fei Yin","Xin Yang","Dekang Ran","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20808v5.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2503.03122v4","updated":"2025-05-21T14:00:20Z","published":"2025-03-05T02:37:41Z","title":"The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models","summary":"  Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.\n","authors":["Zichao Li","Xueru Wen","Jie Lou","Yuqiu Ji","Yaojie Lu","Xianpei Han","Debing Zhang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2503.03122v4.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2501.14544v2","updated":"2025-05-21T13:57:01Z","published":"2025-01-24T14:47:42Z","title":"Distributed Conformal Prediction via Message Passing","summary":"  Post-hoc calibration of pre-trained models is critical for ensuring reliable\ninference, especially in safety-critical domains such as healthcare. Conformal\nPrediction (CP) offers a robust post-hoc calibration framework, providing\ndistribution-free statistical coverage guarantees for prediction sets by\nleveraging held-out datasets. In this work, we address a decentralized setting\nwhere each device has limited calibration data and can communicate only with\nits neighbors over an arbitrary graph topology. We propose two\nmessage-passing-based approaches for achieving reliable inference via CP:\nquantile-based distributed conformal prediction (Q-DCP) and histogram-based\ndistributed conformal prediction (H-DCP). Q-DCP employs distributed quantile\nregression enhanced with tailored smoothing and regularization terms to\naccelerate convergence, while H-DCP uses a consensus-based histogram estimation\napproach. Through extensive experiments, we investigate the trade-offs between\nhyperparameter tuning requirements, communication overhead, coverage\nguarantees, and prediction set sizes across different network topologies. The\ncode of our work is released on:\nhttps://github.com/HaifengWen/Distributed-Conformal-Prediction.\n","authors":["Haifeng Wen","Hong Xing","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2501.14544v2.pdf","comment":"19 pages, 17 figures"},{"id":"http://arxiv.org/abs/2505.15524v1","updated":"2025-05-21T13:50:23Z","published":"2025-05-21T13:50:23Z","title":"Evaluate Bias without Manual Test Sets: A Concept Representation\n  Perspective for LLMs","summary":"  Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.\n","authors":["Lang Gao","Kaiyang Wan","Wei Liu","Chenxi Wang","Zirui Song","Zixiang Xu","Yanbo Wang","Veselin Stoyanov","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2505.15524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02847v3","updated":"2025-05-21T13:45:40Z","published":"2025-05-01T19:06:10Z","title":"Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models","summary":"  Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.\n","authors":["Bang Zhang","Ruotian Ma","Qingxuan Jiang","Peisong Wang","Jiaqi Chen","Zheng Xie","Xingyu Chen","Yue Wang","Fanghua Ye","Jian Li","Yifan Yang","Zhaopeng Tu","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2505.02847v3.pdf","comment":"code: https://github.com/Tencent/digitalhuman/tree/main/SAGE"},{"id":"http://arxiv.org/abs/2505.15517v1","updated":"2025-05-21T13:42:52Z","published":"2025-05-21T13:42:52Z","title":"Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets","summary":"  Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.\n","authors":["Kaiyuan Chen","Shuangyu Xie","Zehan Ma","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2505.15517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15516v1","updated":"2025-05-21T13:42:28Z","published":"2025-05-21T13:42:28Z","title":"Explainable embeddings with Distance Explainer","summary":"  While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.\n","authors":["Christiaan Meijer","E. G. Patrick Bos"],"pdf_url":"https://arxiv.org/pdf/2505.15516v1.pdf","comment":"33 pages, 19 figures. Submitted to JMLR. Method implementation:\n  https://research-software-directory.org/software/distance-explainer"},{"id":"http://arxiv.org/abs/2505.15514v1","updated":"2025-05-21T13:38:45Z","published":"2025-05-21T13:38:45Z","title":"AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization","summary":"  Proximal Policy Optimization (PPO) is a widely used reinforcement learning\nalgorithm that heavily relies on accurate advantage estimates for stable and\nefficient training. However, raw advantage signals can exhibit significant\nvariance, noise, and scale-related issues, impeding optimal learning\nperformance. To address this challenge, we introduce Advantage Modulation PPO\n(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage\nestimates using a dynamic, non-linear scaling mechanism. This adaptive\nmodulation employs an alpha controller that dynamically adjusts the scaling\nfactor based on evolving statistical properties of the advantage signals, such\nas their norm, variance, and a predefined target saturation level. By\nincorporating a tanh-based gating function driven by these adaptively scaled\nadvantages, AM-PPO reshapes the advantage signals to stabilize gradient updates\nand improve the conditioning of the policy gradient landscape. Crucially, this\nmodulation also influences value function training by providing consistent and\nadaptively conditioned learning targets. Empirical evaluations across standard\ncontinuous control benchmarks demonstrate that AM-PPO achieves superior reward\ntrajectories, exhibits sustained learning progression, and significantly\nreduces the clipping required by adaptive optimizers. These findings underscore\nthe potential of advantage modulation as a broadly applicable technique for\nenhancing reinforcement learning optimization.\n","authors":["Soham Sane"],"pdf_url":"https://arxiv.org/pdf/2505.15514v1.pdf","comment":"17 pages, 4 Tables, 9 Figures, 11 equations"},{"id":"http://arxiv.org/abs/2502.07316v4","updated":"2025-05-21T13:38:27Z","published":"2025-02-11T07:26:50Z","title":"CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction","summary":"  Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.\n","authors":["Junlong Li","Daya Guo","Dejian Yang","Runxin Xu","Yu Wu","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2502.07316v4.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2410.02387v4","updated":"2025-05-21T13:32:08Z","published":"2024-10-03T11:07:43Z","title":"BiSSL: Enhancing the Alignment Between Self-Supervised Pretraining and\n  Downstream Fine-Tuning via Bilevel Optimization","summary":"  Models initialized from self-supervised pretraining may suffer from poor\nalignment with downstream tasks, reducing the extent to which subsequent\nfine-tuning can adapt pretrained features toward downstream objectives. To\nmitigate this, we introduce BiSSL, a novel bilevel training framework that\nenhances the alignment of self-supervised pretrained models with downstream\ntasks prior to fine-tuning. BiSSL acts as an intermediate training stage\nconducted after conventional self-supervised pretraining and is tasked with\nsolving a bilevel optimization problem that incorporates the pretext and\ndownstream training objectives in its lower- and upper-level objectives,\nrespectively. This approach explicitly models the interdependence between the\npretraining and fine-tuning stages within the conventional self-supervised\nlearning pipeline, facilitating enhanced information sharing between them that\nultimately leads to a model initialization better aligned with the downstream\ntask. We propose a general training algorithm for BiSSL that is compatible with\na broad range of pretext and downstream tasks. Using SimCLR and Bootstrap Your\nOwn Latent to pretrain ResNet-50 backbones on the ImageNet dataset, we\ndemonstrate that our proposed framework significantly improves accuracy on the\nvast majority of 12 downstream image classification datasets, as well as on\nobject detection. Exploratory analyses alongside investigative experiments\nfurther provide compelling evidence that BiSSL enhances downstream alignment.\n","authors":["Gustav Wagner Zakarias","Lars Kai Hansen","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2410.02387v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15507v1","updated":"2025-05-21T13:27:14Z","published":"2025-05-21T13:27:14Z","title":"Directional Non-Commutative Monoidal Structures for Compositional\n  Embeddings in Machine Learning","summary":"  We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.\n","authors":["Mahesh Godavarti"],"pdf_url":"https://arxiv.org/pdf/2505.15507v1.pdf","comment":"11 pages submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2505.15504v1","updated":"2025-05-21T13:24:47Z","published":"2025-05-21T13:24:47Z","title":"Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole\n  Slide Image Classification","summary":"  Deep learning has advanced computational pathology but expert annotations\nremain scarce. Few-shot learning mitigates annotation burdens yet suffers from\noverfitting and discriminative feature mischaracterization. In addition, the\ncurrent few-shot multiple instance learning (MIL) approaches leverage\npretrained vision-language models to alleviate these issues, but at the cost of\ncomplex preprocessing and high computational cost. We propose a\nSqueeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in\nMIL models to address these challenges. The SR block comprises two core\ncomponents: a pair of low-rank trainable matrices (squeeze pathway, SP) that\nreduces parameter count and imposes a bottleneck to prevent spurious feature\nlearning, and a frozen random recalibration matrix that preserves geometric\nstructure, diversifies feature directions, and redefines the optimization\nobjective for the SP. We provide theoretical guarantees that the SR block can\napproximate any linear mapping to arbitrary precision, thereby ensuring that\nthe performance of a standard MIL model serves as a lower bound for its\nSR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL\nmodels consistently outperform prior methods while requiring significantly\nfewer parameters and no architectural changes.\n","authors":["Conghao Xiong","Zhengrui Guo","Zhe Xu","Yifei Zhang","Raymond Kai-Yu Tong","Si Yong Yeo","Hao Chen","Joseph J. Y. Sung","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2505.15504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15501v1","updated":"2025-05-21T13:22:34Z","published":"2025-05-21T13:22:34Z","title":"Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks:\n  Memorization and Generalization with Knowledge Graphs","summary":"  We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.\n","authors":["Federico Ranaldi","Andrea Zugarini","Leonardo Ranaldi","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2505.15501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05874v2","updated":"2025-05-21T12:50:58Z","published":"2025-04-08T09:58:41Z","title":"Systematic Parameter Decision in Approximate Model Counting","summary":"  This paper proposes a novel approach to determining the internal parameters\nof the hashing-based approximate model counting algorithm $\\mathsf{ApproxMC}$.\nIn this problem, the chosen parameter values must ensure that\n$\\mathsf{ApproxMC}$ is Probably Approximately Correct (PAC), while also making\nit as efficient as possible. The existing approach to this problem relies on\nheuristics; in this paper, we solve this problem by formulating it as an\noptimization problem that arises from generalizing $\\mathsf{ApproxMC}$'s\ncorrectness proof to arbitrary parameter values.\n  Our approach separates the concerns of algorithm soundness and optimality,\nallowing us to address the former without the need for repetitive case-by-case\nargumentation, while establishing a clear framework for the latter.\nFurthermore, after reduction, the resulting optimization problem takes on an\nexceptionally simple form, enabling the use of a basic search algorithm and\nproviding insight into how parameter values affect algorithm performance.\nExperimental results demonstrate that our optimized parameters improve the\nruntime performance of the latest $\\mathsf{ApproxMC}$ by a factor of 1.6 to\n2.4, depending on the error tolerance.\n","authors":["Jinping Lei","Toru Takisaka","Junqiang Peng","Mingyu Xiao"],"pdf_url":"https://arxiv.org/pdf/2504.05874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15475v1","updated":"2025-05-21T12:49:37Z","published":"2025-05-21T12:49:37Z","title":"LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in\n  Large Language Models","summary":"  Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.\n","authors":["Zhanyue Qin","Yue Ding","Deyuan Liu","Qingbin Liu","Junxian Cai","Xi Chen","Zhiying Tu","Dianhui Chu","Cuiyun Gao","Dianbo Sui"],"pdf_url":"https://arxiv.org/pdf/2505.15475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03820v2","updated":"2025-05-21T12:49:19Z","published":"2024-11-06T10:42:04Z","title":"Beyond The Rainbow: High Performance Deep Reinforcement Learning on a\n  Desktop PC","summary":"  Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent\nenhancements could significantly boost a reinforcement learning (RL) agent's\nperformance. In this paper, we present \"Beyond The Rainbow\" (BTR), a novel\nalgorithm that integrates six improvements from across the RL literature to\nRainbow DQN, establishing a new state-of-the-art for RL using a desktop PC,\nwith a human-normalized interquartile mean (IQM) of 7.4 on Atari-60. Beyond\nAtari, we demonstrate BTR's capability to handle complex 3D games, successfully\ntraining agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with\nminimal algorithmic changes. Designing BTR with computational efficiency in\nmind, agents can be trained using a high-end desktop PC on 200 million Atari\nframes within 12 hours. Additionally, we conduct detailed ablation studies of\neach component, analyzing the performance and impact using numerous measures.\nCode is available at https://github.com/VIPTankz/BTR.\n","authors":["Tyler Clark","Mark Towers","Christine Evers","Jonathon Hare"],"pdf_url":"https://arxiv.org/pdf/2411.03820v2.pdf","comment":"9 main pages, 28 total. Accepted at ICML 2025 (Poster)"},{"id":"http://arxiv.org/abs/2505.15469v1","updated":"2025-05-21T12:45:49Z","published":"2025-05-21T12:45:49Z","title":"A Qualitative Investigation into LLM-Generated Multilingual Code\n  Comments and Automatic Evaluation Metrics","summary":"  Large Language Models are essential coding assistants, yet their training is\npredominantly English-centric. In this study, we evaluate the performance of\ncode language models in non-English contexts, identifying challenges in their\nadoption and integration into multilingual workflows. We conduct an open-coding\nstudy to analyze errors in code comments generated by five state-of-the-art\ncode models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2\nacross five natural languages: Chinese, Dutch, English, Greek, and Polish. Our\nstudy yields a dataset of 12,500 labeled generations, which we publicly\nrelease. We then assess the reliability of standard metrics in capturing\ncomment \\textit{correctness} across languages and evaluate their\ntrustworthiness as judgment criteria. Through our open-coding investigation, we\nidentified a taxonomy of 26 distinct error categories in model-generated code\ncomments. They highlight variations in language cohesion, informativeness, and\nsyntax adherence across different natural languages. Our analysis shows that,\nwhile these models frequently produce partially correct comments, modern neural\nmetrics fail to reliably differentiate meaningful completions from random\nnoise. Notably, the significant score overlap between expert-rated correct and\nincorrect comments calls into question the effectiveness of these metrics in\nassessing generated comments.\n","authors":["Jonathan Katzy","Yongcheng Huang","Gopal-Raj Panchu","Maksym Ziemlewski","Paris Loizides","Sander Vermeulen","Arie van Deursen","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2505.15469v1.pdf","comment":"Accepted PROMISE '25"},{"id":"http://arxiv.org/abs/2505.15467v1","updated":"2025-05-21T12:45:28Z","published":"2025-05-21T12:45:28Z","title":"Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning","summary":"  Large language models have achieved remarkable success in various tasks.\nHowever, it is challenging for them to learn new tasks incrementally due to\ncatastrophic forgetting. Existing approaches rely on experience replay,\noptimization constraints, or task differentiation, which encounter strict\nlimitations in real-world scenarios. To address these issues, we propose Joint\nFlashback Adaptation. We first introduce flashbacks -- a limited number of\nprompts from old tasks -- when adapting to new tasks and constrain the\ndeviations of the model outputs compared to the original one. We then\ninterpolate latent tasks between flashbacks and new tasks to enable jointly\nlearning relevant latent tasks, new tasks, and flashbacks, alleviating data\nsparsity in flashbacks and facilitating knowledge sharing for smooth\nadaptation. Our method requires only a limited number of flashbacks without\naccess to the replay data and is task-agnostic. We conduct extensive\nexperiments on state-of-the-art large language models across 1000+\ninstruction-following tasks, arithmetic reasoning tasks, and general reasoning\ntasks. The results demonstrate the superior performance of our method in\nimproving generalization on new tasks and reducing forgetting in old tasks.\n","authors":["Yukun Zhao","Lingyong Yan","Zhenyang Li","Shuaiqiang Wang","Zhumin Chen","Zhaochun Ren","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2505.15467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17403v3","updated":"2025-05-21T12:31:35Z","published":"2025-02-24T18:30:36Z","title":"Large Language Models are Powerful Electronic Health Record Encoders","summary":"  Electronic Health Records (EHRs) offer considerable potential for clinical\nprediction, but their complexity and heterogeneity present significant\nchallenges for traditional machine learning methods. Recently, domain-specific\nEHR foundation models trained on large volumes of unlabeled EHR data have shown\nimproved predictive accuracy and generalization. However, their development is\nconstrained by limited access to diverse, high-quality datasets, and by\ninconsistencies in coding standards and clinical practices. In this study, we\nexplore the use of general-purpose Large Language Models (LLMs) to encode EHR\ninto high-dimensional representations for downstream clinical prediction tasks.\nWe convert structured EHR data into markdown-formatted plain text documents by\nreplacing medical codes with natural language descriptions. This enables the\nuse of LLMs and their extensive semantic understanding and generalization\ncapabilities as effective encoders of EHRs without requiring access to private\nmedical training data. We show that LLM-based embeddings can often match or\neven surpass the performance of a specialized EHR foundation model,\nCLMBR-T-Base, across 15 diverse clinical tasks from the EHRSHOT benchmark. To\ndemonstrate generalizability, we further evaluate the approach on the UK\nBiobank (UKB) cohort, a population distinct from that used to train\nCLMBR-T-Base. Notably, one of the tested LLM-based models achieves superior\nperformance for disease onset, hospitalization, and mortality prediction,\nhighlighting robustness to shifts in patient populations. Our findings suggest\nthat repurposed general-purpose LLMs for EHR encoding provide a scalable and\ngeneralizable alternative to domain-specific models for clinical prediction.\n","authors":["Stefan Hegselmann","Georg von Arnim","Tillmann Rheude","Noel Kronenberg","David Sontag","Gerhard Hindricks","Roland Eils","Benjamin Wild"],"pdf_url":"https://arxiv.org/pdf/2502.17403v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15447v1","updated":"2025-05-21T12:29:40Z","published":"2025-05-21T12:29:40Z","title":"ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification\n  Reinforcement Learning","summary":"  Video understanding is inherently intention-driven-humans naturally focus on\nrelevant frames based on their goals. Recent advancements in multimodal large\nlanguage models (MLLMs) have enabled flexible query-driven reasoning; however,\nvideo-based frameworks like Video Chain-of-Thought lack direct training signals\nto effectively identify relevant frames. Current approaches often rely on\nheuristic methods or pseudo-label supervised annotations, which are both costly\nand limited in scalability across diverse scenarios. To overcome these\nchallenges, we introduce ViaRL, the first framework to leverage rule-based\nreinforcement learning (RL) for optimizing frame selection in intention-driven\nvideo understanding. An iterated amplification strategy is adopted to perform\nalternating cyclic training in the video CoT system, where each component\nundergoes iterative cycles of refinement to improve its capabilities. ViaRL\nutilizes the answer accuracy of a downstream model as a reward signal to train\na frame selector through trial-and-error, eliminating the need for expensive\nannotations while closely aligning with human-like learning processes.\nComprehensive experiments across multiple benchmarks, including VideoMME,\nLVBench, and MLVU, demonstrate that ViaRL consistently delivers superior\ntemporal grounding performance and robust generalization across diverse video\nunderstanding tasks, highlighting its effectiveness and scalability. Notably,\nViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which\nis required to search a specific needle within a long video and regarded as one\nof the most suitable benchmarks for evaluating temporal grounding.\n","authors":["Ziqiang Xu","Qi Dai","Tian Xie","Yifan Yang","Kai Qiu","DongDong Chen","Zuxuan Wu","Chong Luo"],"pdf_url":"https://arxiv.org/pdf/2505.15447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15444v1","updated":"2025-05-21T12:25:12Z","published":"2025-05-21T12:25:12Z","title":"Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation\n  Framework Using Role-Specific Token Optimization","summary":"  Existing studies have optimized retrieval-augmented generation (RAG) across\nvarious sub-tasks, such as query understanding and retrieval refinement, but\nintegrating these optimizations into a unified framework remains challenging.\nTo tackle this problem, this work proposes RoleRAG, a unified RAG framework\nthat achieves efficient multi-task processing through role-specific token\noptimization. RoleRAG comprises six modules, each handling a specific sub-task\nwithin the RAG process. Additionally, we introduce a query graph to represent\nthe decomposition of the query, which can be dynamically resolved according to\nthe decomposing state. All modules are driven by the same underlying LLM,\ndistinguished by task-specific role tokens that are individually optimized.\nThis design allows RoleRAG to dynamically activate different modules within a\nsingle LLM instance, thereby streamlining deployment and reducing resource\nconsumption. Experimental results on five open-domain question-answering\ndatasets demonstrate the effectiveness, generalizability, and flexibility of\nour framework.\n","authors":["Yutao Zhu","Jiajie Jin","Hongjin Qian","Zheng Liu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.15444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15433v1","updated":"2025-05-21T12:14:26Z","published":"2025-05-21T12:14:26Z","title":"Set-LLM: A Permutation-Invariant LLM","summary":"  While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity.\n","authors":["Beni Egressy","Jan Stühmer"],"pdf_url":"https://arxiv.org/pdf/2505.15433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15429v1","updated":"2025-05-21T12:11:07Z","published":"2025-05-21T12:11:07Z","title":"Uncertainty Quantification in SVM prediction","summary":"  This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments.\n","authors":["Pritam Anand"],"pdf_url":"https://arxiv.org/pdf/2505.15429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15427v1","updated":"2025-05-21T12:10:26Z","published":"2025-05-21T12:10:26Z","title":"Responsible Diffusion Models via Constraining Text Embeddings within\n  Safe Regions","summary":"  The remarkable ability of diffusion models to generate high-fidelity images\nhas led to their widespread adoption. However, concerns have also arisen\nregarding their potential to produce Not Safe for Work (NSFW) content and\nexhibit social biases, hindering their practical use in real-world\napplications. In response to this challenge, prior work has focused on\nemploying security filters to identify and exclude toxic text, or\nalternatively, fine-tuning pre-trained diffusion models to erase sensitive\nconcepts. Unfortunately, existing methods struggle to achieve satisfactory\nperformance in the sense that they can have a significant impact on the normal\nmodel output while still failing to prevent the generation of harmful content\nin some cases. In this paper, we propose a novel self-discovery approach to\nidentifying a semantic direction vector in the embedding space to restrict text\nembedding within a safe region. Our method circumvents the need for correcting\nindividual words within the input text and steers the entire text prompt\ntowards a safe region in the embedding space, thereby enhancing model\nrobustness against all possibly unsafe prompts. In addition, we employ Low-Rank\nAdaptation (LoRA) for semantic direction vector initialization to reduce the\nimpact on the model performance for other semantics. Furthermore, our method\ncan also be integrated with existing methods to improve their social\nresponsibility. Extensive experiments on benchmark datasets demonstrate that\nour method can effectively reduce NSFW content and mitigate social bias\ngenerated by diffusion models compared to several state-of-the-art baselines.\n","authors":["Zhiwen Li","Die Chen","Mingyuan Fan","Cen Chen","Yaliang Li","Yanhao Wang","Wenmeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.15427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15420v1","updated":"2025-05-21T12:04:42Z","published":"2025-05-21T12:04:42Z","title":"Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems\n  through Benign Queries","summary":"  Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems.\n","authors":["Yuhao Wang","Wenjie Qu","Yanze Jiang","Zichen Liu","Yue Liu","Shengfang Zhai","Yinpeng Dong","Jiaheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15418v1","updated":"2025-05-21T12:01:08Z","published":"2025-05-21T12:01:08Z","title":"Guided Policy Optimization under Partial Observability","summary":"  Reinforcement Learning (RL) in partially observable environments poses\nsignificant challenges due to the complexity of learning under uncertainty.\nWhile additional information, such as that available in simulations, can\nenhance training, effectively leveraging it remains an open problem. To address\nthis, we introduce Guided Policy Optimization (GPO), a framework that co-trains\na guider and a learner. The guider takes advantage of privileged information\nwhile ensuring alignment with the learner's policy that is primarily trained\nvia imitation learning. We theoretically demonstrate that this learning scheme\nachieves optimality comparable to direct RL, thereby overcoming key limitations\ninherent in existing approaches. Empirical evaluations show strong performance\nof GPO across various tasks, including continuous control with partial\nobservability and noise, and memory-based challenges, significantly\noutperforming existing methods.\n","authors":["Yueheng Li","Guangming Xie","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2505.15418v1.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2504.21659v2","updated":"2025-05-21T11:59:38Z","published":"2025-04-30T14:01:45Z","title":"Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization","summary":"  Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\n(Ada-R1) significantly reduces inference costs compared to other baseline\napproaches, while maintaining performance. Notably, on five mathematical\ndatasets, the average length of reasoning is reduced by more than 50%,\nhighlighting the potential of adaptive strategies to optimize reasoning\nefficiency in large language models. Our code is coming soon at\nhttps://github.com/StarDewXXX/AdaR1\n","authors":["Haotian Luo","Haiying He","Yibo Wang","Jinluan Yang","Rui Liu","Naiqiang Tan","Xiaochun Cao","Dacheng Tao","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2504.21659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15410v1","updated":"2025-05-21T11:52:57Z","published":"2025-05-21T11:52:57Z","title":"ClickSight: Interpreting Student Clickstreams to Reveal Insights on\n  Learning Strategies via LLMs","summary":"  Clickstream data from digital learning environments offer valuable insights\ninto students' learning behaviors, but are challenging to interpret due to\ntheir high dimensionality and granularity. Prior approaches have relied mainly\non handcrafted features, expert labeling, clustering, or supervised models,\ntherefore often lacking generalizability and scalability. In this work, we\nintroduce ClickSight, an in-context Large Language Model (LLM)-based pipeline\nthat interprets student clickstreams to reveal their learning strategies.\nClickSight takes raw clickstreams and a list of learning strategies as input\nand generates textual interpretations of students' behaviors during\ninteraction. We evaluate four different prompting strategies and investigate\nthe impact of self-refinement on interpretation quality. Our evaluation spans\ntwo open-ended learning environments and uses a rubric-based domain-expert\nevaluation. Results show that while LLMs can reasonably interpret learning\nstrategies from clickstreams, interpretation quality varies by prompting\nstrategy, and self-refinement offers limited improvement. ClickSight\ndemonstrates the potential of LLMs to generate theory-driven insights from\neducational interaction data.\n","authors":["Bahar Radmehr","Ekaterina Shved","Fatma Betül Güreş","Adish Singla","Tanja Käser"],"pdf_url":"https://arxiv.org/pdf/2505.15410v1.pdf","comment":"Accepted in Latebreaking results track in AIED 2025(26th\n  International Conference on Artificial Intelligence in Education JULY 22-26,\n  2025 PALERMO, ITALY)"},{"id":"http://arxiv.org/abs/2505.15406v1","updated":"2025-05-21T11:47:47Z","published":"2025-05-21T11:47:47Z","title":"Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large\n  Audio-Language Models","summary":"  The rise of Large Audio Language Models (LAMs) brings both potential and\nrisks, as their audio outputs may contain harmful or unethical content.\nHowever, current research lacks a systematic, quantitative evaluation of LAM\nsafety especially against jailbreak attacks, which are challenging due to the\ntemporal and semantic nature of speech. To bridge this gap, we introduce\nAJailBench, the first benchmark specifically designed to evaluate jailbreak\nvulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of\n1,495 adversarial audio prompts spanning 10 policy-violating categories,\nconverted from textual jailbreak attacks using realistic text to speech\nsynthesis. Using this dataset, we evaluate several state-of-the-art LAMs and\nreveal that none exhibit consistent robustness across attacks. To further\nstrengthen jailbreak testing and simulate more realistic attack conditions, we\npropose a method to generate dynamic adversarial variants. Our Audio\nPerturbation Toolkit (APT) applies targeted distortions across time, frequency,\nand amplitude domains. To preserve the original jailbreak intent, we enforce a\nsemantic consistency constraint and employ Bayesian optimization to efficiently\nsearch for perturbations that are both subtle and highly effective. This\nresults in AJailBench-APT, an extended dataset of optimized adversarial audio\nsamples. Our findings demonstrate that even small, semantically preserved\nperturbations can significantly reduce the safety performance of leading LAMs,\nunderscoring the need for more robust and semantically aware defense\nmechanisms.\n","authors":["Zirui Song","Qian Jiang","Mingxuan Cui","Mingzhe Li","Lang Gao","Zeyu Zhang","Zixiang Xu","Yanbo Wang","Chenxi Wang","Guangxian Ouyang","Zhenhao Chen","Xiuying Chen"],"pdf_url":"https://arxiv.org/pdf/2505.15406v1.pdf","comment":"We release AJailBench, including both static and optimized\n  adversarial data, to facilitate future research:\n  https://github.com/mbzuai-nlp/AudioJailbreak"},{"id":"http://arxiv.org/abs/2505.15400v1","updated":"2025-05-21T11:41:39Z","published":"2025-05-21T11:41:39Z","title":"When to Continue Thinking: Adaptive Thinking Mode Switching for\n  Efficient Reasoning","summary":"  Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.\n","authors":["Xiaoyun Zhang","Jingqing Ruan","Xing Ma","Yawen Zhu","Haodong Zhao","Hao Li","Jiansong Chen","Ke Zeng","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2505.15400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11091v2","updated":"2025-05-21T11:33:14Z","published":"2025-04-15T11:36:27Z","title":"AI-guided Antibiotic Discovery Pipeline from Target Selection to\n  Compound Identification","summary":"  Antibiotic resistance presents a growing global health crisis, demanding new\ntherapeutic strategies that target novel bacterial mechanisms. Recent advances\nin protein structure prediction and machine learning-driven molecule generation\noffer a promising opportunity to accelerate drug discovery. However, practical\nguidance on selecting and integrating these models into real-world pipelines\nremains limited. In this study, we develop an end-to-end, artificial\nintelligence-guided antibiotic discovery pipeline that spans target\nidentification to compound realization. We leverage structure-based clustering\nacross predicted proteomes of multiple pathogens to identify conserved,\nessential, and non-human-homologous targets. We then systematically evaluate\nsix leading 3D-structure-aware generative models$\\unicode{x2014}$spanning\ndiffusion, autoregressive, graph neural network, and language model\narchitectures$\\unicode{x2014}$on their usability, chemical validity, and\nbiological relevance. Rigorous post-processing filters and commercial analogue\nsearches reduce over 100 000 generated compounds to a focused, synthesizable\nset. Our results highlight DeepBlock and TamGen as top performers across\ndiverse criteria, while also revealing critical trade-offs between model\ncomplexity, usability, and output quality. This work provides a comparative\nbenchmark and blueprint for deploying artificial intelligence in early-stage\nantibiotic development.\n","authors":["Maximilian G. Schuh","Joshua Hesse","Stephan A. Sieber"],"pdf_url":"https://arxiv.org/pdf/2504.11091v2.pdf","comment":"12 pages, preprint"},{"id":"http://arxiv.org/abs/2501.14513v2","updated":"2025-05-21T11:27:06Z","published":"2025-01-24T14:18:22Z","title":"ABPT: Amended Backpropagation through Time with Partially Differentiable\n  Rewards","summary":"  Quadrotor control policies can be trained with high performance using the\nexact gradients of the rewards to directly optimize policy parameters via\nbackpropagation-through-time (BPTT). However, designing a fully differentiable\nreward architecture is often challenging. Partially differentiable rewards will\nresult in biased gradient propagation that degrades training performance. To\novercome this limitation, we propose Amended Backpropagation-through-Time\n(ABPT), a novel approach that mitigates gradient bias while preserving the\ntraining efficiency of BPTT. ABPT combines 0-step and N-step returns,\neffectively reducing the bias by leveraging value gradients from the learned\nQ-value function. Additionally, it adopts entropy regularization and state\ninitialization mechanisms to encourage exploration during training. We evaluate\nABPT on four representative quadrotor flight tasks \\li{in both real world and\nsimulation}. Experimental results demonstrate that ABPT converges significantly\nfaster and achieves higher ultimate rewards than existing learning algorithms,\nparticularly in tasks involving partially differentiable rewards. The code will\nbe released at http://github.com/Fanxing-LI/ABPT.\n","authors":["Fanxing Li","Fangyu Sun","Tianbao Zhang","Danping Zou"],"pdf_url":"https://arxiv.org/pdf/2501.14513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15386v1","updated":"2025-05-21T11:23:05Z","published":"2025-05-21T11:23:05Z","title":"RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection","summary":"  Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.\n","authors":["Yiming Huang","Junyan Zhang","Zihao Wang","Biquan Bie","Xuming Hu","Yi R."," Fung","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2505.15386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15380v1","updated":"2025-05-21T11:17:04Z","published":"2025-05-21T11:17:04Z","title":"Accelerating Autoregressive Speech Synthesis Inference With Speech\n  Speculative Decoding","summary":"  Modern autoregressive speech synthesis models leveraging language models have\ndemonstrated remarkable performance. However, the sequential nature of next\ntoken prediction in these models leads to significant latency, hindering their\ndeployment in scenarios where inference speed is critical. In this work, we\npropose Speech Speculative Decoding (SSD), a novel framework for autoregressive\nspeech synthesis acceleration. Specifically, our method employs a lightweight\ndraft model to generate candidate token sequences, which are subsequently\nverified in parallel by the target model using the proposed SSD framework.\nExperimental results demonstrate that SSD achieves a significant speedup of\n1.4x compared with conventional autoregressive decoding, while maintaining high\nfidelity and naturalness. Subjective evaluations further validate the\neffectiveness of SSD in preserving the perceptual quality of the target model\nwhile accelerating inference.\n","authors":["Zijian Lin","Yang Zhang","Yougen Yuan","Yuming Yan","Jinjiang Liu","Zhiyong Wu","Pengfei Hu","Qun Yu"],"pdf_url":"https://arxiv.org/pdf/2505.15380v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.14147v2","updated":"2025-05-21T11:15:03Z","published":"2025-05-20T09:54:42Z","title":"SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large\n  Reasoning Models Reinforcement Learning","summary":"  Training large reasoning models (LRMs) with reinforcement learning in STEM\ndomains is hindered by the scarcity of high-quality, diverse, and verifiable\nproblem sets. Existing synthesis methods, such as Chain-of-Thought prompting,\noften generate oversimplified or uncheckable data, limiting model advancement\non complex tasks. To address these challenges, we introduce SHARP, a unified\napproach to Synthesizing High-quality Aligned Reasoning Problems for LRMs\nreinforcement learning with verifiable rewards (RLVR). SHARP encompasses a\nstrategic set of self-alignment principles -- targeting graduate and\nOlympiad-level difficulty, rigorous logical consistency, and unambiguous,\nverifiable answers -- and a structured three-phase framework (Alignment,\nInstantiation, Inference) that ensures thematic diversity and fine-grained\ncontrol over problem generation. We implement SHARP by leveraging a\nstate-of-the-art LRM to infer and verify challenging STEM questions, then\nemploy a reinforcement learning loop to refine the model's reasoning through\nverifiable reward signals. Experiments on benchmarks such as GPQA demonstrate\nthat SHARP-augmented training substantially outperforms existing methods,\nmarkedly improving complex reasoning accuracy and pushing LRM performance\ncloser to expert-level proficiency. Our contributions include the SHARP\nstrategy, framework design, end-to-end implementation, and experimental\nevaluation of its effectiveness in elevating LRM reasoning capabilities.\n","authors":["Xiong Jun Wu","Zhenduo Zhang","ZuJie Wen","Zhiqiang Zhang","Wang Ren","Lei Shi","Cai Chen","Deng Zhao","Dingnan Jin","Qing Cui","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08005v2","updated":"2025-05-21T11:06:04Z","published":"2025-01-14T10:49:26Z","title":"DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But\n  Only If You Can Trust Them","summary":"  Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code will be made publicly available\n","authors":["Francisco Caetano","Christiaan Viviers","Luis A. Zavala-Mondragón","Peter H. N. de With","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2501.08005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15367v1","updated":"2025-05-21T10:57:40Z","published":"2025-05-21T10:57:40Z","title":"Better Safe Than Sorry? Overreaction Problem of Vision Language Models\n  in Visual Emergency Recognition","summary":"  Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.\n","authors":["Dasol Choi","Seunghyun Lee","Youngsook Song"],"pdf_url":"https://arxiv.org/pdf/2505.15367v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2407.11104v2","updated":"2025-05-21T10:55:54Z","published":"2024-07-15T14:28:50Z","title":"Exploring the Potentials and Challenges of Deep Generative Models in\n  Product Design Conception","summary":"  The synthesis of product design concepts stands at the crux of early-phase\ndevelopment processes for technical products, traditionally posing an intricate\ninterdisciplinary challenge. The application of deep learning methods,\nparticularly Deep Generative Models (DGMs), holds the promise of automating and\nstreamlining manual iterations and therefore introducing heightened levels of\ninnovation and efficiency. However, DGMs have yet to be widely adopted into the\nsynthesis of product design concepts. This paper aims to explore the reasons\nbehind this limited application and derive the requirements for successful\nintegration of these technologies. We systematically analyze DGM-families (VAE,\nGAN, Diffusion, Transformer, Radiance Field), assessing their strengths,\nweaknesses, and general applicability for product design conception. Our\nobjective is to provide insights that simplify the decision-making process for\nengineers, helping them determine which method might be most effective for\ntheir specific challenges. Recognizing the rapid evolution of this field, we\nhope that our analysis contributes to a fundamental understanding and guides\npractitioners towards the most promising approaches. This work seeks not only\nto illuminate current challenges but also to propose potential solutions,\nthereby offering a clear roadmap for leveraging DGMs in the realm of product\ndesign conception.\n","authors":["Phillip Mueller","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2407.11104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04715v5","updated":"2025-05-21T10:48:37Z","published":"2025-03-06T18:58:29Z","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining","summary":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/\n","authors":["Houyi Li","Wenzhen Zheng","Qiufeng Wang","Hanshan Zhang","Zili Wang","Shijie Xuyang","Yuantao Fan","Shuigeng Zhou","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.04715v5.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2405.11783v2","updated":"2025-05-21T10:43:52Z","published":"2024-05-20T05:02:12Z","title":"Inverse Design of Metal-Organic Frameworks Using Quantum Natural\n  Language Processing","summary":"  In this study, we explore the potential of using quantum natural language\nprocessing (QNLP) to inverse design metal-organic frameworks (MOFs) with\ntargeted properties. Specifically, by analyzing 450 hypothetical MOF structures\nconsisting of 3 topologies, 10 metal nodes and 15 organic ligands, we\ncategorize these structures into four distinct classes for pore volume and\n$CO_{2}$ Henry's constant values. We then compare various QNLP models (i.e. the\nbag-of-words, DisCoCat (Distributional Compositional Categorical), and\nsequence-based models) to identify the most effective approach to process the\nMOF dataset. Using a classical simulator provided by the IBM Qiskit, the\nbag-of-words model is identified to be the optimum model, achieving validation\naccuracies of 88.6% and 78.0% for binary classification tasks on pore volume\nand $CO_{2}$ Henry's constant, respectively. Further, we developed multi-class\nclassification models tailored to the probabilistic nature of quantum circuits,\nwith average test accuracies of 92% and 80% across different classes for pore\nvolume and $CO_{2}$ Henry's constant datasets. Finally, the performance of\ngenerating MOF with target properties showed accuracies of 93.5% for pore\nvolume and 87% for $CO_{2}$ Henry's constant, respectively. Although our\ninvestigation covers only a fraction of the vast MOF search space, it marks a\npromising first step towards using quantum computing for materials design,\noffering a new perspective through which to explore the complex landscape of\nMOFs.\n","authors":["Shinyoung Kang","Jihan Kim"],"pdf_url":"https://arxiv.org/pdf/2405.11783v2.pdf","comment":"46 pages, 7 figures, 6 supplementary figures, 1 table, 2\n  supplementary tables, 1 supplementary note"},{"id":"http://arxiv.org/abs/2502.01941v2","updated":"2025-05-21T10:37:50Z","published":"2025-02-04T02:23:06Z","title":"Can LLMs Maintain Fundamental Abilities under KV Cache Compression?","summary":"  This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.\n","authors":["Xiang Liu","Zhenheng Tang","Hong Chen","Peijie Dong","Zeyu Li","Xiuze Zhou","Bo Li","Xuming Hu","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2502.01941v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2504.04045v2","updated":"2025-05-21T10:27:37Z","published":"2025-04-05T03:44:09Z","title":"A Survey of Pathology Foundation Model: Progress and Future Directions","summary":"  Computational pathology, which involves analyzing whole slide images for\nautomated cancer diagnosis, relies on multiple instance learning, where\nperformance depends heavily on the feature extractor and aggregator. Recent\nPathology Foundation Models (PFMs), pretrained on large-scale histopathology\ndata, have significantly enhanced both the extractor and aggregator, but they\nlack a systematic analysis framework. In this survey, we present a hierarchical\ntaxonomy organizing PFMs through a top-down philosophy applicable to foundation\nmodel analysis in any domain: model scope, model pretraining, and model design.\nAdditionally, we systematically categorize PFM evaluation tasks into\nslide-level, patch-level, multimodal, and biological tasks, providing\ncomprehensive benchmarking criteria. Our analysis identifies critical\nchallenges in both PFM development (pathology-specific methodology, end-to-end\npretraining, data-model scalability) and utilization (effective adaptation,\nmodel maintenance), paving the way for future directions in this promising\nfield. Resources referenced in this survey are available at\nhttps://github.com/BearCleverProud/AwesomeWSI.\n","authors":["Conghao Xiong","Hao Chen","Joseph J. Y. Sung"],"pdf_url":"https://arxiv.org/pdf/2504.04045v2.pdf","comment":"Accepted to IJCAI 2025 Survey Track, 10 Pages"},{"id":"http://arxiv.org/abs/2505.15345v1","updated":"2025-05-21T10:19:49Z","published":"2025-05-21T10:19:49Z","title":"Hadamax Encoding: Elevating Performance in Model-Free Atari","summary":"  Neural network architectures have a large impact in machine learning. In\nreinforcement learning, network architectures have remained notably simple, as\nchanges often lead to small gains in performance. This work introduces a novel\nencoder architecture for pixel-based model-free reinforcement learning. The\nHadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves\nstate-of-the-art performance by max-pooling Hadamard products between\nGELU-activated parallel hidden layers. Based on the recent PQN algorithm, the\nHadamax encoder achieves state-of-the-art model-free performance in the\nAtari-57 benchmark. Specifically, without applying any algorithmic\nhyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain\nover vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,\nthe full code is available on\n\\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.\n","authors":["Jacob E. Kooi","Zhao Yang","Vincent François-Lavet"],"pdf_url":"https://arxiv.org/pdf/2505.15345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02863v2","updated":"2025-05-21T10:19:16Z","published":"2025-02-05T03:45:33Z","title":"OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable\n  Attitude and Behavior Change","summary":"  Marine ecosystems face unprecedented threats from climate change and plastic\npollution, yet traditional environmental education often struggles to translate\nawareness into sustained behavioral change. This paper presents OceanChat, an\ninteractive system leveraging large language models to create conversational AI\nagents represented as animated marine creatures -- specifically a beluga whale,\na jellyfish, and a seahorse -- designed to promote environmental behavior (PEB)\nand foster awareness through personalized dialogue. Through a between-subjects\nexperiment (N=900), we compared three conditions: (1) Static Scientific\nInformation, providing conventional environmental education through text and\nimages; (2) Static Character Narrative, featuring first-person storytelling\nfrom 3D-rendered marine creatures; and (3) Conversational Character Narrative,\nenabling real-time dialogue with AI-powered marine characters. Our analysis\nrevealed that the Conversational Character Narrative condition significantly\nincreased behavioral intentions and sustainable choice preferences compared to\nstatic approaches. The beluga whale character demonstrated consistently\nstronger emotional engagement across multiple measures, including perceived\nanthropomorphism and empathy. However, impacts on deeper measures like climate\npolicy support and psychological distance were limited, highlighting the\ncomplexity of shifting entrenched beliefs. Our work extends research on\nsustainability interfaces facilitating PEB and offers design principles for\ncreating emotionally resonant, context-aware AI characters. By balancing\nanthropomorphism with species authenticity, OceanChat demonstrates how\ninteractive narratives can bridge the gap between environmental knowledge and\nreal-world behavior change.\n","authors":["Pat Pataranutaporn","Alexander Doudkin","Pattie Maes"],"pdf_url":"https://arxiv.org/pdf/2502.02863v2.pdf","comment":"21 pages, 18 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.15344v1","updated":"2025-05-21T10:18:49Z","published":"2025-05-21T10:18:49Z","title":"Alpay Algebra: A Universal Structural Foundation","summary":"  Alpay Algebra is introduced as a universal, category-theoretic framework that\nunifies classical algebraic structures with modern needs in symbolic recursion\nand explainable AI. Starting from a minimal list of axioms, we model each\nalgebra as an object in a small cartesian closed category $\\mathcal{A}$ and\ndefine a transfinite evolution functor $\\phi\\colon\\mathcal{A}\\to\\mathcal{A}$.\nWe prove that the fixed point $\\phi^{\\infty}$ exists for every initial object\nand satisfies an internal universal property that recovers familiar constructs\n-- limits, colimits, adjunctions -- while extending them to ordinal-indexed\nfolds. A sequence of theorems establishes (i) soundness and conservativity over\nstandard universal algebra, (ii) convergence of $\\phi$-iterates under regular\ncardinals, and (iii) an explanatory correspondence between $\\phi^{\\infty}$ and\nminimal sufficient statistics in information-theoretic AI models. We conclude\nby outlining computational applications: type-safe functional languages,\ncategorical model checking, and signal-level reasoning engines that leverage\nAlpay Algebra's structural invariants. All proofs are self-contained; no\nexternal set-theoretic axioms beyond ZFC are required. This exposition\npositions Alpay Algebra as a bridge between foundational mathematics and\nhigh-impact AI systems, and provides a reference for further work in category\ntheory, transfinite fixed-point analysis, and symbolic computation.\n","authors":["Faruk Alpay"],"pdf_url":"https://arxiv.org/pdf/2505.15344v1.pdf","comment":"37 pages, 0 figures. Self-contained categorical framework built\n  directly on Mac Lane and Bourbaki; minimal references are intentional to\n  foreground the new construction"},{"id":"http://arxiv.org/abs/2505.15337v1","updated":"2025-05-21T10:08:39Z","published":"2025-05-21T10:08:39Z","title":"Your Language Model Can Secretly Write Like Humans: Contrastive\n  Paraphrase Attacks on LLM-Generated Text Detectors","summary":"  The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios.\n","authors":["Hao Fang","Jiawei Kong","Tianqu Zhuang","Yixiang Qiu","Kuofeng Gao","Bin Chen","Shu-Tao Xia","Yaowei Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15333v1","updated":"2025-05-21T10:05:25Z","published":"2025-05-21T10:05:25Z","title":"Leveraging Unit Language Guidance to Advance Speech Modeling in Textless\n  Speech-to-Speech Translation","summary":"  The success of building textless speech-to-speech translation (S2ST) models\nhas attracted much attention. However, S2ST still faces two main challenges: 1)\nextracting linguistic features for various speech signals, called cross-modal\n(CM), and 2) learning alignment of difference languages in long sequences,\ncalled cross-lingual (CL). We propose the unit language to overcome the two\nmodeling challenges. The unit language can be considered a text-like\nrepresentation format, constructed using $n$-gram language modeling. We\nimplement multi-task learning to utilize the unit language in guiding the\nspeech modeling process. Our initial results reveal a conflict when applying\nsource and target unit languages simultaneously. We propose task prompt\nmodeling to mitigate this conflict. We conduct experiments on four languages of\nthe Voxpupil dataset. Our method demonstrates significant improvements over a\nstrong baseline and achieves performance comparable to models trained with\ntext.\n","authors":["Yuhao Zhang","Xiangnan Ma","Kaiqi Kou","Peizhuo Liu","Weiqiao Shan","Benyou Wang","Tong Xiao","Yuxin Huang","Zhengtao Yu","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.15333v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.08728v2","updated":"2025-05-21T09:45:04Z","published":"2025-05-13T16:39:00Z","title":"Securing RAG: A Risk Assessment and Mitigation Framework","summary":"  Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.\n","authors":["Lukas Ammann","Sara Ott","Christoph R. Landolt","Marco P. Lehmann"],"pdf_url":"https://arxiv.org/pdf/2505.08728v2.pdf","comment":"8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally.\n  This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2505.15311v1","updated":"2025-05-21T09:41:53Z","published":"2025-05-21T09:41:53Z","title":"Trajectory Bellman Residual Minimization: A Simple Value-Based Method\n  for LLM Reasoning","summary":"  Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs.\n","authors":["Yurun Yuan","Fan Chen","Zeyu Jia","Alexander Rakhlin","Tengyang Xie"],"pdf_url":"https://arxiv.org/pdf/2505.15311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15308v1","updated":"2025-05-21T09:36:35Z","published":"2025-05-21T09:36:35Z","title":"BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution","summary":"  With the widespread application of super-resolution (SR) in various fields,\nresearchers have begun to investigate its security. Previous studies have\ndemonstrated that SR models can also be subjected to backdoor attacks through\ndata poisoning, affecting downstream tasks. A backdoor SR model generates an\nattacker-predefined target image when given a triggered image while producing a\nnormal high-resolution (HR) output for clean images. However, prior backdoor\nattacks on SR models have primarily focused on the stealthiness of poisoned\nlow-resolution (LR) images while ignoring the stealthiness of poisoned HR\nimages, making it easy for users to detect anomalous data. To address this\nproblem, we propose BadSR, which improves the stealthiness of poisoned HR\nimages. The key idea of BadSR is to approximate the clean HR image and the\npre-defined target image in the feature space while ensuring that modifications\nto the clean HR image remain within a constrained range. The poisoned HR images\ngenerated by BadSR can be integrated with existing triggers. To further improve\nthe effectiveness of BadSR, we design an adversarially optimized trigger and a\nbackdoor gradient-driven poisoned sample selection method based on a genetic\nalgorithm. The experimental results show that BadSR achieves a high attack\nsuccess rate in various models and data sets, significantly affecting\ndownstream tasks.\n","authors":["Ji Guo","Xiaolei Wen","Wenbo Jiang","Cheng Huang","Jinjin Li","Hongwei Li"],"pdf_url":"https://arxiv.org/pdf/2505.15308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15306v1","updated":"2025-05-21T09:35:43Z","published":"2025-05-21T09:35:43Z","title":"Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak\n  Reinforcement Learning Agents into a Supreme One","summary":"  Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE.\n","authors":["Yiwen Song","Qianyue Hao","Qingmin Liao","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2505.15306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15303v1","updated":"2025-05-21T09:34:27Z","published":"2025-05-21T09:34:27Z","title":"Laplace Sample Information: Data Informativeness Through a Bayesian Lens","summary":"  Accurately estimating the informativeness of individual samples in a dataset\nis an important objective in deep learning, as it can guide sample selection,\nwhich can improve model efficiency and accuracy by removing redundant or\npotentially harmful samples. We propose Laplace Sample Information (LSI)\nmeasure of sample informativeness grounded in information theory widely\napplicable across model architectures and learning settings. LSI leverages a\nBayesian approximation to the weight posterior and the KL divergence to measure\nthe change in the parameter distribution induced by a sample of interest from\nthe dataset. We experimentally show that LSI is effective in ordering the data\nwith respect to typicality, detecting mislabeled samples, measuring class-wise\ninformativeness, and assessing dataset difficulty. We demonstrate these\ncapabilities of LSI on image and text data in supervised and unsupervised\nsettings. Moreover, we show that LSI can be computed efficiently through probes\nand transfers well to the training of large models.\n","authors":["Johannes Kaiser","Kristian Schwethelm","Daniel Rueckert","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2505.15303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15293v1","updated":"2025-05-21T09:24:23Z","published":"2025-05-21T09:24:23Z","title":"LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration\n  Enhancement Driven by Large Language Models","summary":"  Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility.\n","authors":["Qianyue Hao","Yiwen Song","Qingmin Liao","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2505.15293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15285v1","updated":"2025-05-21T09:10:31Z","published":"2025-05-21T09:10:31Z","title":"Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction","summary":"  Mesh reconstruction is a cornerstone process across various applications,\nincluding in-silico trials, digital twins, surgical planning, and navigation.\nRecent advancements in deep learning have notably enhanced mesh reconstruction\nspeeds. Yet, traditional methods predominantly rely on deforming a standardised\ntemplate mesh for individual subjects, which overlooks the unique anatomical\nvariations between them, and may compromise the fidelity of the\nreconstructions. In this paper, we propose an adaptive-template-based mesh\nreconstruction network (ATMRN), which generates adaptive templates from the\ngiven images for the subsequent deformation, moving beyond the constraints of a\nsingular, fixed template. Our approach, validated on cortical magnetic\nresonance (MR) images from the OASIS dataset, sets a new benchmark in\nvoxel-to-cortex mesh reconstruction, achieving an average symmetric surface\ndistance of 0.267mm across four cortical structures. Our proposed method is\ngeneric and can be easily transferred to other image modalities and anatomical\nstructures.\n","authors":["Fengting Zhang","Boxu Liang","Qinghao Liu","Min Liu","Xiang Chen","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.15285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14350v3","updated":"2025-05-21T09:05:29Z","published":"2025-04-19T16:32:28Z","title":"An Empirical Study of LLM Reasoning Ability Under Strict Output Length\n  Constraint","summary":"  Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making models think before answering,\nthey are able to achieve much higher accuracy with extra inference computation.\nHowever, in many real-world scenarios, models are used under time constraints,\nwhere an answer should be given within a certain output length. It is unclear\nwhether and how the reasoning ability of different LLMs remain effective under\nstrict constraints. We take a first look at this problem by conducting an\nin-depth empirical study. Specifically, we test 30 LLMs on common reasoning\ndatasets under a wide range of output length budgets, and we analyze the\ncorrelation between the inference accuracy and various properties including\nmodel type, model size, prompt style, etc. We also consider the mappings\nbetween token budgets and actual on-device latency budgets. The results have\ndemonstrated several interesting findings regarding the budget-aware LLM\nreasoning ability that differ from the unconstrained situation, e.g. the\noptimal choices of either model size or prompt style change under different\nbudgets. These findings offer timely evaluation to this area and practical\nguidance for users to deploy LLMs under real-world latency constraints.\n","authors":["Yi Sun","Han Wang","Jiaqiang Li","Jiacheng Liu","Xiangyu Li","Hao Wen","Yizhen Yuan","Huiwen Zheng","Yan Liang","Yuanchun Li","Yunxin Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14350v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12128v3","updated":"2025-05-21T08:58:58Z","published":"2025-02-17T18:49:13Z","title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked\n  Entities","summary":"  Generative models are spearheading recent progress in deep learning,\nshowcasing strong promise for trajectory sampling in dynamical systems as well.\nHowever, whereas latent space modeling paradigms have transformed image and\nvideo generation, similar approaches are more difficult for most dynamical\nsystems. Such systems -- from chemical molecule structures to collective human\nbehavior -- are described by interactions of entities, making them inherently\nlinked to connectivity patterns, entity conservation, and the traceability of\nentities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial\nDynamical Systems via Linked Entities), bridges the gap between: (1) keeping\nthe traceability of individual entities in a latent system representation, and\n(2) leveraging the efficiency and scalability of recent advances in image and\nvideo generation, where pre-trained encoder and decoder enable generative\nmodeling directly in latent space. The core idea of LaM-SLidE is the\nintroduction of identifier representations (IDs) that enable the retrieval of\nentity properties and entity composition from latent system representations,\nthus fostering traceability. Experimentally, across different domains, we show\nthat LaM-SLidE performs favorably in terms of speed, accuracy, and\ngeneralizability. Code is available at https://github.com/ml-jku/LaM-SLidE .\n","authors":["Florian Sestak","Artur Toshev","Andreas Fürst","Günter Klambauer","Andreas Mayr","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.12128v3.pdf","comment":"Project page: https://ml-jku.github.io/LaM-SLidE/"},{"id":"http://arxiv.org/abs/2505.15276v1","updated":"2025-05-21T08:55:35Z","published":"2025-05-21T08:55:35Z","title":"When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of\n  Behavioral Divergence in Reasoning","summary":"  Large reasoning models (LRMs) have significantly advanced performance on\ncomplex tasks, yet their tendency to overthink introduces inefficiencies. This\nstudy investigates the internal mechanisms of reinforcement learning\n(RL)-trained LRMs when prompted to save thinking, revealing three distinct\nthinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking\n(IT). Through comprehensive analysis of confidence in thinking termination,\nattention from thinking to generation, and attentional focus on input sections,\nwe uncover key factors influencing the reasoning behaviors. We further find\nthat NT reduces output length at the cost of accuracy, while ET and IT maintain\naccuracy with reduced response length. Our findings expose fundamental\ninconsistencies in RL-optimized LRMs, necessitating adaptive improvements for\nreliable efficiency.\n","authors":["Rongzhi Zhu","Yi Liu","Zequn Sun","Yiwei Wang","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2505.15276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15275v1","updated":"2025-05-21T08:53:38Z","published":"2025-05-21T08:53:38Z","title":"Learning-based Autonomous Oversteer Control and Collision Avoidance","summary":"  Oversteer, wherein a vehicle's rear tires lose traction and induce\nunintentional excessive yaw, poses critical safety challenges. Failing to\ncontrol oversteer often leads to severe traffic accidents. Although recent\nautonomous driving efforts have attempted to handle oversteer through\nstabilizing maneuvers, the majority rely on expert-defined trajectories or\nassume obstacle-free environments, limiting real-world applicability. This\npaper introduces a novel end-to-end (E2E) autonomous driving approach that\ntackles oversteer control and collision avoidance simultaneously. Existing E2E\ntechniques, including Imitation Learning (IL), Reinforcement Learning (RL), and\nHybrid Learning (HL), generally require near-optimal demonstrations or\nextensive experience. Yet even skilled human drivers struggle to provide\nperfect demonstrations under oversteer, and high transition variance hinders\naccumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic\n(QC-SAC), a new HL algorithm that effectively learns from suboptimal\ndemonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we\nintroduce a benchmark inspired by real-world driver training: a vehicle\nencounters sudden oversteer on a slippery surface and must avoid randomly\nplaced obstacles ahead. Experimental results show QC-SAC attains near-optimal\ndriving policies, significantly surpassing state-of-the-art IL, RL, and HL\nbaselines. Our method demonstrates the world's first safe autonomous oversteer\ncontrol with obstacle avoidance.\n","authors":["Seokjun Lee","Seung-Hyun Kong"],"pdf_url":"https://arxiv.org/pdf/2505.15275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15274v1","updated":"2025-05-21T08:50:12Z","published":"2025-05-21T08:50:12Z","title":"Identification of Probabilities of Causation: A Complete\n  Characterization","summary":"  Probabilities of causation are fundamental to modern decision-making. Pearl\nfirst introduced three binary probabilities of causation, and Tian and Pearl\nlater derived tight bounds for them using Balke's linear programming. The\ntheoretical characterization of probabilities of causation with multi-valued\ntreatments and outcomes has remained unresolved for decades, limiting the scope\nof causality-based decision-making. In this paper, we resolve this foundational\ngap by proposing a complete set of representative probabilities of causation\nand proving that they are sufficient to characterize all possible probabilities\nof causation within the framework of Structural Causal Models (SCMs). We then\nformally derive tight bounds for these representative quantities using formal\nmathematical proofs. Finally, we demonstrate the practical relevance of our\nresults through illustrative toy examples.\n","authors":["Xin Shu","Shuai Wang","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2505.15274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20930v2","updated":"2025-05-21T08:50:11Z","published":"2025-04-29T16:48:23Z","title":"ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\n  through Step-by-Step Verification","summary":"  Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.\n","authors":["Ziqing Fan","Cheng Liang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2504.20930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15270v1","updated":"2025-05-21T08:49:03Z","published":"2025-05-21T08:49:03Z","title":"Scaling Diffusion Transformers Efficiently via $μ$P","summary":"  Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers.\n","authors":["Chenyu Zheng","Xinyu Zhang","Rongzhen Wang","Wei Huang","Zhi Tian","Weilin Huang","Jun Zhu","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2505.15270v1.pdf","comment":"35 pages, 10 figures, 15 tables"},{"id":"http://arxiv.org/abs/2505.15265v1","updated":"2025-05-21T08:45:43Z","published":"2025-05-21T08:45:43Z","title":"Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic\n  Concepts for LVLMs","summary":"  Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.\n","authors":["Zihao Pan","Yu Tong","Weibin Wu","Jingyi Wang","Lifeng Chen","Zhe Zhao","Jiajia Wei","Yitong Qiao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.15265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17778v2","updated":"2025-05-21T08:41:45Z","published":"2024-12-23T18:38:32Z","title":"From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based\n  Methodology","summary":"  Deep neural network (DNN)-based speech enhancement (SE) usually uses\nconventional activation functions, which lack the expressiveness to capture\ncomplex multiscale structures needed for high-fidelity SE. Group-Rational KAN\n(GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KAN's\nexpressiveness while improving scalability on complex tasks. We adapt GR-KAN to\nexisting DNN-based SE by replacing dense layers with GR-KAN layers in the\ntime-frequency (T-F) domain MP-SENet and adapting GR-KAN's activations into the\n1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that\nGR-KAN requires up to 4x fewer parameters while improving PESQ by up to 0.1. In\ncontrast, KAN, facing scalability issues, outperforms MLP on a small-scale\nsignal modeling task but fails to improve MP-SENet. We demonstrate the first\nsuccessful use of KAN-based methods for consistent improvement in both time-\nand SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE.\n","authors":["Haoyang Li","Yuchen Hu","Chen Chen","Sabato Marco Siniscalchi","Songting Liu","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2412.17778v2.pdf","comment":"Accepted to Interspeech2025"},{"id":"http://arxiv.org/abs/2412.06141v3","updated":"2025-05-21T08:39:51Z","published":"2024-12-09T01:50:39Z","title":"MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware\n  Multimodal Preference Optimization","summary":"  The advancement of Large Vision-Language Models (LVLMs) has propelled their\napplication in the medical field. However, Medical LVLMs (Med-LVLMs) encounter\nfactuality challenges due to modality misalignment, where the models prioritize\ntextual knowledge over visual input, leading to hallucinations that contradict\ninformation in medical images. Previous attempts to enhance modality alignment\nin Med-LVLMs through preference optimization have inadequately mitigated\nclinical relevance in preference data, making these samples easily\ndistinguishable and reducing alignment effectiveness. To address this\nchallenge, we propose MMedPO, a novel multimodal medical preference\noptimization approach that considers the clinical relevance of preference\nsamples to enhance Med-LVLM alignment. MMedPO curates multimodal preference\ndata by introducing two types of dispreference: (1) plausible hallucinations\ninjected through target Med-LVLMs or GPT-4o to produce medically inaccurate\nresponses, and (2) lesion region neglect achieved through local lesion-noising,\ndisrupting visual understanding of critical areas. We then calculate clinical\nrelevance for each sample based on scores from multiple Med-LLMs and visual\ntools, and integrate these scores into the preference optimization process as\nweights, enabling effective alignment. Our experiments demonstrate that MMedPO\nsignificantly enhances factual accuracy in Med-LVLMs, achieving substantial\nimprovements over existing preference optimization methods by averaging 14.2%\nand 51.7% across the Med-VQA and report generation tasks. Our code are\navailable in https://github.com/aiming-lab/MMedPO.\n","authors":["Kangyu Zhu","Peng Xia","Yun Li","Hongtu Zhu","Sheng Wang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2412.06141v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.15256v1","updated":"2025-05-21T08:34:13Z","published":"2025-05-21T08:34:13Z","title":"Zero-Shot Gaze-based Volumetric Medical Image Segmentation","summary":"  Accurate segmentation of anatomical structures in volumetric medical images\nis crucial for clinical applications, including disease monitoring and cancer\ntreatment planning. Contemporary interactive segmentation models, such as\nSegment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on\nmanually provided prompts like bounding boxes and mouse clicks. In this study,\nwe introduce eye gaze as a novel informational modality for interactive\nsegmentation, marking the application of eye-tracking for 3D medical image\nsegmentation. We evaluate the performance of using gaze-based prompts with\nSAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to\nbounding boxes, gaze-based prompts offer a time-efficient interaction approach\nwith slightly lower segmentation quality. Our findings highlight the potential\nof using gaze as a complementary input modality for interactive 3D medical\nimage segmentation.\n","authors":["Tatyana Shmykova","Leila Khaertdinova","Ilya Pershin"],"pdf_url":"https://arxiv.org/pdf/2505.15256v1.pdf","comment":"Accepted to MMFM-BIOMED Workshop @ CVPR 2025"},{"id":"http://arxiv.org/abs/2411.02126v3","updated":"2025-05-21T08:30:40Z","published":"2024-11-04T14:37:07Z","title":"Unsupervised detection of semantic correlations in big data","summary":"  In real-world data, information is stored in extremely large feature vectors.\nThese variables are typically correlated due to complex interactions involving\nmany features simultaneously. Such correlations qualitatively correspond to\nsemantic roles and are naturally recognized by both the human brain and\nartificial neural networks. This recognition enables, for instance, the\nprediction of missing parts of an image or text based on their context. We\npresent a method to detect these correlations in high-dimensional data\nrepresented as binary numbers. We estimate the binary intrinsic dimension of a\ndataset, which quantifies the minimum number of independent coordinates needed\nto describe the data, and is therefore a proxy of semantic complexity. The\nproposed algorithm is largely insensitive to the so-called curse of\ndimensionality, and can therefore be used in big data analysis. We test this\napproach identifying phase transitions in model magnetic systems and we then\napply it to the detection of semantic correlations of images and text inside\ndeep neural networks.\n","authors":["Santiago Acevedo","Alex Rodriguez","Alessandro Laio"],"pdf_url":"https://arxiv.org/pdf/2411.02126v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15250v1","updated":"2025-05-21T08:26:20Z","published":"2025-05-21T08:26:20Z","title":"Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty\n  Characterization and Pattern Classification","summary":"  Fuzzy rough feature selection (FRFS) is an effective means of addressing the\ncurse of dimensionality in high-dimensional data. By removing redundant and\nirrelevant features, FRFS helps mitigate classifier overfitting, enhance\ngeneralization performance, and lessen computational overhead. However, most\nexisting FRFS algorithms primarily focus on reducing uncertainty in pattern\nclassification, neglecting that lower uncertainty does not necessarily result\nin improved classification performance, despite it commonly being regarded as a\nkey indicator of feature selection effectiveness in the FRFS literature. To\nbridge uncertainty characterization and pattern classification, we propose a\nMargin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers\nboth the compactness and separation of label classes. MAFRFS effectively\nreduces uncertainty in pattern classification tasks, while guiding the feature\nselection towards more separable and discriminative label class structures.\nExtensive experiments on 15 public datasets demonstrate that MAFRFS is highly\nscalable and more effective than FRFS. The algorithms developed using MAFRFS\noutperform six state-of-the-art feature selection algorithms.\n","authors":["Suping Xu","Lin Shang","Keyu Liu","Hengrong Ju","Xibei Yang","Witold Pedrycz"],"pdf_url":"https://arxiv.org/pdf/2505.15250v1.pdf","comment":null}]},"2025-05-22T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2505.15242v2","updated":"2025-05-22T06:10:20Z","published":"2025-05-21T08:18:41Z","title":"Adaptive Plan-Execute Framework for Smart Contract Security Auditing","summary":"  Large Language Models (LLMs) have shown great promise in code analysis and\nauditing; however, they still struggle with hallucinations and limited\ncontext-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute\nframework that enhances smart contract security analysis through dynamic audit\nplanning and structured execution. Unlike conventional LLM-based auditing\napproaches that follow fixed workflows and predefined steps, SmartAuditFlow\ndynamically generates and refines audit plans based on the unique\ncharacteristics of each smart contract. It continuously adjusts its auditing\nstrategy in response to intermediate LLM outputs and newly detected\nvulnerabilities, ensuring a more adaptive and precise security assessment. The\nframework then executes these plans step by step, applying a structured\nreasoning process to enhance vulnerability detection accuracy while minimizing\nhallucinations and false positives. To further improve audit precision,\nSmartAuditFlow integrates iterative prompt optimization and external knowledge\nsources, such as static analysis tools and Retrieval-Augmented Generation\n(RAG). This ensures audit decisions are contextually informed and backed by\nreal-world security knowledge, producing comprehensive security reports.\nExtensive evaluations across multiple benchmarks demonstrate that\nSmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on\ncommon and critical vulnerabilities, 41.2 percent accuracy for comprehensive\ncoverage of known smart contract weaknesses in real-world projects, and\nsuccessfully identifying all 13 tested CVEs. These results highlight\nSmartAuditFlow's scalability, cost-effectiveness, and superior adaptability\nover traditional static analysis tools and contemporary LLM-based approaches,\nestablishing it as a robust solution for automated smart contract auditing.\n","authors":["Zhiyuan Wei","Jing Sun","Zijian Zhang","Zhe Hou","Zixiao Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.15242v2.pdf","comment":"30 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.13862v2","updated":"2025-05-22T03:49:11Z","published":"2025-05-20T03:14:57Z","title":"PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking\n  Attacks","summary":"  Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.\n","authors":["Guobin Shen","Dongcheng Zhao","Linghao Feng","Xiang He","Jihang Wang","Sicheng Shen","Haibo Tong","Yiting Dong","Jindong Li","Xiang Zheng","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.13862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12582v2","updated":"2025-05-22T07:26:07Z","published":"2025-05-19T00:05:18Z","title":"Compile-Time Fully Homomorphic Encryption of Vectors: Eliminating Online\n  Encryption via Algebraic Basis Synthesis","summary":"  We propose a framework for compile-time ciphertext synthesis in fully\nhomomorphic encryption (FHE) systems, where ciphertexts are constructed from\nprecomputed encrypted basis vectors combined with a runtime-scaled encryption\nof zero. This design eliminates online encryption and instead relies solely on\nciphertext-level additions and scalar multiplications, enabling efficient data\ningestion and algebraic reuse. We formalize the method as a randomized\n$\\mathbb{Z}_t$-module morphism and prove that it satisfies IND-CPA security\nunder standard assumptions. The proof uses a hybrid game reduction, showing\nthat adversarial advantage in distinguishing synthesized ciphertexts is\nnegligible if the underlying FHE scheme is IND-CPA secure. Unlike prior designs\nthat require a pool of random encryptions of zero, our construction achieves\nequivalent security using a single zero ciphertext multiplied by a fresh scalar\nat runtime, reducing memory overhead while preserving ciphertext randomness.\nThe resulting primitive supports efficient integration with standard FHE APIs\nand maintains compatibility with batching, rotation, and aggregation, making it\nwell-suited for encrypted databases, streaming pipelines, and secure compiler\nbackends.\n","authors":["Dongfang Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.12582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16957v1","updated":"2025-05-22T17:36:33Z","published":"2025-05-22T17:36:33Z","title":"Invisible Prompts, Visible Threats: Malicious Font Injection in External\n  Resources for Large Language Models","summary":"  Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.\n","authors":["Junjie Xiong","Changjia Zhu","Shuhang Lin","Chong Zhang","Yongfeng Zhang","Yao Liu","Lingyao Li"],"pdf_url":"https://arxiv.org/pdf/2505.16957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16916v1","updated":"2025-05-22T17:11:58Z","published":"2025-05-22T17:11:58Z","title":"Backdoor Cleaning without External Guidance in MLLM Fine-tuning","summary":"  Multimodal Large Language Models (MLLMs) are increasingly deployed in\nfine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt\ngeneral-purpose models to downstream tasks. This flexibility, however,\nintroduces serious security risks, as malicious fine-tuning can implant\nbackdoors into MLLMs with minimal effort. In this paper, we observe that\nbackdoor triggers systematically disrupt cross-modal processing by causing\nabnormal attention concentration on non-semantic regions--a phenomenon we term\nattention collapse. Based on this insight, we propose Believe Your Eyes (BYE),\na data filtering framework that leverages attention entropy patterns as\nself-supervised signals to identify and filter backdoor samples. BYE operates\nvia a three-stage pipeline: (1) extracting attention maps using the fine-tuned\nmodel, (2) computing entropy scores and profiling sensitive layers via bimodal\nseparation, and (3) performing unsupervised clustering to remove suspicious\nsamples. Unlike prior defenses, BYE equires no clean supervision, auxiliary\nlabels, or model modifications. Extensive experiments across various datasets,\nmodels, and diverse trigger types validate BYE's effectiveness: it achieves\nnear-zero attack success rates while maintaining clean-task performance,\noffering a robust and generalizable solution against backdoor threats in MLLMs.\n","authors":["Xuankun Rong","Wenke Huang","Jian Liang","Jinhe Bi","Xun Xiao","Yiming Li","Bo Du","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2505.16916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16888v1","updated":"2025-05-22T16:47:15Z","published":"2025-05-22T16:47:15Z","title":"CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious\n  System Prompt Generation and Refining Framework","summary":"  Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.\n","authors":["Viet Pham","Thai Le"],"pdf_url":"https://arxiv.org/pdf/2505.16888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16831v1","updated":"2025-05-22T16:02:10Z","published":"2025-05-22T16:02:10Z","title":"Unlearning Isn't Deletion: Investigating Reversibility of Machine\n  Unlearning in LLMs","summary":"  Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.\n","authors":["Xiaoyu Xu","Xiang Yue","Yang Liu","Qingqing Ye","Haibo Hu","Minxin Du"],"pdf_url":"https://arxiv.org/pdf/2505.16831v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2505.16785v1","updated":"2025-05-22T15:28:25Z","published":"2025-05-22T15:28:25Z","title":"CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of\n  Large Language Models","summary":"  Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification.\n","authors":["Zhenzhen Ren","GuoBiao Li","Sheng Li","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16765v1","updated":"2025-05-22T15:07:34Z","published":"2025-05-22T15:07:34Z","title":"When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak\n  Attack on LLMs via Steganographic Techniques","summary":"  Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66\n","authors":["Jianing Geng","Biao Yi","Zekun Fei","Tongxi Wu","Lihai Nie","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2505.16765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16737v1","updated":"2025-05-22T14:52:10Z","published":"2025-05-22T14:52:10Z","title":"Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing\n  Optimization","summary":"  The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.\n","authors":["Chengcan Wu","Zhixin Zhang","Zeming Wei","Yihao Zhang","Meng Sun"],"pdf_url":"https://arxiv.org/pdf/2505.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01822v4","updated":"2025-05-22T14:33:36Z","published":"2025-02-03T21:00:14Z","title":"Firewalls to Secure Dynamic LLM Agentic Networks","summary":"  LLM agents will likely communicate on behalf of users with other\nentity-representing agents on tasks involving long-horizon plans with\ninterdependent goals. Current work neglects these agentic networks and their\nchallenges. We identify required properties for agent communication:\nproactivity, adaptability, privacy (sharing only task-necessary information),\nand security (preserving integrity and utility against selfish entities). After\ndemonstrating communication vulnerabilities, we propose a practical design and\nprotocol inspired by network security principles. Our framework automatically\nderives task-specific rules from prior conversations to build firewalls. These\nfirewalls construct a closed language that is completely controlled by the\ndeveloper. They transform any personal data to the allowed degree of\npermissibility entailed by the task. Both operations are completely quarantined\nfrom external attackers, disabling the potential for prompt injections,\njailbreaks, or manipulation. By incorporating rules learned from their previous\nmistakes, agents rewrite their instructions and self-correct during\ncommunication. Evaluations on diverse attacks demonstrate our framework\nsignificantly reduces privacy and security vulnerabilities while allowing\nadaptability.\n","authors":["Sahar Abdelnabi","Amr Gomaa","Eugene Bagdasarian","Per Ola Kristensson","Reza Shokri"],"pdf_url":"https://arxiv.org/pdf/2502.01822v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16723v1","updated":"2025-05-22T14:32:23Z","published":"2025-05-22T14:32:23Z","title":"Robust LLM Fingerprinting via Domain-Specific Watermarks","summary":"  As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios.\n","authors":["Thibaud Gloaguen","Robin Staab","Nikola Jovanović","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2505.16723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02693v2","updated":"2025-05-22T14:09:01Z","published":"2024-10-03T17:18:37Z","title":"Discovering Spoofing Attempts on Language Model Watermarks","summary":"  LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. Despite recent work\ndemonstrating that state-of-the-art schemes are, in fact, vulnerable to\nspoofing, no prior work has focused on post-hoc methods to discover spoofing\nattempts. In this work, we for the first time propose a reliable statistical\nmethod to distinguish spoofed from genuinely watermarked text, suggesting that\ncurrent spoofing attacks are less effective than previously thought. In\nparticular, we show that regardless of their underlying approach, all current\nlearning-based spoofing methods consistently leave observable artifacts in\nspoofed texts, indicative of watermark forgery. We build upon these findings to\npropose rigorous statistical tests that reliably reveal the presence of such\nartifacts and thus demonstrate that a watermark has been spoofed. Our\nexperimental evaluation shows high test power across all learning-based\nspoofing methods, providing insights into their fundamental limitations and\nsuggesting a way to mitigate this threat. We make all our code available at\nhttps://github.com/eth-sri/watermark-spoofing-detection .\n","authors":["Thibaud Gloaguen","Nikola Jovanović","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.02693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20798v4","updated":"2025-05-22T14:09:00Z","published":"2024-12-30T08:43:28Z","title":"Reconciling Privacy and Explainability in High-Stakes: A Systematic\n  Inquiry","summary":"  Deep learning's preponderance across scientific domains has reshaped\nhigh-stakes decision-making, making it essential to follow rigorous operational\nframeworks that include both Right-to-Privacy (RTP) and Right-to-Explanation\n(RTE). This paper examines the complexities of combining these two\nrequirements. For RTP, we focus on `Differential privacy` (DP), which is\nconsidered the current gold standard for privacy-preserving machine learning\ndue to its strong quantitative guarantee of privacy. For RTE, we focus on\npost-hoc explainers: they are the go-to option for model auditing as they\noperate independently of model training. We formally investigate DP models and\nvarious commonly-used post-hoc explainers: how to evaluate these explainers\nsubject to RTP, and analyze the intrinsic interactions between DP models and\nthese explainers. Furthermore, our work throws light on how RTP and RTE can be\neffectively combined in high-stakes applications. Our study concludes by\noutlining an industrial software pipeline, with the example of a wildly used\nuse-case, that respects both RTP and RTE requirements.\n","authors":["Supriya Manna","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2412.20798v4.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2505.16670v1","updated":"2025-05-22T13:36:00Z","published":"2025-05-22T13:36:00Z","title":"BitHydra: Towards Bit-flip Inference Cost Attack against Large Language\n  Models","summary":"  Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs.\n","authors":["Xiaobei Yan","Yiming Li","Zhaoxin Fan","Han Qiu","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16650v1","updated":"2025-05-22T13:19:30Z","published":"2025-05-22T13:19:30Z","title":"Unsupervised Network Anomaly Detection with Autoencoders and Traffic\n  Images","summary":"  Due to the recent increase in the number of connected devices, the need to\npromptly detect security issues is emerging. Moreover, the high number of\ncommunication flows creates the necessity of processing huge amounts of data.\nFurthermore, the connected devices are heterogeneous in nature, having\ndifferent computational capacities. For this reason, in this work we propose an\nimage-based representation of network traffic which allows to realize a compact\nsummary of the current network conditions with 1-second time windows. The\nproposed representation highlights the presence of anomalies thus reducing the\nneed for complex processing architectures. Finally, we present an unsupervised\nlearning approach which effectively detects the presence of anomalies. The code\nand the dataset are available at\nhttps://github.com/michaelneri/image-based-network-traffic-anomaly-detection.\n","authors":["Michael Neri","Sara Baldoni"],"pdf_url":"https://arxiv.org/pdf/2505.16650v1.pdf","comment":"Accepted for publication in EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2411.04680v4","updated":"2025-05-22T13:18:55Z","published":"2024-11-07T13:08:06Z","title":"Differential Privacy in Continual Learning: Which Labels to Update?","summary":"  The goal of continual learning (CL) is to retain knowledge across tasks, but\nthis conflicts with strict privacy required for sensitive training data that\nprevents storing or memorising individual samples. To address that, we combine\nCL and differential privacy (DP). We highlight that failing to account for\nprivacy leakage through the set of labels a model can output can break the\nprivacy of otherwise valid DP algorithms. This is especially relevant in CL. We\nshow that mitigating the issue with a data-independent overly large label space\ncan have minimal negative impact on utility when fine-tuning a pre-trained\nmodel under DP, while learning the labels with a separate DP mechanism risks\nlosing small classes.\n","authors":["Marlon Tobaben","Talal Alrawajfeh","Marcus Klasson","Mikko Heikkilä","Arno Solin","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2411.04680v4.pdf","comment":"39 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.16008v2","updated":"2025-05-22T13:18:38Z","published":"2024-12-20T15:56:09Z","title":"Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep\n  Learning","summary":"  Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a\ncornerstone to assessing the authenticity of the received information and\nguaranteeing robust service delivery in several application domains. The\nsolutions available today for spoofing detection either rely on additional\ncommunication systems, receivers, and antennas, or require mobile deployments.\nDetection systems working at the Physical (PHY) layer of the satellite\ncommunication link also require time-consuming and energy-hungry training\nprocesses on all satellites of the constellation, and rely on the availability\nof spoofed data, which are often challenging to collect. Moreover, none of such\ncontributions investigate the feasibility of aerial spoofing attacks launched\nvia drones operating at various altitudes. In this paper, we propose a new\nspoofing detection technique for LEO satellite constellation systems, applying\nanomaly detection on the received PHY signal via autoencoders. We validate our\nsolution through an extensive measurement campaign involving the deployment of\nan actual spoofer (Software-Defined Radio) installed on a drone and injecting\nrogue IRIDIUM messages while flying at different altitudes with various\nmovement patterns. Our results demonstrate that the proposed technique can\nreliably detect LEO spoofing attacks launched at different altitudes, while\nstate-of-the-art competing approaches simply fail. We also release the\ncollected data as open source, fostering further research on satellite\nsecurity.\n","authors":["Jos Wigchert","Savio Sciancalepore","Gabriele Oligeri"],"pdf_url":"https://arxiv.org/pdf/2412.16008v2.pdf","comment":"Accepted for Publication by Elsevier Computer Networks"},{"id":"http://arxiv.org/abs/2505.16640v1","updated":"2025-05-22T13:12:46Z","published":"2025-05-22T13:12:46Z","title":"BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via\n  Objective-Decoupled Optimization","summary":"  Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/.\n","authors":["Xueyang Zhou","Guiyao Tie","Guowen Zhang","Hechang Wang","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2505.16640v1.pdf","comment":"19 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.10481v3","updated":"2025-05-22T12:52:24Z","published":"2024-10-14T13:18:20Z","title":"Model-based Large Language Model Customization as Service","summary":"  Prominent Large Language Model (LLM) services from providers like OpenAI and\nGoogle excel at general tasks but often underperform on domain-specific\napplications. Current customization services for these LLMs typically require\nusers to upload data for fine-tuning, posing significant privacy risks. While\ndifferentially private (DP) data synthesis presents a potential alternative,\nits application commonly results in low effectiveness due to the introduction\nof excessive noise on data for DP. To overcome this, we introduce Llamdex, a\nnovel framework that facilitates LLM customization as a service, where the\nclient uploads pre-trained domain-specific models rather than data. This\nclient-uploaded model, optionally protected by DP with much lower noise, is\ninserted into the base LLM via connection modules. Significantly, these\nconnecting modules are trained without requiring sensitive domain data,\nenabling clients to customize LLM services while preserving data privacy.\nExperiments demonstrate that Llamdex improves domain-specific accuracy by up to\n26\\% over state-of-the-art private data synthesis methods under identical\nprivacy constraints and, by obviating the need for users to provide domain\ncontext within queries, maintains inference efficiency comparable to the\noriginal LLM service.\n","authors":["Zhaomin Wu","Jizhou Guo","Junyi Hou","Bingsheng He","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16614v1","updated":"2025-05-22T12:49:18Z","published":"2025-05-22T12:49:18Z","title":"Energy Consumption Framework and Analysis of Post-Quantum Key-Generation\n  on Embedded Devices","summary":"  The emergence of quantum computing and Shor's algorithm necessitates an\nimminent shift from current public key cryptography techniques to post-quantum\nrobust techniques. NIST has responded by standardising Post-Quantum\nCryptography (PQC) algorithms, with ML-KEM (FIPS-203) slated to replace ECDH\n(Elliptic Curve Diffie-Hellman) for key exchange. A key practical concern for\nPQC adoption is energy consumption. This paper introduces a new framework for\nmeasuring the PQC energy consumption on a Raspberry Pi when performing key\ngeneration. The framework uses both available traditional methods and the newly\nstandardised ML-KEM algorithm via the commonly utilised OpenSSL library.\n","authors":["J Cameron Patterson","William J Buchanan","Callum Turino"],"pdf_url":"https://arxiv.org/pdf/2505.16614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16567v1","updated":"2025-05-22T11:59:44Z","published":"2025-05-22T11:59:44Z","title":"Finetuning-Activated Backdoors in LLMs","summary":"  Finetuning openly accessible Large Language Models (LLMs) has become standard\npractice for achieving task-specific performance improvements. Until now,\nfinetuning has been regarded as a controlled and secure process in which\ntraining on benign datasets led to predictable behaviors. In this paper, we\ndemonstrate for the first time that an adversary can create poisoned LLMs that\ninitially appear benign but exhibit malicious behaviors once finetuned by\ndownstream users. To this end, our proposed attack, FAB (Finetuning-Activated\nBackdoor), poisons an LLM via meta-learning techniques to simulate downstream\nfinetuning, explicitly optimizing for the emergence of malicious behaviors in\nthe finetuned models. At the same time, the poisoned LLM is regularized to\nretain general capabilities and to exhibit no malicious behaviors prior to\nfinetuning. As a result, when users finetune the seemingly benign model on\ntheir own datasets, they unknowingly trigger its hidden backdoor behavior. We\ndemonstrate the effectiveness of FAB across multiple LLMs and three target\nbehaviors: unsolicited advertising, refusal, and jailbreakability.\nAdditionally, we show that FAB-backdoors are robust to various finetuning\nchoices made by the user (e.g., dataset, number of steps, scheduler). Our\nfindings challenge prevailing assumptions about the security of finetuning,\nrevealing yet another critical attack vector exploiting the complexities of\nLLMs.\n","authors":["Thibaud Gloaguen","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2505.16567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16559v1","updated":"2025-05-22T11:47:08Z","published":"2025-05-22T11:47:08Z","title":"CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from\n  Harmful Fine-Tuning","summary":"  Fine-tuning-as-a-service, while commercially successful for Large Language\nModel (LLM) providers, exposes models to harmful fine-tuning attacks. As a\nwidely explored defense paradigm against such attacks, unlearning attempts to\nremove malicious knowledge from LLMs, thereby essentially preventing them from\nbeing used to perform malicious tasks. However, we highlight a critical flaw:\nthe powerful general adaptability of LLMs allows them to easily bypass\nselective unlearning by rapidly relearning or repurposing their capabilities\nfor harmful tasks. To address this fundamental limitation, we propose a\nparadigm shift: instead of selective removal, we advocate for inducing model\ncollapse--effectively forcing the model to \"unlearn everything\"--specifically\nin response to updates characteristic of malicious adaptation. This collapse\ndirectly neutralizes the very general capabilities that attackers exploit,\ntackling the core issue unaddressed by selective unlearning. We introduce the\nCollapse Trap (CTRAP) as a practical mechanism to implement this concept\nconditionally. Embedded during alignment, CTRAP pre-configures the model's\nreaction to subsequent fine-tuning dynamics. If updates during fine-tuning\nconstitute a persistent attempt to reverse safety alignment, the pre-configured\ntrap triggers a progressive degradation of the model's core language modeling\nabilities, ultimately rendering it inert and useless for the attacker.\nCrucially, this collapse mechanism remains dormant during benign fine-tuning,\nensuring the model's utility and general capabilities are preserved for\nlegitimate users. Extensive empirical results demonstrate that CTRAP\neffectively counters harmful fine-tuning risks across various LLMs and attack\nsettings, while maintaining high performance in benign scenarios. Our code is\navailable at https://anonymous.4open.science/r/CTRAP.\n","authors":["Biao Yi","Tiansheng Huang","Baolei Zhang","Tong Li","Lihai Nie","Zheli Liu","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2505.16559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16530v1","updated":"2025-05-22T11:16:46Z","published":"2025-05-22T11:16:46Z","title":"DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection","summary":"  Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.\n","authors":["Yuliang Yan","Haochun Tang","Shuo Yan","Enyan Dai"],"pdf_url":"https://arxiv.org/pdf/2505.16530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04163v2","updated":"2025-05-22T10:43:16Z","published":"2024-12-05T13:54:53Z","title":"On the Lack of Robustness of Binary Function Similarity Systems","summary":"  Binary function similarity, which often relies on learning-based algorithms\nto identify what functions in a pool are most similar to a given query\nfunction, is a sought-after topic in different communities, including machine\nlearning, software engineering, and security. Its importance stems from the\nimpact it has in facilitating several crucial tasks, from reverse engineering\nand malware analysis to automated vulnerability detection. Whereas recent work\ncast light around performance on this long-studied problem, the research\nlandscape remains largely lackluster in understanding the resiliency of the\nstate-of-the-art machine learning models against adversarial attacks. As\nsecurity requires to reason about adversaries, in this work we assess the\nrobustness of such models through a simple yet effective black-box greedy\nattack, which modifies the topology and the content of the control flow of the\nattacked functions. We demonstrate that this attack is successful in\ncompromising all the models, achieving average attack success rates of 57.06%\nand 95.81% depending on the problem settings (targeted and untargeted attacks).\nOur findings are insightful: top performance on clean data does not necessarily\nrelate to top robustness properties, which explicitly highlights\nperformance-robustness trade-offs one should consider when deploying such\nmodels, calling for further research.\n","authors":["Gianluca Capozzi","Tong Tang","Jie Wan","Ziqi Yang","Daniele Cono D'Elia","Giuseppe Antonio Di Luna","Lorenzo Cavallaro","Leonardo Querzoni"],"pdf_url":"https://arxiv.org/pdf/2412.04163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16503v1","updated":"2025-05-22T10:36:50Z","published":"2025-05-22T10:36:50Z","title":"Language-based Security and Time-inserting Supervisor","summary":"  Algebraic methods are employed in order to define language-based security\nproperties of processes. A supervisor is introduced that can disable unwanted\nbehavior of an insecure process by controlling some of its actions or by\ninserting timed actions to make an insecure process secure. We assume a\nsituation where neither the supervisor nor the attacker has complete\ninformation about the ongoing systems behavior. We study the conditions under\nwhich such a supervisor exists, as well as its properties and limitations.\n","authors":["Damas P. Gruska"],"pdf_url":"https://arxiv.org/pdf/2505.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16439v1","updated":"2025-05-22T09:27:40Z","published":"2025-05-22T09:27:40Z","title":"Password Strength Detection via Machine Learning: Analysis, Modeling,\n  and Evaluation","summary":"  As network security issues continue gaining prominence, password security has\nbecome crucial in safeguarding personal information and network systems. This\nstudy first introduces various methods for system password cracking, outlines\npassword defense strategies, and discusses the application of machine learning\nin the realm of password security. Subsequently, we conduct a detailed public\npassword database analysis, uncovering standard features and patterns among\npasswords. We extract multiple characteristics of passwords, including length,\nthe number of digits, the number of uppercase and lowercase letters, and the\nnumber of special characters. We then experiment with six different machine\nlearning algorithms: support vector machines, logistic regression, neural\nnetworks, decision trees, random forests, and stacked models, evaluating each\nmodel's performance based on various metrics, including accuracy, recall, and\nF1 score through model validation and hyperparameter tuning. The evaluation\nresults on the test set indicate that decision trees and stacked models excel\nin accuracy, recall, and F1 score, making them a practical option for the\nstrong and weak password classification task.\n","authors":["Jiazhi Mo","Hailu Kuang","Xiaoqi Li"],"pdf_url":"https://arxiv.org/pdf/2505.16439v1.pdf","comment":"22 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.06688v3","updated":"2025-05-22T09:22:23Z","published":"2025-02-10T17:14:37Z","title":"Network Intrusion Datasets: A Survey, Limitations, and Recommendations","summary":"  Data-driven cyberthreat detection has become a crucial defense technique in\nmodern cybersecurity. Network defense, supported by Network Intrusion Detection\nSystems (NIDSs), has also increasingly adopted data-driven approaches, leading\nto greater reliance on data. Despite the importance of data, its scarcity has\nlong been recognized as a major obstacle in NIDS research. In response, the\ncommunity has published many new datasets recently. However, many of them\nremain largely unknown and unanalyzed, leaving researchers uncertain about\ntheir suitability for specific use cases.\n  In this paper, we aim to address this knowledge gap by performing a\nsystematic literature review (SLR) of 89 public datasets for NIDS research.\nEach dataset is comparatively analyzed across 13 key properties, and its\npotential applications are outlined. Beyond the review, we also discuss\ndomain-specific challenges and common data limitations to facilitate a critical\nview on data quality. To aid in data selection, we conduct a dataset popularity\nanalysis in contemporary state-of-the-art NIDS research. Furthermore, the paper\npresents best practices for dataset selection, generation, and usage. By\nproviding a comprehensive overview of the domain and its data, this work aims\nto guide future research toward improving data quality and the robustness of\nNIDS solutions.\n","authors":["Patrik Goldschmidt","Daniela Chudá"],"pdf_url":"https://arxiv.org/pdf/2502.06688v3.pdf","comment":"42 pages, 8 figures, 6 tables. Accepted version for the journal\n  Computers & Security"},{"id":"http://arxiv.org/abs/2409.06280v3","updated":"2025-05-22T09:01:03Z","published":"2024-09-10T07:31:56Z","title":"Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep\n  Learning Models","summary":"  The rise of deep learning (DL) has led to a surging demand for training data,\nwhich incentivizes the creators of DL models to trawl through the Internet for\ntraining materials. Meanwhile, users often have limited control over whether\ntheir data (e.g., facial images) are used to train DL models without their\nconsent, which has engendered pressing concerns.\n  This work proposes MembershipTracker, a practical data auditing tool that can\nempower ordinary users to reliably detect the unauthorized use of their data in\ntraining DL models. We view data auditing through the lens of membership\ninference (MI). MembershipTracker consists of a lightweight data marking\ncomponent to mark the target data with small and targeted changes, which can be\nstrongly memorized by the model trained on them; and a specialized MI-based\nverification process to audit whether the model exhibits strong memorization on\nthe target samples.\n  MembershipTracker only requires the users to mark a small fraction of data\n(0.005% to 0.1% in proportion to the training set), and it enables the users to\nreliably detect the unauthorized use of their data (average 0% FPR@100% TPR).\nWe show that MembershipTracker is highly effective across various settings,\nincluding industry-scale training on the full-size ImageNet-1k dataset. We\nfinally evaluate MembershipTracker under multiple classes of countermeasures.\n","authors":["Zitao Chen","Karthik Pattabiraman"],"pdf_url":"https://arxiv.org/pdf/2409.06280v3.pdf","comment":"A shorter version of this paper will appear in CCS'25"},{"id":"http://arxiv.org/abs/2505.16398v1","updated":"2025-05-22T08:49:35Z","published":"2025-05-22T08:49:35Z","title":"Consistent and Compatible Modelling of Cyber Intrusions and Incident\n  Response Demonstrated in the Context of Malware Attacks on Critical\n  Infrastructure","summary":"  Cyber Security Incident Response (IR) Playbooks are used to capture the steps\nrequired to recover from a cyber intrusion. Individual IR playbooks should\nfocus on a specific type of incident and be aligned with the architecture of a\nsystem under attack. Intrusion modelling focuses on a specific potential cyber\nintrusion and is used to identify where and what countermeasures are needed,\nand the resulting intrusion models are expected to be used in effective IR,\nideally by feeding IR Playbooks designs. IR playbooks and intrusion models,\nhowever, are created in isolation and at varying stages of the system's\nlifecycle. We take nine critical national infrastructure intrusion models -\nexpressed using Sequential AND Attack Trees - and transform them into models of\nthe same format as IR playbooks. We use Security Modelling Framework for\nmodelling attacks and playbooks, and for demonstrating the feasibility of the\nbetter integration between risk assessment and IR at the modelling level. This\nresults in improved intrusion models and tighter coupling between IR playbooks\nand threat modelling which - as we demonstrate - yields novel insights into the\nanalysis of attacks and response actions. The main contributions of this paper\nare (a) a novel way of representing attack trees using the Security Modelling\nFramework,(b) a new tool for converting Sequential AND attack trees into models\ncompatible with playbooks, and (c) the examples of nine intrusion models\nrepresented using the Security Modelling Framework.\n","authors":["Peter Maynard","Yulia Cherdantseva","Avi Shaked","Pete Burnap","Arif Mehmood"],"pdf_url":"https://arxiv.org/pdf/2505.16398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16371v1","updated":"2025-05-22T08:26:09Z","published":"2025-05-22T08:26:09Z","title":"Privacy-Aware Cyberterrorism Network Analysis using Graph Neural\n  Networks and Federated Learning","summary":"  Cyberterrorism poses a formidable threat to digital infrastructures, with\nincreasing reliance on encrypted, decentralized platforms that obscure threat\nactor activity. To address the challenge of analyzing such adversarial networks\nwhile preserving the privacy of distributed intelligence data, we propose a\nPrivacy-Aware Federated Graph Neural Network (PA-FGNN) framework. PA-FGNN\nintegrates graph attention networks, differential privacy, and homomorphic\nencryption into a robust federated learning pipeline tailored for\ncyberterrorism network analysis. Each client trains locally on sensitive graph\ndata and exchanges encrypted, noise-perturbed model updates with a central\naggregator, which performs secure aggregation and broadcasts global updates. We\nimplement anomaly detection for flagging high-risk nodes and incorporate\ndefenses against gradient poisoning. Experimental evaluations on simulated dark\nweb and cyber-intelligence graphs demonstrate that PA-FGNN achieves over 91\\%\nclassification accuracy, maintains resilience under 20\\% adversarial client\nbehavior, and incurs less than 18\\% communication overhead. Our results\nhighlight that privacy-preserving GNNs can support large-scale cyber threat\ndetection without compromising on utility, privacy, or robustness.\n","authors":["Anas Ali","Mubashar Husain","Peter Hans"],"pdf_url":"https://arxiv.org/pdf/2505.16371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16366v1","updated":"2025-05-22T08:21:39Z","published":"2025-05-22T08:21:39Z","title":"ReCopilot: Reverse Engineering Copilot in Binary Analysis","summary":"  Binary analysis plays a pivotal role in security domains such as malware\ndetection and vulnerability discovery, yet it remains labor-intensive and\nheavily reliant on expert knowledge. General-purpose large language models\n(LLMs) perform well in programming analysis on source code, while\nbinaryspecific LLMs are underexplored. In this work, we present ReCopilot, an\nexpert LLM designed for binary analysis tasks. ReCopilot integrates binary code\nknowledge through a meticulously constructed dataset, encompassing continue\npretraining (CPT), supervised fine-tuning (SFT), and direct preference\noptimization (DPO) stages. It leverages variable data flow and call graph to\nenhance context awareness and employs test-time scaling to improve reasoning\ncapabilities. Evaluations on a comprehensive binary analysis benchmark\ndemonstrate that ReCopilot achieves state-of-the-art performance in tasks such\nas function name recovery and variable type inference on the decompiled pseudo\ncode, outperforming both existing tools and LLMs by 13%. Our findings highlight\nthe effectiveness of domain-specific training and context enhancement, while\nalso revealing challenges in building super long chain-of-thought. ReCopilot\nrepresents a significant step toward automating binary analysis with\ninterpretable and scalable AI assistance in this domain.\n","authors":["Guoqiang Chen","Huiqi Sun","Daguang Liu","Zhiqi Wang","Qiang Wang","Bin Yin","Lu Liu","Lingyun Ying"],"pdf_url":"https://arxiv.org/pdf/2505.16366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16318v1","updated":"2025-05-22T07:21:04Z","published":"2025-05-22T07:21:04Z","title":"SuperPure: Efficient Purification of Localized and Distributed\n  Adversarial Patches via Super-Resolution GAN Models","summary":"  As vision-based machine learning models are increasingly integrated into\nautonomous and cyber-physical systems, concerns about (physical) adversarial\npatch attacks are growing. While state-of-the-art defenses can achieve\ncertified robustness with minimal impact on utility against highly-concentrated\nlocalized patch attacks, they fall short in two important areas: (i)\nState-of-the-art methods are vulnerable to low-noise distributed patches where\nperturbations are subtly dispersed to evade detection or masking, as shown\nrecently by the DorPatch attack; (ii) Achieving high robustness with\nstate-of-the-art methods is extremely time and resource-consuming, rendering\nthem impractical for latency-sensitive applications in many cyber-physical\nsystems.\n  To address both robustness and latency issues, this paper proposes a new\ndefense strategy for adversarial patch attacks called SuperPure. The key\nnovelty is developing a pixel-wise masking scheme that is robust against both\ndistributed and localized patches. The masking involves leveraging a GAN-based\nsuper-resolution scheme to gradually purify the image from adversarial patches.\nOur extensive evaluations using ImageNet and two standard classifiers, ResNet\nand EfficientNet, show that SuperPure advances the state-of-the-art in three\nmajor directions: (i) it improves the robustness against conventional localized\npatches by more than 20%, on average, while also improving top-1 clean accuracy\nby almost 10%; (ii) It achieves 58% robustness against distributed patch\nattacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It\ndecreases the defense end-to-end latency by over 98% compared to PatchCleanser.\nOur further analysis shows that SuperPure is robust against white-box attacks\nand different patch sizes. Our code is open-source.\n","authors":["Hossein Khalili","Seongbin Park","Venkat Bollapragada","Nader Sehatbakhsh"],"pdf_url":"https://arxiv.org/pdf/2505.16318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05328v3","updated":"2025-05-22T07:08:51Z","published":"2025-05-08T15:20:19Z","title":"Timestamp Manipulation: Timestamp-based Nakamoto-style Blockchains are\n  Vulnerable","summary":"  Nakamoto consensus are the most widely adopted decentralized consensus\nmechanism in cryptocurrency systems. Since it was proposed in 2008, many\nstudies have focused on analyzing its security. Most of them focus on\nmaximizing the profit of the adversary. Examples include the selfish mining\nattack [FC '14] and the recent riskless uncle maker (RUM) attack [CCS '23]. In\nthis work, we introduce the Staircase-Unrestricted Uncle Maker (SUUM), the\nfirst block withholding attack targeting the timestamp-based Nakamoto-style\nblockchain. Through block withholding, timestamp manipulation, and difficulty\nrisk control, SUUM adversaries are capable of launching persistent attacks with\nzero cost and minimal difficulty risk characteristics, indefinitely exploiting\nrewards from honest participants. This creates a self-reinforcing cycle that\nthreatens the security of blockchains. We conduct a comprehensive and\nsystematic evaluation of SUUM, including the attack conditions, its impact on\nblockchains, and the difficulty risks. Finally, we further discuss four\nfeasible mitigation measures against SUUM.\n","authors":["Junjie Hu","Na Ruan","Sisi Duan"],"pdf_url":"https://arxiv.org/pdf/2505.05328v3.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.16300v1","updated":"2025-05-22T06:54:38Z","published":"2025-05-22T06:54:38Z","title":"Poster: Towards an Automated Security Testing Framework for Industrial\n  UEs","summary":"  With the ongoing adoption of 5G for communication in industrial systems and\ncritical infrastructure, the security of industrial UEs such as 5G-enabled\nindustrial robots becomes an increasingly important topic. Most notably, to\nmeet the stringent security requirements of industrial deployments, industrial\nUEs not only have to fully comply with the 5G specifications but also implement\nand use correctly secure communication protocols such as TLS. To ensure the\nsecurity of industrial UEs, operators of industrial 5G networks rely on\nsecurity testing before deploying new devices to their production networks.\nHowever, currently only isolated tests for individual security aspects of\nindustrial UEs exist, severely hindering comprehensive testing. In this paper,\nwe report on our ongoing efforts to alleviate this situation by creating an\nautomated security testing framework for industrial UEs to comprehensively\nevaluate their security posture before deployment. With this framework, we aim\nto provide stakeholders with a fully automated-method to verify that\nhigher-layer security protocols are correctly implemented, while simultaneously\nensuring that the UE's protocol stack adheres to 3GPP specifications.\n","authors":["Sotiris Michaelides","Daniel Eguiguren Chavez","Martin Henze"],"pdf_url":"https://arxiv.org/pdf/2505.16300v1.pdf","comment":"EuroS&P 2025"},{"id":"http://arxiv.org/abs/2505.02344v2","updated":"2025-05-22T06:06:24Z","published":"2025-05-05T03:50:28Z","title":"An End-to-End Model For Logits Based Large Language Models Watermarking","summary":"  The rise of LLMs has increased concerns over source tracing and copyright\nprotection for AIGC, highlighting the need for advanced detection technologies.\nPassive detection methods usually face high false positives, while active\nwatermarking techniques using logits or sampling manipulation offer more\neffective protection. Existing LLM watermarking methods, though effective on\nunaltered content, suffer significant performance drops when the text is\nmodified and could introduce biases that degrade LLM performance in downstream\ntasks. These methods fail to achieve an optimal tradeoff between text quality\nand robustness, particularly due to the lack of end-to-end optimization of the\nencoder and decoder. In this paper, we introduce a novel end-to-end logits\nperturbation method for watermarking LLM-generated text. By jointly\noptimization, our approach achieves a better balance between quality and\nrobustness. To address non-differentiable operations in the end-to-end training\npipeline, we introduce an online prompting technique that leverages the\non-the-fly LLM as a differentiable surrogate. Our method achieves superior\nrobustness, outperforming distortion-free methods by 37-39% under paraphrasing\nand 17.2% on average, while maintaining text quality on par with these\ndistortion-free methods in terms of text perplexity and downstream tasks. Our\nmethod can be easily generalized to different LLMs. Code is available at\nhttps://github.com/KAHIMWONG/E2E_LLM_WM.\n","authors":["Kahim Wong","Jicheng Zhou","Jiantao Zhou","Yain-Whar Si"],"pdf_url":"https://arxiv.org/pdf/2505.02344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16263v1","updated":"2025-05-22T05:55:26Z","published":"2025-05-22T05:55:26Z","title":"All You Need is \"Leet\": Evading Hate-speech Detection AI","summary":"  Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.\n","authors":["Sampanna Yashwant Kahu","Naman Ahuja"],"pdf_url":"https://arxiv.org/pdf/2505.16263v1.pdf","comment":"10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet"},{"id":"http://arxiv.org/abs/2505.16261v1","updated":"2025-05-22T05:50:39Z","published":"2025-05-22T05:50:39Z","title":"Interpretable Anomaly Detection in Encrypted Traffic Using SHAP with\n  Machine Learning Models","summary":"  The widespread adoption of encrypted communication protocols such as HTTPS\nand TLS has enhanced data privacy but also rendered traditional anomaly\ndetection techniques less effective, as they often rely on inspecting\nunencrypted payloads. This study aims to develop an interpretable machine\nlearning-based framework for anomaly detection in encrypted network traffic.\nThis study proposes a model-agnostic framework that integrates multiple machine\nlearning classifiers, with SHapley Additive exPlanations SHAP to ensure\npost-hoc model interpretability. The models are trained and evaluated on three\nbenchmark encrypted traffic datasets. Performance is assessed using standard\nclassification metrics, and SHAP is used to explain model predictions by\nattributing importance to individual input features. SHAP visualizations\nsuccessfully revealed the most influential traffic features contributing to\nanomaly predictions, enhancing the transparency and trustworthiness of the\nmodels. Unlike conventional approaches that treat machine learning as a black\nbox, this work combines robust classification techniques with explainability\nthrough SHAP, offering a novel interpretable anomaly detection system tailored\nfor encrypted traffic environments. While the framework is generalizable,\nreal-time deployment and performance under adversarial conditions require\nfurther investigation. Future work may explore adaptive models and real-time\ninterpretability in operational network environments. This interpretable\nanomaly detection framework can be integrated into modern security operations\nfor encrypted environments, allowing analysts not only to detect anomalies with\nhigh precision but also to understand why a model made a particular decision a\ncrucial capability in compliance-driven and mission-critical settings.\n","authors":["Kalindi Singh","Aayush Kashyap","Aswani Kumar Cherukuri"],"pdf_url":"https://arxiv.org/pdf/2505.16261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16246v1","updated":"2025-05-22T05:31:22Z","published":"2025-05-22T05:31:22Z","title":"Verifying Differentially Private Median Estimation","summary":"  Differential Privacy (DP) is a robust privacy guarantee that is widely\nemployed in private data analysis today, finding broad application in domains\nsuch as statistical query release and machine learning. However, DP achieves\nprivacy by introducing noise into data or query answers, which malicious actors\ncould exploit during analysis. To address this concern, we propose the first\nverifiable differentially private median estimation scheme based on zk-SNARKs.\nOur scheme combines the exponential mechanism and a utility function for median\nestimation into an arithmetic circuit, leveraging a scaled version of the\ninverse cumulative distribution function (CDF) method for precise sampling from\nthe distribution derived from the utility function. This approach not only\nensures privacy but also provides a mechanism to verify that the algorithm\nachieves DP guarantees without revealing sensitive information in the process.\n","authors":["Hyukjun Kwon","Chenglin Fan"],"pdf_url":"https://arxiv.org/pdf/2505.16246v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.16215v1","updated":"2025-05-22T04:30:26Z","published":"2025-05-22T04:30:26Z","title":"A Scalable Hierarchical Intrusion Detection System for Internet of\n  Vehicles","summary":"  Due to its nature of dynamic, mobility, and wireless data transfer, the\nInternet of Vehicles (IoV) is prone to various cyber threats, ranging from\nspoofing and Distributed Denial of Services (DDoS) attacks to malware. To\nsafeguard the IoV ecosystem from intrusions, malicious activities, policy\nviolations, intrusion detection systems (IDS) play a critical role by\ncontinuously monitoring and analyzing network traffic to identify and mitigate\npotential threats in real-time. However, most existing research has focused on\ndeveloping centralized, machine learning-based IDS systems for IoV without\naccounting for its inherently distributed nature. Due to intensive computing\nrequirements, these centralized systems often rely on the cloud to detect cyber\nthreats, increasing delay of system response. On the other hand, edge nodes\ntypically lack the necessary resources to train and deploy complex machine\nlearning algorithms. To address this issue, this paper proposes an effective\nhierarchical classification framework tailored for IoV networks. Hierarchical\nclassification allows classifiers to be trained and tested at different levels,\nenabling edge nodes to detect specific types of attacks independently. With\nthis approach, edge nodes can conduct targeted attack detection while\nleveraging cloud nodes for comprehensive threat analysis and support. Given the\nresource constraints of edge nodes, we have employed the Boruta feature\nselection method to reduce data dimensionality, optimizing processing\nefficiency. To evaluate our proposed framework, we utilize the latest IoV\nsecurity dataset CIC-IoV2024, achieving promising results that demonstrate the\nfeasibility and effectiveness of our models in securing IoV networks.\n","authors":["Md Ashraf Uddin","Nam H. Chu","Reza Rafeh","Mutaz Barika"],"pdf_url":"https://arxiv.org/pdf/2505.16215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16205v1","updated":"2025-05-22T04:16:56Z","published":"2025-05-22T04:16:56Z","title":"VIVID: A Novel Approach to Remediation Prioritization in Static\n  Application Security Testing (SAST)","summary":"  Static Application Security Testing (SAST) enables organizations to detect\nvulnerabilities in code early; however, major SAST platforms do not include\nvisual aids and present little insight on correlations between tainted data\nchains. We propose VIVID - Vulnerability Information Via Data flow - a novel\nmethod to extract and consume SAST insights, which is to graph the\napplication's vulnerability data flows (VDFs) and carry out graph theory\nanalysis on the resulting VDF directed graph. Nine metrics were assessed to\nevaluate their effectiveness in analyzing the VDF graphs of deliberately\ninsecure web applications. These metrics include 3 centrality metrics, 2\nstructural metrics, PageRank, in-degree, out-degree, and cross-clique\nconnectivity. We present simulations that find that out-degree, betweenness\ncentrality, in-eigenvector centrality, and cross-clique connectivity were found\nto be associated with files exhibiting high vulnerability traffic, making them\nrefactoring candidates where input sanitization may have been missed.\nMeanwhile, out-eigenvector centrality, PageRank, and in-degree were found to be\nassociated with nodes enabling vulnerability flow and sinks, but not\nnecessarily where input validation should be placed. This is a novel method to\nautomatically provide development teams an evidence-based prioritized list of\nfiles to embed security controls into, informed by vulnerability propagation\npatterns in the application architecture.\n","authors":["Naeem Budhwani","Mohammad Faghani","Hayden Richard"],"pdf_url":"https://arxiv.org/pdf/2505.16205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16186v1","updated":"2025-05-22T03:46:03Z","published":"2025-05-22T03:46:03Z","title":"SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning","summary":"  Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.\n","authors":["Kaiwen Zhou","Xuandong Zhao","Gaowen Liu","Jayanth Srinivasa","Aosong Feng","Dawn Song","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07332v4","updated":"2025-05-22T03:38:57Z","published":"2024-02-11T23:50:12Z","title":"DePLOI: Applying NL2SQL to Synthesize and Audit Database Access Control","summary":"  In every enterprise database, administrators must define an access control\npolicy that specifies which users have access to which tables. Access control\nstraddles two worlds: policy (organization-level principles that define who\nshould have access) and process (database-level primitives that actually\nimplement the policy). Assessing and enforcing process compliance with a policy\nis a manual and ad-hoc task. This paper introduces a new access control model\ncalled Intent-Based Access Control for Databases (IBAC-DB). In IBAC-DB, access\ncontrol policies are expressed using abstractions that scale to high numbers of\ndatabase objects, and are traceable with respect to implementations. This paper\nproposes DePLOI (Deployment Policy Linter for Organization Intents), a\nLLM-backed system leveraging access control-specific task decompositions to\naccurately synthesize and audit access control implementation from IBAC-DB\nabstractions. As DePLOI is the first system of its kind to our knowledge, this\npaper further proposes IBACBench, the first benchmark for evaluating the\nsynthesis and auditing capabilities of DePLOI. IBACBench leverages a\ncombination of current NL2SQL benchmarks, real-world role hierarchies and\naccess control policies, and LLM-generated data. We find that DePLOI achieves\nhigh synthesis accuracies and auditing F1 scores overall, and greatly\noutperforms other LLM prompting strategies (e.g., by 10 F1 points).\n","authors":["Pranav Subramaniam","Sanjay Krishnan"],"pdf_url":"https://arxiv.org/pdf/2402.07332v4.pdf","comment":"13 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.16137v1","updated":"2025-05-22T02:26:50Z","published":"2025-05-22T02:26:50Z","title":"Outsourcing SAT-based Verification Computations in Network Security","summary":"  The emergence of cloud computing gives huge impact on large computations.\nCloud computing platforms offer servers with large computation power to be\navailable for customers. These servers can be used efficiently to solve\nproblems that are complex by nature, for example, satisfiability (SAT)\nproblems. Many practical problems can be converted to SAT, for example, circuit\nverification and network configuration analysis. However, outsourcing SAT\ninstances to the servers may cause data leakage that can jeopardize system's\nsecurity. Before\n  outsourcing the SAT instance, one needs to hide the input information. One\nway to preserve privacy and hide information is to randomize the SAT\n  instance before outsourcing. In this paper, we present multiple novel methods\nto randomize SAT instances. We present a novel method to randomize the SAT\ninstance, a variable randomization method to randomize the solution set, and\nmethods to randomize Mincost SAT and MAX3SAT instances. Our analysis and\nevaluation show the correctness and feasibility of these randomization methods.\nThe scalability and generality of our methods make it applicable for real world\nproblems.\n","authors":["Qi Duan","Ehab Al-Shaer"],"pdf_url":"https://arxiv.org/pdf/2505.16137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16112v1","updated":"2025-05-22T01:34:17Z","published":"2025-05-22T01:34:17Z","title":"Extensible Post Quantum Cryptography Based Authentication","summary":"  Cryptography underpins the security of modern digital infrastructure, from\ncloud services to health data. However, many widely deployed systems will\nbecome vulnerable after the advent of scalable quantum computing. Although\nquantum-safe cryptographic primitives have been developed, such as\nlattice-based digital signature algorithms (DSAs) and key encapsulation\nmechanisms (KEMs), their unique structural and performance characteristics make\nthem unsuitable for existing protocols. In this work, we introduce a\nquantum-safe single-shot protocol for machine-to-machine authentication and\nauthorization that is specifically designed to leverage the strengths of\nlattice-based DSAs and KEMs. Operating entirely over insecure channels, this\nprotocol enables the forward-secure establishment of tokens in constrained\nenvironments. By demonstrating how new quantum-safe cryptographic primitives\ncan be incorporated into secure systems, this study lays the groundwork for\nscalable, resilient, and future-proof identity infrastructures in a\nquantum-enabled world.\n","authors":["Homer A. Riva-Cambrin","Rahul Singh","Sanju Lama","Garnette R. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2505.16112v1.pdf","comment":"20 pages, single spaced, preprint"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.15810v2","updated":"2025-05-22T11:15:23Z","published":"2025-05-21T17:59:09Z","title":"GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI\n  Agents","summary":"  Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.\n","authors":["Yuqi Zhou","Sunhao Dai","Shuai Wang","Kaiwen Zhou","Qinglin Jia","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2505.15810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15747v2","updated":"2025-05-22T03:58:27Z","published":"2025-05-21T16:51:49Z","title":"Multi-modal Integration Analysis of Alzheimer's Disease Using Large\n  Language Models and Knowledge Graphs","summary":"  We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research.\n","authors":["Kanan Kiguchi","Yunhao Tu","Katsuhiro Ajito","Fady Alnajjar","Kazuyuki Murase"],"pdf_url":"https://arxiv.org/pdf/2505.15747v2.pdf","comment":"38 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.17531v3","updated":"2025-05-22T10:57:51Z","published":"2025-04-24T13:19:17Z","title":"Towards Machine-Generated Code for the Resolution of User Intentions","summary":"  The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code. This development represents a significant progression in\nthe realm of hybrid workflows, where human and artificial intelligence\ncollaborate to address user intentions, with the former responsible for\ndefining these intentions and the latter for implementing the solutions to\naddress them. In this paper, we investigate the feasibility of generating and\nexecuting workflows through code generation that results from prompting an LLM\nwith a concrete user intention, and a simplified application programming\ninterface for a GUI-less operating system. We provide an in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate the general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.\n","authors":["Justus Flerlage","Ilja Behnke","Odej Kao"],"pdf_url":"https://arxiv.org/pdf/2504.17531v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11741v3","updated":"2025-05-22T15:06:30Z","published":"2025-02-17T12:28:11Z","title":"SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL","summary":"  Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1.\n","authors":["Shuai Lyu","Haoran Luo","Ripeng Li","Zhonghong Ou","Jiangfeng Sun","Yang Qin","Xiaoran Shang","Meina Song","Yifan Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.11741v3.pdf","comment":"28 pages,12 figures"},{"id":"http://arxiv.org/abs/2505.15553v2","updated":"2025-05-22T09:39:49Z","published":"2025-05-21T14:14:47Z","title":"Social Bias in Popular Question-Answering Benchmarks","summary":"  Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.\n","authors":["Angelie Kraft","Judith Simon","Sonja Schimmler"],"pdf_url":"https://arxiv.org/pdf/2505.15553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01316v3","updated":"2025-05-22T07:20:42Z","published":"2025-02-03T12:46:02Z","title":"Learning Fused State Representations for Control from Multi-View\n  Observations","summary":"  Multi-View Reinforcement Learning (MVRL) seeks to provide agents with\nmulti-view observations, enabling them to perceive environment with greater\neffectiveness and precision. Recent advancements in MVRL focus on extracting\nlatent representations from multiview observations and leveraging them in\ncontrol tasks. However, it is not straightforward to learn compact and\ntask-relevant representations, particularly in the presence of redundancy,\ndistracting information, or missing views. In this paper, we propose Multi-view\nFusion State for Control (MFSC), firstly incorporating bisimulation metric\nlearning into MVRL to learn task-relevant representations. Furthermore, we\npropose a multiview-based mask and latent reconstruction auxiliary task that\nexploits shared information across views and improves MFSC's robustness in\nmissing views by introducing a mask token. Extensive experimental results\ndemonstrate that our method outperforms existing approaches in MVRL tasks. Even\nin more realistic scenarios with interference or missing views, MFSC\nconsistently maintains high performance.\n","authors":["Zeyu Wang","Yao-Hui Li","Xin Li","Hongyu Zang","Romain Laroche","Riashat Islam"],"pdf_url":"https://arxiv.org/pdf/2502.01316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15441v2","updated":"2025-05-22T15:33:46Z","published":"2025-05-21T12:22:53Z","title":"Stronger ViTs With Octic Equivariance","summary":"  Recent efforts at scaling computer vision models have established Vision\nTransformers (ViTs) as the leading architecture. ViTs incorporate weight\nsharing over image patches as an important inductive bias. In this work, we\nshow that ViTs benefit from incorporating equivariance under the octic group,\ni.e., reflections and 90-degree rotations, as a further inductive bias. We\ndevelop new architectures, octic ViTs, that use octic-equivariant layers and\nput them to the test on both supervised and self-supervised learning. Through\nextensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show\nthat octic ViTs yield more computationally efficient networks while also\nimproving performance. In particular, we achieve approximately 40% reduction in\nFLOPs for ViT-H while simultaneously improving both classification and\nsegmentation results.\n","authors":["David Nordström","Johan Edstedt","Fredrik Kahl","Georg Bökman"],"pdf_url":"https://arxiv.org/pdf/2505.15441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15358v2","updated":"2025-05-22T08:25:18Z","published":"2025-05-21T10:42:41Z","title":"Objective Bicycle Occlusion Level Classification using a Deformable\n  Parts-Based Model","summary":"  Road safety is a critical challenge, particularly for cyclists, who are among\nthe most vulnerable road users. This study aims to enhance road safety by\nproposing a novel benchmark for bicycle occlusion level classification using\nadvanced computer vision techniques. Utilizing a parts-based detection model,\nimages are annotated and processed through a custom image detection pipeline. A\nnovel method of bicycle occlusion level is proposed to objectively quantify the\nvisibility and occlusion level of bicycle semantic parts. The findings indicate\nthat the model robustly quantifies the visibility and occlusion level of\nbicycles, a significant improvement over the subjective methods used by the\ncurrent state of the art. Widespread use of the proposed methodology will\nfacilitate accurate performance reporting of cyclist detection algorithms for\noccluded cyclists, informing the development of more robust vulnerable road\nuser detection methods for autonomous vehicles.\n","authors":["Angelique Mangubat","Shane Gilroy"],"pdf_url":"https://arxiv.org/pdf/2505.15358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17022v1","updated":"2025-05-22T17:59:58Z","published":"2025-05-22T17:59:58Z","title":"GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning","summary":"  Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.\n","authors":["Chengqi Duan","Rongyao Fang","Yuqing Wang","Kun Wang","Linjiang Huang","Xingyu Zeng","Hongsheng Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2505.17022v1.pdf","comment":"Github page refer to: https://github.com/gogoduan/GoT-R1"},{"id":"http://arxiv.org/abs/2505.17019v1","updated":"2025-05-22T17:59:53Z","published":"2025-05-22T17:59:53Z","title":"Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework","summary":"  Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.\n","authors":["Chenhao Zhang","Yazhe Niu"],"pdf_url":"https://arxiv.org/pdf/2505.17019v1.pdf","comment":"16 pages, 9 figures. Code & Dataset:\n  https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep"},{"id":"http://arxiv.org/abs/2505.17017v1","updated":"2025-05-22T17:59:49Z","published":"2025-05-22T17:59:49Z","title":"Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO","summary":"  Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT\n","authors":["Chengzhuo Tong","Ziyu Guo","Renrui Zhang","Wenyu Shan","Xinyu Wei","Zhenghao Xing","Hongsheng Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2505.17017v1.pdf","comment":"Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT"},{"id":"http://arxiv.org/abs/2505.17016v1","updated":"2025-05-22T17:59:45Z","published":"2025-05-22T17:59:45Z","title":"Interactive Post-Training for Vision-Language-Action Models","summary":"  We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.\n","authors":["Shuhan Tan","Kairan Dou","Yue Zhao","Philipp Krähenbühl"],"pdf_url":"https://arxiv.org/pdf/2505.17016v1.pdf","comment":"Project page: https://ariostgx.github.io/ript_vla/"},{"id":"http://arxiv.org/abs/2502.06776v2","updated":"2025-05-22T17:59:11Z","published":"2025-02-10T18:54:05Z","title":"InSTA: Towards Internet-Scale Training For Agents","summary":"  The predominant approach for training web navigation agents is to gather\nhuman demonstrations for a set of popular websites and hand-written tasks, but\nit is becoming clear that human data is an inefficient resource. We develop a\npipeline to facilitate internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM annotates 150k sites with agentic\ntasks. In the next stage, LLM agents complete tasks and produce trajectories.\nIn the final stage, an LLM filters trajectories by judging their success.\nLanguage models are powerful data curation tools, identifying harmful content\nwith an accuracy of 97%, judging successful trajectories with an accuracy of\n82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that\nare competitive with frontier LLMs as web agents, while being smaller and\nfaster. Our top agent reaches a success rate of 56.9%, outperforming the data\ncollection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and\nreaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code,\nmodels and data at: https://data-for-agents.github.io.\n","authors":["Brandon Trabucco","Gunnar Sigurdsson","Robinson Piramuthu","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2502.06776v2.pdf","comment":"Improved results, zero-shot transfer to Web Voyager"},{"id":"http://arxiv.org/abs/2505.17012v1","updated":"2025-05-22T17:59:03Z","published":"2025-05-22T17:59:03Z","title":"SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding","summary":"  Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.\n","authors":["Haoning Wu","Xiao Huang","Yaohui Chen","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2505.17012v1.pdf","comment":"Technical Report; Project Page:\n  https://haoningwu3639.github.io/SpatialScore"},{"id":"http://arxiv.org/abs/2505.17010v1","updated":"2025-05-22T17:58:53Z","published":"2025-05-22T17:58:53Z","title":"Understanding Prompt Tuning and In-Context Learning via Meta-Learning","summary":"  Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory.\n","authors":["Tim Genewein","Kevin Wenliang Li","Jordi Grau-Moya","Anian Ruoss","Laurent Orseau","Marcus Hutter"],"pdf_url":"https://arxiv.org/pdf/2505.17010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17005v1","updated":"2025-05-22T17:58:26Z","published":"2025-05-22T17:58:26Z","title":"R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning","summary":"  Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.\n","authors":["Huatong Song","Jinhao Jiang","Wenqing Tian","Zhipeng Chen","Yuhuan Wu","Jiahao Zhao","Yingqian Min","Wayne Xin Zhao","Lei Fang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.17005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17004v1","updated":"2025-05-22T17:58:12Z","published":"2025-05-22T17:58:12Z","title":"Guided Diffusion Sampling on Function Spaces with Applications to PDEs","summary":"  We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS\n","authors":["Jiachen Yao","Abbas Mammadov","Julius Berner","Gavin Kerrigan","Jong Chul Ye","Kamyar Azizzadenesheli","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2505.17004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17002v1","updated":"2025-05-22T17:57:55Z","published":"2025-05-22T17:57:55Z","title":"PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for\n  Face-Voice Association","summary":"  We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach.\n","authors":["Abdul Hannan","Muhammad Arslan Manzoor","Shah Nawaz","Muhammad Irzam Liaqat","Markus Schedl","Mubashir Noman"],"pdf_url":"https://arxiv.org/pdf/2505.17002v1.pdf","comment":"Accepted at InterSpeech 2025"},{"id":"http://arxiv.org/abs/2505.16998v1","updated":"2025-05-22T17:57:23Z","published":"2025-05-22T17:57:23Z","title":"Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?","summary":"  Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.\n","authors":["Jin Jiang","Jianing Wang","Yuchen Yan","Yang Liu","Jianhua Zhu","Mengdi Zhang","Xunliang Cai","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2505.16998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16997v1","updated":"2025-05-22T17:56:39Z","published":"2025-05-22T17:56:39Z","title":"X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs","summary":"  LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.\n","authors":["Rui Ye","Xiangrui Liu","Qimin Wu","Xianghe Pang","Zhenfei Yin","Lei Bai","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2505.16997v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.05066v2","updated":"2025-05-22T17:55:58Z","published":"2025-03-07T01:11:39Z","title":"Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of\n  Experts","summary":"  The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct.\n","authors":["Shwai He","Weilin Cai","Jiayi Huang","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2503.05066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16994v1","updated":"2025-05-22T17:55:43Z","published":"2025-05-22T17:55:43Z","title":"$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning","summary":"  Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.\n","authors":["Runyang You","Yongqi Li","Xinyu Lin","Xin Zhang","Wenjie Wang","Wenjie Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2505.16994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16988v1","updated":"2025-05-22T17:54:38Z","published":"2025-05-22T17:54:38Z","title":"MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent\n  Systems","summary":"  LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.\n","authors":["Rui Ye","Keduan Huang","Qimin Wu","Yuzhu Cai","Tian Jin","Xianghe Pang","Xiangrui Liu","Jiaqi Su","Chen Qian","Bohan Tang","Kaiqu Liang","Jiaao Chen","Yue Hu","Zhenfei Yin","Rongye Shi","Bo An","Yang Gao","Wenjun Wu","Lei Bai","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2505.16988v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.16986v1","updated":"2025-05-22T17:54:32Z","published":"2025-05-22T17:54:32Z","title":"T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.\n","authors":["Amartya Chakraborty","Paresh Dashore","Nadia Bathaee","Anmol Jain","Anirban Das","Shi-Xiong Zhang","Sambit Sahu","Milind Naphade","Genta Indra Winata"],"pdf_url":"https://arxiv.org/pdf/2505.16986v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.16985v1","updated":"2025-05-22T17:54:30Z","published":"2025-05-22T17:54:30Z","title":"Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution\n  Detection and Segmentation","summary":"  Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing.\n","authors":["Moru Liu","Hao Dong","Jessica Kelly","Olga Fink","Mario Trapp"],"pdf_url":"https://arxiv.org/pdf/2505.16985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16982v1","updated":"2025-05-22T17:52:59Z","published":"2025-05-22T17:52:59Z","title":"Beyond Correlation: Towards Causal Large Language Model Agents in\n  Biomedicine","summary":"  Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.\n","authors":["Adib Bazgir","Amir Habibdoust Lafmajani","Yuwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16979v1","updated":"2025-05-22T17:52:33Z","published":"2025-05-22T17:52:33Z","title":"Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\n  Design","summary":"  Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired.\n","authors":["Zhenkun Li","Lingyao Li","Shuhang Lin","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16978v1","updated":"2025-05-22T17:52:31Z","published":"2025-05-22T17:52:31Z","title":"HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation","summary":"  Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.\n","authors":["Weizhi Tang","Yixuan Li","Chris Sypherd","Elizabeth Polgreen","Vaishak Belle"],"pdf_url":"https://arxiv.org/pdf/2505.16978v1.pdf","comment":"Accepted to ACL 2025 Findings. Code available at\n  https://github.com/RutaTang/HyGenar"},{"id":"http://arxiv.org/abs/2505.14625v2","updated":"2025-05-22T17:49:50Z","published":"2025-05-20T17:16:44Z","title":"TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning","summary":"  Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.\n","authors":["Zhangchen Xu","Yuetai Li","Fengqing Jiang","Bhaskar Ramasubramanian","Luyao Niu","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2505.14625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16968v1","updated":"2025-05-22T17:48:53Z","published":"2025-05-22T17:48:53Z","title":"CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark","summary":"  We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.\n","authors":["Ahmed Heakl","Sarim Hashmi","Gustavo Bertolo Stahl","Seung Hun Eddie Han","Salman Khan","Abdulrahman Mahmoud"],"pdf_url":"https://arxiv.org/pdf/2505.16968v1.pdf","comment":"20 pages, 11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.16967v1","updated":"2025-05-22T17:47:57Z","published":"2025-05-22T17:47:57Z","title":"Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval","summary":"  Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.\n","authors":["Nandan Thakur","Crystina Zhang","Xueguang Ma","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2505.16967v1.pdf","comment":"Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn"},{"id":"http://arxiv.org/abs/2505.16965v1","updated":"2025-05-22T17:46:23Z","published":"2025-05-22T17:46:23Z","title":"BP-Seg: A graphical model approach to unsupervised and non-contiguous\n  text segmentation using belief propagation","summary":"  Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.\n","authors":["Fengyi Li","Kayhan Behdin","Natesh Pillai","Xiaofeng Wang","Zhipeng Wang","Ercan Yildiz"],"pdf_url":"https://arxiv.org/pdf/2505.16965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16957v1","updated":"2025-05-22T17:36:33Z","published":"2025-05-22T17:36:33Z","title":"Invisible Prompts, Visible Threats: Malicious Font Injection in External\n  Resources for Large Language Models","summary":"  Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.\n","authors":["Junjie Xiong","Changjia Zhu","Shuhang Lin","Chong Zhang","Yongfeng Zhang","Yao Liu","Lingyao Li"],"pdf_url":"https://arxiv.org/pdf/2505.16957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16950v1","updated":"2025-05-22T17:33:49Z","published":"2025-05-22T17:33:49Z","title":"Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning","summary":"  Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.\n","authors":["Adnan Oomerjee","Zafeirios Fountas","Zhongwei Yu","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16947v1","updated":"2025-05-22T17:32:50Z","published":"2025-05-22T17:32:50Z","title":"MixAT: Combining Continuous and Discrete Adversarial Training for LLMs","summary":"  Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT.\n","authors":["Csaba Dékány","Stefan Balauca","Robin Staab","Dimitar I. Dimitrov","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2505.16947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16944v1","updated":"2025-05-22T17:31:10Z","published":"2025-05-22T17:31:10Z","title":"AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios","summary":"  Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.\n","authors":["Yunjia Qi","Hao Peng","Xiaozhi Wang","Amy Xin","Youfeng Liu","Bin Xu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2505.16944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16941v1","updated":"2025-05-22T17:29:52Z","published":"2025-05-22T17:29:52Z","title":"FoMoH: A clinically meaningful foundation model evaluation for\n  structured electronic health records","summary":"  Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.\n","authors":["Chao Pang","Vincent Jeanselme","Young Sang Choi","Xinzhuo Jiang","Zilin Jing","Aparajita Kashyap","Yuta Kobayashi","Yanwei Li","Florent Pollet","Karthik Natarajan","Shalmali Joshi"],"pdf_url":"https://arxiv.org/pdf/2505.16941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16938v1","updated":"2025-05-22T17:27:43Z","published":"2025-05-22T17:27:43Z","title":"NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification","summary":"  Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.\n","authors":[" NovelSeek Team","Bo Zhang","Shiyang Feng","Xiangchao Yan","Jiakang Yuan","Zhiyin Yu","Xiaohan He","Songtao Huang","Shaowei Hou","Zheng Nie","Zhilong Wang","Jinyao Liu","Runmin Ma","Tianshuo Peng","Peng Ye","Dongzhan Zhou","Shufei Zhang","Xiaosong Wang","Yilan Zhang","Meng Li","Zhongying Tu","Xiangyu Yue","Wangli Ouyang","Bowen Zhou","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2505.16938v1.pdf","comment":"HomePage: https://alpha-innovator.github.io/NovelSeek-project-page"},{"id":"http://arxiv.org/abs/2505.12269v2","updated":"2025-05-22T17:27:15Z","published":"2025-05-18T07:18:58Z","title":"Vague Knowledge: Evidence from Analyst Reports","summary":"  People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in representing subjective expectations. Empirically, we find that\nin their reports, analysts include useful information in linguistic expressions\nbut not numerical forecasts. Specifically, the textual tone of analyst reports\nhas predictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language.\n","authors":["Kerry Xiao","Amy Zang"],"pdf_url":"https://arxiv.org/pdf/2505.12269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16932v1","updated":"2025-05-22T17:23:14Z","published":"2025-05-22T17:23:14Z","title":"The Polar Express: Optimal Matrix Sign Methods and Their Application to\n  the Muon Algorithm","summary":"  Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.\n","authors":["Noah Amsel","David Persson","Christopher Musco","Robert Gower"],"pdf_url":"https://arxiv.org/pdf/2505.16932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16928v1","updated":"2025-05-22T17:20:38Z","published":"2025-05-22T17:20:38Z","title":"Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning","summary":"  We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.\n","authors":["Bosung Kim","Prithviraj Ammanabrolu"],"pdf_url":"https://arxiv.org/pdf/2505.16928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16927v1","updated":"2025-05-22T17:20:18Z","published":"2025-05-22T17:20:18Z","title":"Latent Principle Discovery for Language Model Self-Improvement","summary":"  When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.\n","authors":["Keshav Ramji","Tahira Naseem","Ramón Fernandez Astudillo"],"pdf_url":"https://arxiv.org/pdf/2505.16927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09795v5","updated":"2025-05-22T17:19:19Z","published":"2024-10-13T10:48:22Z","title":"WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for\n  Molecular Ground-State Conformation Prediction","summary":"  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem, while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for ground-state conformation prediction. Specifically, our method\ntackles this task within an auto-encoding framework, which encodes low-quality\nconformations by the proposed WGFormer and decodes corresponding ground-state\nconformations by an MLP. The architecture of WGFormer corresponds to\nWasserstein gradient flows -- it optimizes conformations by minimizing an\nenergy function defined on the latent mixture models of atoms, thereby\nsignificantly improving performance and interpretability. Extensive experiments\ndemonstrate that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict ground-state\nconformation.\n","authors":["Fanmeng Wang","Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09795v5.pdf","comment":"Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)"},{"id":"http://arxiv.org/abs/2505.16915v1","updated":"2025-05-22T17:11:27Z","published":"2025-05-22T17:11:27Z","title":"DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?","summary":"  While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Xika Lin","Ying Shen","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2505.16915v1.pdf","comment":"22 pages, 8 figures, 10 tables"},{"id":"http://arxiv.org/abs/2505.16911v1","updated":"2025-05-22T17:10:18Z","published":"2025-05-22T17:10:18Z","title":"Active Speech Enhancement: Active Speech Denoising Decliping and\n  Deveraberation","summary":"  We introduce a new paradigm for active sound modification: Active Speech\nEnhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on\nsuppressing external interference, ASE goes further by actively shaping the\nspeech signal -- both attenuating unwanted noise components and amplifying\nspeech-relevant frequencies -- to improve intelligibility and perceptual\nquality. To enable this, we propose a novel Transformer-Mamba-based\narchitecture, along with a task-specific loss function designed to jointly\noptimize interference suppression and signal enrichment. Our method outperforms\nexisting baselines across multiple speech processing tasks -- including\ndenoising, dereverberation, and declipping -- demonstrating the effectiveness\nof active, targeted modulation in challenging acoustic environments.\n","authors":["Ofir Yaish","Yehuda Mishaly","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2505.16911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16899v1","updated":"2025-05-22T16:58:48Z","published":"2025-05-22T16:58:48Z","title":"Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships","summary":"  Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships.\n","authors":["Kerem Oktar","Katherine M. Collins","Jose Hernandez-Orallo","Diane Coyle","Stephen Cave","Adrian Weller","Ilia Sucholutsky"],"pdf_url":"https://arxiv.org/pdf/2505.16899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16896v1","updated":"2025-05-22T16:56:12Z","published":"2025-05-22T16:56:12Z","title":"Structure-Aligned Protein Language Model","summary":"  Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.\n","authors":["Can Chen","David Heurtel-Depeiges","Robert M. Vernon","Christopher James Langmead","Yoshua Bengio","Quentin Fournier"],"pdf_url":"https://arxiv.org/pdf/2505.16896v1.pdf","comment":"16 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.15814v2","updated":"2025-05-22T16:55:59Z","published":"2025-02-19T17:21:15Z","title":"Slamming: Training a Speech Language Model on One GPU in a Day","summary":"  We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/slamming .\n","authors":["Gallil Maimon","Avishai Elmakies","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2502.15814v2.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2503.09499v2","updated":"2025-05-22T16:47:33Z","published":"2025-03-12T16:03:03Z","title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric\n  Fine-Tuning?","summary":"  Large foundation models face challenges in acquiring transferable, structured\nthinking abilities, especially when supervised with rigid templates or\ncrowd-annotated instruction datasets. Unlike prior approaches, we focus on a\nthinking-centric data synthesis paradigm that enables models to evolve through\nself-generated, cognitively guided data. We propose MindGYM, a structured and\nscalable framework for question synthesis, composed of: (1) Cognitive Thinking\nProcess Injection, which infuses high-level reasoning objectives to shape the\nmodel's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating\natomic questions from diverse semantic types to encourage broader thinking; and\n(3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop\nquestions based on QA seeds for deeper reasoning. Detailed analysis shows that\nsynthetic data generated by our method achieves 16.7% higher average quality\nand 67.91% lower quality variance compared to baseline sources, highlighting\nthat both high-quality and self-contained data are essential for effective,\nthinking-oriented fine-tuning. MindGYM improves performance on six reasoning\nbenchmarks, achieving gains of up to 16% on MathVision using only 400 data\nsamples, and generalizable improvements across different model sizes and\narchitectures. MindGYM underscores the viability of self-challenging mechanisms\nin refining large model capabilities while minimizing human intervention and\nresource demands. Code and data are released to promote data-centric research\ninto self-evolving foundation models driven by their internal reasoning\ncapabilities.\n","authors":["Zhe Xu","Daoyuan Chen","Zhenqing Ling","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2503.09499v2.pdf","comment":"22 pages, 7 tables"},{"id":"http://arxiv.org/abs/2505.16888v1","updated":"2025-05-22T16:47:15Z","published":"2025-05-22T16:47:15Z","title":"CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious\n  System Prompt Generation and Refining Framework","summary":"  Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.\n","authors":["Viet Pham","Thai Le"],"pdf_url":"https://arxiv.org/pdf/2505.16888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16886v1","updated":"2025-05-22T16:41:37Z","published":"2025-05-22T16:41:37Z","title":"Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?","summary":"  With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.\n","authors":["Nour Jedidi","Yung-Sung Chuang","James Glass","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2505.16886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16881v1","updated":"2025-05-22T16:35:33Z","published":"2025-05-22T16:35:33Z","title":"CASTILLO: Characterizing Response Length Distributions of Large Language\n  Models","summary":"  Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.\n","authors":["Daniel F. Perez-Ramirez","Dejan Kostic","Magnus Boman"],"pdf_url":"https://arxiv.org/pdf/2505.16881v1.pdf","comment":"Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo"},{"id":"http://arxiv.org/abs/2503.21322v2","updated":"2025-05-22T16:34:30Z","published":"2025-03-27T10:01:16Z","title":"HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured\n  Knowledge Representation","summary":"  Standard Retrieval-Augmented Generation (RAG) relies on chunk-based\nretrieval, whereas GraphRAG advances this approach by graph-based knowledge\nrepresentation. However, existing graph-based RAG approaches are constrained by\nbinary relations, as each edge in an ordinary graph connects only two entities,\nlimiting their ability to represent the n-ary relations (n >= 2) in real-world\nknowledge. In this work, we propose HyperGraphRAG, a novel hypergraph-based RAG\nmethod that represents n-ary relational facts via hyperedges, and consists of\nknowledge hypergraph construction, retrieval, and generation. Experiments\nacross medicine, agriculture, computer science, and law demonstrate that\nHyperGraphRAG outperforms both standard RAG and previous graph-based RAG\nmethods in answer accuracy, retrieval efficiency, and generation quality.\n","authors":["Haoran Luo","Haihong E","Guanting Chen","Yandan Zheng","Xiaobao Wu","Yikai Guo","Qika Lin","Yu Feng","Zemin Kuang","Meina Song","Yifan Zhu","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2503.21322v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.04380v2","updated":"2025-05-22T16:34:02Z","published":"2025-02-05T17:21:01Z","title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data","summary":"  Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.\n","authors":["Zhenqing Ling","Daoyuan Chen","Liuyi Yao","Qianli Shen","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2502.04380v2.pdf","comment":"33 pages, 20 figures, 21 tables"},{"id":"http://arxiv.org/abs/2505.16877v1","updated":"2025-05-22T16:33:20Z","published":"2025-05-22T16:33:20Z","title":"Predicate-Conditional Conformalized Answer Sets for Knowledge Graph\n  Embeddings","summary":"  Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is\ncrucial for ensuring the reliability of downstream applications. A recent work\napplies conformal prediction to KGE methods, providing uncertainty estimates by\ngenerating a set of answers that is guaranteed to include the true answer with\na predefined confidence level. However, existing methods provide probabilistic\nguarantees averaged over a reference set of queries and answers (marginal\ncoverage guarantee). In high-stakes applications such as medical diagnosis, a\nstronger guarantee is often required: the predicted sets must provide\nconsistent coverage per query (conditional coverage guarantee). We propose\nCondKGCP, a novel method that approximates predicate-conditional coverage\nguarantees while maintaining compact prediction sets. CondKGCP merges\npredicates with similar vector representations and augments calibration with\nrank information. We prove the theoretical guarantees and demonstrate empirical\neffectiveness of CondKGCP by comprehensive evaluations.\n","authors":["Yuqicheng Zhu","Daniel Hernández","Yuan He","Zifeng Ding","Bo Xiong","Evgeny Kharlamov","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2505.16877v1.pdf","comment":"Accepted to the Finding of ACL 2025"},{"id":"http://arxiv.org/abs/2505.16875v1","updated":"2025-05-22T16:31:43Z","published":"2025-05-22T16:31:43Z","title":"T2I-ConBench: Text-to-Image Benchmark for Continual Post-training","summary":"  Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.\n","authors":["Zhehao Huang","Yuhang Liu","Yixin Lou","Zhengbao He","Mingzhen He","Wenxing Zhou","Tao Li","Kehan Li","Zeyi Huang","Xiaolin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.16875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16860v1","updated":"2025-05-22T16:19:19Z","published":"2025-05-22T16:19:19Z","title":"GCAL: Adapting Graph Models to Evolving Domain Shifts","summary":"  This paper addresses the challenge of graph domain adaptation on evolving,\nmultiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation\nmethods are confined to single-step adaptation, making them ineffective in\nhandling continuous domain shifts and prone to catastrophic forgetting. This\npaper introduces the Graph Continual Adaptive Learning (GCAL) method, designed\nto enhance model sustainability and adaptability across various graph domains.\nGCAL employs a bilevel optimization strategy. The \"adapt\" phase uses an\ninformation maximization approach to fine-tune the model with new graph domains\nwhile re-adapting past memories to mitigate forgetting. Concurrently, the\n\"generate memory\" phase, guided by a theoretical lower bound derived from\ninformation bottleneck theory, involves a variational memory graph generation\nmodule to condense original graphs into memories. Extensive experimental\nevaluations demonstrate that GCAL substantially outperforms existing methods in\nterms of adaptability and knowledge retention.\n","authors":["Ziyue Qiao","Qianyi Cai","Hao Dong","Jiawei Gu","Pengyang Wang","Meng Xiao","Xiao Luo","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.16860v1.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2503.15463v3","updated":"2025-05-22T16:17:34Z","published":"2025-03-19T17:41:46Z","title":"From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment","summary":"  Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems.\n","authors":["Jia-Nan Li","Jian Guan","Songhao Wu","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2503.15463v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16856v1","updated":"2025-05-22T16:14:08Z","published":"2025-05-22T16:14:08Z","title":"Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only","summary":"  Improving the performance of pre-trained policies through online\nreinforcement learning (RL) is a critical yet challenging topic. Existing\nonline RL fine-tuning methods require continued training with offline\npretrained Q-functions for stability and performance. However, these offline\npretrained Q-functions commonly underestimate state-action pairs beyond the\noffline dataset due to the conservatism in most offline RL methods, which\nhinders further exploration when transitioning from the offline to the online\nsetting. Additionally, this requirement limits their applicability in scenarios\nwhere only pre-trained policies are available but pre-trained Q-functions are\nabsent, such as in imitation learning (IL) pre-training. To address these\nchallenges, we propose a method for efficient online RL fine-tuning using\nsolely the offline pre-trained policy, eliminating reliance on pre-trained\nQ-functions. We introduce PORL (Policy-Only Reinforcement Learning\nFine-Tuning), which rapidly initializes the Q-function from scratch during the\nonline phase to avoid detrimental pessimism. Our method not only achieves\ncompetitive performance with advanced offline-to-online RL algorithms and\nonline RL approaches that leverage data or policies prior, but also pioneers a\nnew path for directly fine-tuning behavior cloning (BC) policies.\n","authors":["Wei Xiao","Jiacheng Liu","Zifeng Zhuang","Runze Suo","Shangke Lyu","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16854v1","updated":"2025-05-22T16:13:29Z","published":"2025-05-22T16:13:29Z","title":"Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models","summary":"  Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.\n","authors":["Jiaqi Wang","Kevin Qinghong Lin","James Cheng","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2505.16854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16845v1","updated":"2025-05-22T16:10:01Z","published":"2025-05-22T16:10:01Z","title":"Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame\n  Rate","summary":"  Most neural speech codecs achieve bitrate adjustment through intra-frame\nmechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,\nspeech segments inherently have time-varying information density (e.g., silent\nintervals versus voiced regions). This property makes CFR not optimal in terms\nof bitrate and token sequence length, hindering efficiency in real-time\napplications. In this work, we propose a Temporally Flexible Coding (TFC)\ntechnique, introducing variable frame rate (VFR) into neural speech codecs for\nthe first time. TFC enables seamlessly tunable average frame rates and\ndynamically allocates frame rates based on temporal entropy. Experimental\nresults show that a codec with TFC achieves optimal reconstruction quality with\nhigh flexibility, and maintains competitive performance even at lower frame\nrates. Our approach is promising for the integration with other efforts to\ndevelop low-frame-rate neural speech codecs for more efficient downstream\ntasks.\n","authors":["Hanglei Zhang","Yiwei Guo","Zhihan Li","Xiang Hao","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2505.16845v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2408.15966v3","updated":"2025-05-22T16:09:52Z","published":"2024-08-28T17:38:44Z","title":"More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding","summary":"  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n","authors":["Yuan Tang","Xu Han","Xianzhi Li","Qiao Yu","Jinfeng Xu","Yixue Hao","Long Hu","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2408.15966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16836v1","updated":"2025-05-22T16:05:06Z","published":"2025-05-22T16:05:06Z","title":"Fact-R1: Towards Explainable Video Misinformation Detection with Deep\n  Reasoning","summary":"  The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification.\n","authors":["Fanrui Zhang","Dian Li","Qiang Zhang"," Chenjun"," sinbadliu","Junxiong Lin","Jiahong Yan","Jiawei Liu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2505.16836v1.pdf","comment":"28 pages, 27 figures"},{"id":"http://arxiv.org/abs/2505.16834v1","updated":"2025-05-22T16:05:02Z","published":"2025-05-22T16:05:02Z","title":"SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis","summary":"  Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.\n","authors":["Shuang Sun","Huatong Song","Yuhao Wang","Ruiyang Ren","Jinhao Jiang","Junjie Zhang","Fei Bai","Jia Deng","Wayne Xin Zhao","Zheng Liu","Lei Fang","Zhongyuan Wang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.16834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12822v2","updated":"2025-05-22T16:03:57Z","published":"2025-05-19T08:05:13Z","title":"Emergent Specialization: Rare Token Neurons in Language Models","summary":"  Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization.\n","authors":["Jing Liu","Haozheng Wang","Yueheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.12822v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.16832v1","updated":"2025-05-22T16:02:18Z","published":"2025-05-22T16:02:18Z","title":"From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework\n  for Pedagogical Visualization","summary":"  While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.\n","authors":["Haonian Ji","Shi Qiu","Siyang Xin","Siwei Han","Zhaorun Chen","Hongyi Wang","Dake Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2505.16832v1.pdf","comment":"16 pages; 7 figures"},{"id":"http://arxiv.org/abs/2505.16831v1","updated":"2025-05-22T16:02:10Z","published":"2025-05-22T16:02:10Z","title":"Unlearning Isn't Deletion: Investigating Reversibility of Machine\n  Unlearning in LLMs","summary":"  Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.\n","authors":["Xiaoyu Xu","Xiang Yue","Yang Liu","Qingqing Ye","Haibo Hu","Minxin Du"],"pdf_url":"https://arxiv.org/pdf/2505.16831v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2505.16827v1","updated":"2025-05-22T16:01:06Z","published":"2025-05-22T16:01:06Z","title":"GUI-explorer: Autonomous Exploration and Mining of Transition-aware\n  Knowledge for GUI Agent","summary":"  GUI automation faces critical challenges in dynamic environments. MLLMs\nsuffer from two key issues: misinterpreting UI components and outdated\nknowledge. Traditional fine-tuning methods are costly for app-specific\nknowledge updates. We propose GUI-explorer, a training-free GUI agent that\nincorporates two fundamental mechanisms: (1) Autonomous Exploration of\nFunction-aware Trajectory. To comprehensively cover all application\nfunctionalities, we design a Function-aware Task Goal Generator that\nautomatically constructs exploration goals by analyzing GUI structural\ninformation (e.g., screenshots and activity hierarchies). This enables\nsystematic exploration to collect diverse trajectories. (2) Unsupervised Mining\nof Transition-aware Knowledge. To establish precise screen-operation logic, we\ndevelop a Transition-aware Knowledge Extractor that extracts effective\nscreen-operation logic through unsupervised analysis the state transition of\nstructured interaction triples (observation, action, outcome). This eliminates\nthe need for human involvement in knowledge extraction. With a task success\nrate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows\nsignificant improvements over SOTA agents. It requires no parameter updates for\nnew apps. GUI-explorer is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/GUI-explorer.\n","authors":["Bin Xie","Rui Shao","Gongwei Chen","Kaiwen Zhou","Yinchuan Li","Jie Liu","Min Zhang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2505.16827v1.pdf","comment":"ACL 2025. Github: https://github.com/JiuTian-VL/GUI-explorer"},{"id":"http://arxiv.org/abs/2505.16826v1","updated":"2025-05-22T16:00:33Z","published":"2025-05-22T16:00:33Z","title":"KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in\n  Mathematical Reasoning","summary":"  Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.\n","authors":["Wei Sun","Wen Yang","Pu Jian","Qianlong Du","Fuwei Cui","Shuo Ren","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14510v2","updated":"2025-05-22T15:50:56Z","published":"2025-05-20T15:39:05Z","title":"BACON: A fully explainable AI model with graded logic for decision\n  making problems","summary":"  As machine learning models and autonomous agents are increasingly deployed in\nhigh-stakes, real-world domains such as healthcare, security, finance, and\nrobotics, the need for transparent and trustworthy explanations has become\ncritical. To ensure end-to-end transparency of AI decisions, we need models\nthat are not only accurate but also fully explainable and human-tunable. We\nintroduce BACON, a novel framework for automatically training explainable AI\nmodels for decision making problems using graded logic. BACON achieves high\npredictive accuracy while offering full structural transparency and precise,\nlogic-based symbolic explanations, enabling effective human-AI collaboration\nand expert-guided refinement. We evaluate BACON with a diverse set of\nscenarios: classic Boolean approximation, Iris flower classification, house\npurchasing decisions and breast cancer diagnosis. In each case, BACON provides\nhigh-performance models while producing compact, human-verifiable decision\nlogic. These results demonstrate BACON's potential as a practical and\nprincipled approach for delivering crisp, trustworthy explainable AI.\n","authors":["Haishi Bai","Jozo Dujmovic","Jianwu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16813v1","updated":"2025-05-22T15:50:45Z","published":"2025-05-22T15:50:45Z","title":"Dynamic Reservoir Computing with Physical Neuromorphic Networks","summary":"  Reservoir Computing (RC) with physical systems requires an understanding of\nthe underlying structure and internal dynamics of the specific physical\nreservoir. In this study, physical nano-electronic networks with neuromorphic\ndynamics are investigated for their use as physical reservoirs in an RC\nframework. These neuromorphic networks operate as dynamic reservoirs, with node\nactivities in general coupled to the edge dynamics through nonlinear\nnano-electronic circuit elements, and the reservoir outputs influenced by the\nunderlying network connectivity structure. This study finds that networks with\nvarying degrees of sparsity generate more useful nonlinear temporal outputs for\ndynamic RC compared to dense networks. Dynamic RC is also tested on an\nautonomous multivariate chaotic time series prediction task with networks of\nvarying densities, which revealed the importance of network sparsity in\nmaintaining network activity and overall dynamics, that in turn enabled the\nlearning of the chaotic Lorenz63 system's attractor behavior.\n","authors":["Yinhao Xu","Georg A. Gottwald","Zdenka Kuncic"],"pdf_url":"https://arxiv.org/pdf/2505.16813v1.pdf","comment":"8 pages, 8 figures, IJCNN 2025, accepted"},{"id":"http://arxiv.org/abs/2504.05695v3","updated":"2025-05-22T15:45:56Z","published":"2025-04-08T05:37:38Z","title":"Architecture independent generalization bounds for overparametrized deep\n  ReLU networks","summary":"  We prove that overparametrized neural networks are able to generalize with a\ntest error that is independent of the level of overparametrization, and\nindependent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds\nthat only depend on the metric geometry of the test and training sets, on the\nregularity properties of the activation function, and on the operator norms of\nthe weights and norms of biases. For overparametrized deep ReLU networks with a\ntraining sample size bounded by the input space dimension, we explicitly\nconstruct zero loss minimizers without use of gradient descent, and prove that\nthe generalization error is independent of the network architecture.\n","authors":["Thomas Chen","Chun-Kai Kevin Chien","Patricia Muñoz Ewald","Andrew G. Moore"],"pdf_url":"https://arxiv.org/pdf/2504.05695v3.pdf","comment":"AMS Latex, 13 pages. Both main theorems are now in Section 1"},{"id":"http://arxiv.org/abs/2407.10973v4","updated":"2025-05-22T15:45:34Z","published":"2024-07-15T17:59:57Z","title":"Make-An-Agent: A Generalizable Policy Network Generator with\n  Behavior-Prompted Diffusion","summary":"  Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/\n","authors":["Yongyuan Liang","Tingqiang Xu","Kaizhe Hu","Guangqi Jiang","Furong Huang","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2407.10973v4.pdf","comment":"Annual Conference on Neural Information Processing Systems 38"},{"id":"http://arxiv.org/abs/2503.17682v2","updated":"2025-05-22T15:42:20Z","published":"2025-03-22T07:40:20Z","title":"Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback","summary":"  Multimodal large language models (MLLMs) are essential for building\ngeneral-purpose AI assistants; however, they pose increasing safety risks. How\ncan we ensure safety alignment of MLLMs to prevent undesired behaviors? Going\nfurther, it is critical to explore how to fine-tune MLLMs to preserve\ncapabilities while meeting safety constraints. Fundamentally, this challenge\ncan be formulated as a min-max optimization problem. However, existing datasets\nhave not yet disentangled single preference signals into explicit safety\nconstraints, hindering systematic investigation in this direction. Moreover, it\nremains an open question whether such constraints can be effectively\nincorporated into the optimization process for multi-modal models. In this\nwork, we present the first exploration of the Safe RLHF-V -- the first\nmultimodal safety alignment framework. The framework consists of:\n$\\mathbf{(I)}$ BeaverTails-V, the first open-source dataset featuring dual\npreference annotations for helpfulness and safety, supplemented with\nmulti-level safety labels (minor, moderate, severe); $\\mathbf{(II)}$\nBeaver-Guard-V, a multi-level guardrail system to proactively defend against\nunsafe queries and adversarial attacks. Applying the guard model over five\nrounds of filtering and regeneration significantly enhances the precursor\nmodel's overall safety by an average of 40.9%. $\\mathbf{(III)}$ Based on dual\npreference, we initiate the first exploration of multi-modal safety alignment\nwithin a constrained optimization. Experimental results demonstrate that Safe\nRLHF effectively improves both model helpfulness and safety. Specifically, Safe\nRLHF-V enhances model safety by 34.2% and helpfulness by 34.3%.\n","authors":["Jiaming Ji","Xinyu Chen","Rui Pan","Conghui Zhang","Han Zhu","Jiahao Li","Donghai Hong","Boyuan Chen","Jiayi Zhou","Kaile Wang","Juntao Dai","Chi-Min Chan","Yida Tang","Sirui Han","Yike Guo","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2503.17682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16801v1","updated":"2025-05-22T15:40:56Z","published":"2025-05-22T15:40:56Z","title":"A modular framework for automated evaluation of procedural content\n  generation in serious games with deep reinforcement learning agents","summary":"  Serious Games (SGs) are nowadays shifting focus to include procedural content\ngeneration (PCG) in the development process as a means of offering personalized\nand enhanced player experience. However, the development of a framework to\nassess the impact of PCG techniques when integrated into SGs remains\nparticularly challenging. This study proposes a methodology for automated\nevaluation of PCG integration in SGs, incorporating deep reinforcement learning\n(DRL) game testing agents. To validate the proposed framework, a previously\nintroduced SG featuring card game mechanics and incorporating three different\nversions of PCG for nonplayer character (NPC) creation has been deployed.\nVersion 1 features random NPC creation, while versions 2 and 3 utilize a\ngenetic algorithm approach. These versions are used to test the impact of\ndifferent dynamic SG environments on the proposed framework's agents. The\nobtained results highlight the superiority of the DRL game testing agents\ntrained on Versions 2 and 3 over those trained on Version 1 in terms of win\nrate (i.e. number of wins per played games) and training time. More\nspecifically, within the execution of a test emulating regular gameplay, both\nVersions 2 and 3 peaked at a 97% win rate and achieved statistically\nsignificant higher (p=0009) win rates compared to those achieved in Version 1\nthat peaked at 94%. Overall, results advocate towards the proposed framework's\ncapability to produce meaningful data for the evaluation of procedurally\ngenerated content in SGs.\n","authors":["Eleftherios Kalafatis","Konstantinos Mitsis","Konstantia Zarkogianni","Maria Athanasiou","Konstantina Nikita"],"pdf_url":"https://arxiv.org/pdf/2505.16801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01031v3","updated":"2025-05-22T15:40:11Z","published":"2024-12-02T01:27:47Z","title":"Evaluating Automated Radiology Report Quality through Fine-Grained\n  Phrasal Grounding of Clinical Findings","summary":"  Several evaluation metrics have been developed recently to automatically\nassess the quality of generative AI reports for chest radiographs based only on\ntextual information using lexical, semantic, or clinical named entity\nrecognition methods. In this paper, we develop a new method of report quality\nevaluation by first extracting fine-grained finding patterns capturing the\nlocation, laterality, and severity of a large number of clinical findings. We\nthen performed phrasal grounding to localize their associated anatomical\nregions on chest radiograph images. The textual and visual measures are then\ncombined to rate the quality of the generated reports. We present results that\ncompare this evaluation metric with other textual metrics on a gold standard\ndataset derived from the MIMIC collection and show its robustness and\nsensitivity to factual errors.\n","authors":["Razi Mahmood","Pingkun Yan","Diego Machado Reyes","Ge Wang","Mannudeep K. Kalra","Parisa Kaviani","Joy T. Wu","Tanveer Syeda-Mahmood"],"pdf_url":"https://arxiv.org/pdf/2412.01031v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16798v1","updated":"2025-05-22T15:38:37Z","published":"2025-05-22T15:38:37Z","title":"SEED: Speaker Embedding Enhancement Diffusion Model","summary":"  A primary challenge when deploying speaker recognition systems in real-world\napplications is performance degradation caused by environmental mismatch. We\npropose a diffusion-based method that takes speaker embeddings extracted from a\npre-trained speaker recognition model and generates refined embeddings. For\ntraining, our approach progressively adds Gaussian noise to both clean and\nnoisy speaker embeddings extracted from clean and noisy speech, respectively,\nvia forward process of a diffusion model, and then reconstructs them to clean\nembeddings in the reverse process. While inferencing, all embeddings are\nregenerated via diffusion process. Our method needs neither speaker label nor\nany modification to the existing speaker recognition pipeline. Experiments on\nevaluation sets simulating environment mismatch scenarios show that our method\ncan improve recognition accuracy by up to 19.6% over baseline models while\nretaining performance on conventional scenarios. We publish our code here\nhttps://github.com/kaistmm/seed-pytorch\n","authors":["KiHyun Nam","Jungwoo Heo","Jee-weon Jung","Gangin Park","Chaeyoung Jung","Ha-Jin Yu","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2505.16798v1.pdf","comment":"Accepted to Interspeech 2025. The official code can be found at\n  https://github.com/kaistmm/seed-pytorch"},{"id":"http://arxiv.org/abs/2505.16792v1","updated":"2025-05-22T15:34:33Z","published":"2025-05-22T15:34:33Z","title":"REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment\n  Supercharges Diffusion Training","summary":"  Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .\n","authors":["Ziqiao Wang","Wangbo Zhao","Yuhao Zhou","Zekai Li","Zhiyuan Liang","Mingjia Shi","Xuanlei Zhao","Pengfei Zhou","Kaipeng Zhang","Zhangyang Wang","Kai Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2505.16792v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2505.16791v1","updated":"2025-05-22T15:32:50Z","published":"2025-05-22T15:32:50Z","title":"Cohort-Based Active Modality Acquisition","summary":"  Real-world machine learning applications often involve data from multiple\nmodalities that must be integrated effectively to make robust predictions.\nHowever, in many practical settings, not all modalities are available for every\nsample, and acquiring additional modalities can be costly. This raises the\nquestion: which samples should be prioritized for additional modality\nacquisition when resources are limited? While prior work has explored\nindividual-level acquisition strategies and training-time active learning\nparadigms, test-time and cohort-based acquisition remain underexplored despite\ntheir importance in many real-world settings. We introduce Cohort-based Active\nModality Acquisition (CAMA), a novel test-time setting to formalize the\nchallenge of selecting which samples should receive additional modalities. We\nderive acquisition strategies that leverage a combination of generative\nimputation and discriminative modeling to estimate the expected benefit of\nacquiring missing modalities based on common evaluation metrics. We also\nintroduce upper-bound heuristics that provide performance ceilings to benchmark\nacquisition strategies. Experiments on common multimodal datasets demonstrate\nthat our proposed imputation-based strategies can more effectively guide the\nacquisition of new samples in comparison to those relying solely on unimodal\ninformation, entropy guidance, and random selections. Our work provides an\neffective solution for optimizing modality acquisition at the cohort level,\nenabling better utilization of resources in constrained settings.\n","authors":["Tillmann Rheude","Roland Eils","Benjamin Wild"],"pdf_url":"https://arxiv.org/pdf/2505.16791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16790v1","updated":"2025-05-22T15:30:17Z","published":"2025-05-22T15:30:17Z","title":"Learning Flexible Forward Trajectories for Masked Molecular Diffusion","summary":"  Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks.\n","authors":["Hyunjin Seo","Taewon Kim","Sihyun Yu","SungSoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2505.16790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16789v1","updated":"2025-05-22T15:30:00Z","published":"2025-05-22T15:30:00Z","title":"Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected\n  Vulnerability","summary":"  As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.\n","authors":["Punya Syon Pandey","Samuel Simko","Kellin Pelrine","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2505.16789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16787v1","updated":"2025-05-22T15:28:50Z","published":"2025-05-22T15:28:50Z","title":"Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce","summary":"  Model-based reinforcement learning (MBRL) offers an intuitive way to increase\nthe sample efficiency of model-free RL methods by simultaneously training a\nworld model that learns to predict the future. MBRL methods have progressed by\nlargely prioritising the actor; optimising the world model learning has been\nneglected meanwhile. Improving the fidelity of the world model and reducing its\ntime to convergence can yield significant downstream benefits, one of which is\nimproving the ensuing performance of any actor it may train. We propose a novel\napproach that anticipates and actively seeks out high-entropy states using\nshort-horizon latent predictions generated by the world model, offering a\nprincipled alternative to traditional curiosity-driven methods that chase\nonce-novel states well after they were stumbled into. While many model\npredictive control (MPC) based methods offer similar alternatives, they\ntypically lack commitment, synthesising multi step plans after every step. To\nmitigate this, we present a hierarchical planner that dynamically decides when\nto replan, planning horizon length, and the weighting between reward and\nentropy. While our method can theoretically be applied to any model that trains\nits own actors with solely model generated data, we have applied it to just\nDreamer as a proof of concept. Our method finishes the Miniworld procedurally\ngenerated mazes 50% faster than base Dreamer at convergence and the policy\ntrained in imagination converges in only 60% of the environment steps that base\nDreamer needs.\n","authors":["Ashish Sundar","Chunbo Luo","Xiaoyang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16787v1.pdf","comment":"9 pages without appendix, 15 Figures, preprint"},{"id":"http://arxiv.org/abs/2505.16785v1","updated":"2025-05-22T15:28:25Z","published":"2025-05-22T15:28:25Z","title":"CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of\n  Large Language Models","summary":"  Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification.\n","authors":["Zhenzhen Ren","GuoBiao Li","Sheng Li","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04233v3","updated":"2025-05-22T15:28:13Z","published":"2024-12-05T15:09:51Z","title":"HyperMARL: Adaptive Hypernetworks for Multi-Agent RL","summary":"  Adaptability to specialised or homogeneous behaviours is critical in\ncooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS)\ntechniques, common for efficient adaptation, often limit behavioural diversity\ndue to cross-agent gradient interference, which we show can be exacerbated by\nthe coupling of observations and agent IDs. Current remedies typically add\ncomplexity through altered objectives, manual preset diversity levels, or\nsequential updates. We ask: can shared policies adapt without these\ncomplexities? We propose HyperMARL, a PS approach using hypernetworks for\ndynamic agent-specific parameters, without altering the RL objective or\nrequiring preset diversity levels. HyperMARL's explicit decoupling of\nobservation- and agent-conditioned gradients empirically reduces policy\ngradient variance, facilitates shared-policy adaptation (including\nspecialisation), and helps mitigate cross-agent interference. Across diverse\nMARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or\nmixed behaviours, HyperMARL achieves competitive performance against key\nbaselines -- fully shared, non-parameter sharing, and three diversity-promoting\nmethods -- while preserving behavioural diversity comparable to non-parameter\nsharing. These findings establish HyperMARL as a versatile approach for\nadaptive MARL. The code is publicly available at\nhttps://github.com/KaleabTessera/HyperMARL.\n","authors":["Kale-ab Abebe Tessera","Arrasy Rahman","Amos Storkey","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2412.04233v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16781v1","updated":"2025-05-22T15:26:48Z","published":"2025-05-22T15:26:48Z","title":"Fuzzy Information Evolution with Three-Way Decision in Social Network\n  Group Decision-Making","summary":"  In group decision-making (GDM) scenarios, uncertainty, dynamic social\nstructures, and vague information present major challenges for traditional\nopinion dynamics models. To address these issues, this study proposes a novel\nsocial network group decision-making (SNGDM) framework that integrates\nthree-way decision (3WD) theory, dynamic network reconstruction, and linguistic\nopinion representation. First, the 3WD mechanism is introduced to explicitly\nmodel hesitation and ambiguity in agent judgments, thereby preventing\nirrational decisions. Second, a connection adjustment rule based on opinion\nsimilarity is developed, enabling agents to adaptively update their\ncommunication links and better reflect the evolving nature of social\nrelationships. Third, linguistic terms are used to describe agent opinions,\nallowing the model to handle subjective, vague, or incomplete information more\neffectively. Finally, an integrated multi-agent decision-making framework is\nconstructed, which simultaneously considers individual uncertainty, opinion\nevolution, and network dynamics. The proposed model is applied to a multi-UAV\ncooperative decision-making scenario, where simulation results and consensus\nanalysis demonstrate its effectiveness. Experimental comparisons further verify\nthe advantages of the algorithm in enhancing system stability and representing\nrealistic decision-making behaviors.\n","authors":["Qianlei Jia","Xinliang Zhou","Ondrej Krejcar","Enrique Herrera-Viedma"],"pdf_url":"https://arxiv.org/pdf/2505.16781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10612v3","updated":"2025-05-22T15:22:06Z","published":"2025-04-14T18:10:58Z","title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for\n  Generative Modeling","summary":"  The most widely used generative models map noise and data distributions by\nmatching flows or scores. However, they struggle to incorporate partial\nobservations and additional priors--something energy-based models (EBMs) handle\nelegantly by simply adding corresponding scalar energy terms. We address this\nissue by proposing Energy Matching, a framework that endows flow-based\napproaches with the flexibility of EBMs. Far from the data manifold, samples\nmove along curl-free, optimal transport paths from noise to data. As they\napproach the data manifold, an entropic energy term guides the system into a\nBoltzmann equilibrium distribution, explicitly capturing the underlying\nlikelihood structure of the data. We parameterize this dynamic with a single\ntime-independent scalar field, which serves as both a powerful generator and a\nflexible prior for effective regularization of inverse problems. Our method\nsubstantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in\nterms of fidelity, while retaining simulation-free training of transport-based\napproaches away from the data manifold. Furthermore, we leverage the method's\nflexibility to introduce an interaction energy that supports diverse mode\nexploration, which we demonstrate in a controlled protein-generation setting.\nOur approach focuses on learning a scalar potential energy--without\ntime-conditioning, auxiliary generators, or additional networks--which marks a\nsignificant departure from recent EBM methods. We believe that this simplified\nframework significantly advances EBMs capabilities and paves the way for their\nwider adoption in generative modeling across diverse domains.\n","authors":["Michal Balcerak","Tamaz Amiranashvili","Antonio Terpin","Suprosanna Shit","Lea Bogensperger","Sebastian Kaltenbach","Petros Koumoutsakos","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2504.10612v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08701v2","updated":"2025-05-22T15:19:34Z","published":"2024-11-13T15:42:28Z","title":"TRACE: Transformer-based Risk Assessment for Clinical Evaluation","summary":"  We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),\na novel method for clinical risk assessment based on clinical data, leveraging\nthe self-attention mechanism for enhanced feature interaction and result\ninterpretation. Our approach is able to handle different data modalities,\nincluding continuous, categorical and multiple-choice (checkbox) attributes.\nThe proposed architecture features a shared representation of the clinical data\nobtained by integrating specialized embeddings of each data modality, enabling\nthe detection of high-risk individuals using Transformer encoder layers. To\nassess the effectiveness of the proposed method, a strong baseline based on\nnon-negative multi-layer perceptrons (MLPs) is introduced. The proposed method\noutperforms various baselines widely used in the domain of clinical risk\nassessment, while effectively handling missing values. In terms of\nexplainability, our Transformer-based method offers easily interpretable\nresults via attention weights, further enhancing the clinicians'\ndecision-making process.\n","authors":["Dionysis Christopoulos","Sotiris Spanos","Valsamis Ntouskos","Konstantinos Karantzalos"],"pdf_url":"https://arxiv.org/pdf/2411.08701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15046v2","updated":"2025-05-22T15:16:47Z","published":"2025-05-21T03:07:47Z","title":"ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart\n  Understanding","summary":"  The emergence of Multi-modal Large Language Models (MLLMs) presents new\nopportunities for chart understanding. However, due to the fine-grained nature\nof these tasks, applying MLLMs typically requires large, high-quality datasets\nfor task-specific fine-tuning, leading to high data collection and training\ncosts. To address this, we propose ChartCards, a unified chart-metadata\ngeneration framework for multi-task chart understanding. ChartCards\nsystematically synthesizes various chart information, including data tables,\nvisualization code, visual elements, and multi-dimensional semantic captions.\nBy structuring this information into organized metadata, ChartCards enables a\nsingle chart to support multiple downstream tasks, such as text-to-chart\nretrieval, chart summarization, chart-to-table conversion, chart description,\nand chart question answering. Using ChartCards, we further construct MetaChart,\na large-scale high-quality dataset containing 10,862 data tables, 85K charts,\nand 170 K high-quality chart captions. We validate the dataset through\nqualitative crowdsourcing evaluations and quantitative fine-tuning experiments\nacross various chart understanding tasks. Fine-tuning six different models on\nMetaChart resulted in an average performance improvement of 5% across all\ntasks. The most notable improvements are seen in text-to-chart retrieval and\nchart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements\nof 17% and 28%, respectively.\n","authors":["Yifan Wu","Lutao Yan","Leixian Shen","Yinan Mei","Jiannan Wang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2505.15046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16773v1","updated":"2025-05-22T15:15:17Z","published":"2025-05-22T15:15:17Z","title":"Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining\n  vs. ImageNet Transfer Learning for Dermatological Diagnosis","summary":"  Deep learning has transformed computer vision but relies heavily on large\nlabeled datasets and computational resources. Transfer learning, particularly\nfine-tuning pretrained models, offers a practical alternative; however, models\npretrained on natural image datasets such as ImageNet may fail to capture\ndomain-specific characteristics in medical imaging. This study introduces an\nunsupervised learning framework that extracts high-value dermatological\nfeatures instead of relying solely on ImageNet-based pretraining. We employ a\nVariational Autoencoder (VAE) trained from scratch on a proprietary\ndermatological dataset, allowing the model to learn a structured and clinically\nrelevant latent space. This self-supervised feature extractor is then compared\nto an ImageNet-pretrained backbone under identical classification conditions,\nhighlighting the trade-offs between general-purpose and domain-specific\npretraining. Our results reveal distinct learning patterns. The self-supervised\nmodel achieves a final validation loss of 0.110 (-33.33%), while the\nImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.\nAccuracy trends confirm this: the self-supervised model improves from 45% to\n65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained\nmodel reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting\ngap increasing to +0.060. These findings suggest that while ImageNet\npretraining accelerates convergence, it also amplifies overfitting on\nnon-clinically relevant features. In contrast, self-supervised learning\nachieves steady improvements, stronger generalization, and superior\nadaptability, underscoring the importance of domain-specific feature extraction\nin medical imaging.\n","authors":["Iván Matas","Carmen Serrano","Miguel Nogales","David Moreno","Lara Ferrándiz","Teresa Ojeda","Begoña Acha"],"pdf_url":"https://arxiv.org/pdf/2505.16773v1.pdf","comment":"6 pages, 2 tables, 2 figures"},{"id":"http://arxiv.org/abs/2505.16771v1","updated":"2025-05-22T15:12:48Z","published":"2025-05-22T15:12:48Z","title":"Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A\n  Comprehensive Review","summary":"  This paper presents a comprehensive synthesis of major breakthroughs in\nartificial intelligence (AI) over the past fifteen years, integrating\nhistorical, theoretical, and technological perspectives. It identifies key\ninflection points in AI' s evolution by tracing the convergence of\ncomputational resources, data access, and algorithmic innovation. The analysis\nhighlights how researchers enabled GPU based model training, triggered a data\ncentric shift with ImageNet, simplified architectures through the Transformer,\nand expanded modeling capabilities with the GPT series. Rather than treating\nthese advances as isolated milestones, the paper frames them as indicators of\ndeeper paradigm shifts. By applying concepts from statistical learning theory\nsuch as sample complexity and data efficiency, the paper explains how\nresearchers translated breakthroughs into scalable solutions and why the field\nmust now embrace data centric approaches. In response to rising privacy\nconcerns and tightening regulations, the paper evaluates emerging solutions\nlike federated learning, privacy enhancing technologies (PETs), and the data\nsite paradigm, which reframe data access and security. In cases where real\nworld data remains inaccessible, the paper also assesses the utility and\nconstraints of mock and synthetic data generation. By aligning technical\ninsights with evolving data infrastructure, this study offers strategic\nguidance for future AI research and policy development.\n","authors":["Beyazit Bestami Yuksel","Ayse Yilmazer Metin"],"pdf_url":"https://arxiv.org/pdf/2505.16771v1.pdf","comment":"10 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.16765v1","updated":"2025-05-22T15:07:34Z","published":"2025-05-22T15:07:34Z","title":"When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak\n  Attack on LLMs via Steganographic Techniques","summary":"  Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66\n","authors":["Jianing Geng","Biao Yi","Zekun Fei","Tongxi Wu","Lihai Nie","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2505.16765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07849v2","updated":"2025-05-22T14:59:12Z","published":"2025-02-11T10:29:29Z","title":"Classifier-Free Guidance: From High-Dimensional Analysis to Generalized\n  Guidance Forms","summary":"  Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion and\nflow-based generative models, enabling high-quality conditional generation. A\nkey theoretical challenge is characterizing the distribution induced by CFG,\nparticularly in high-dimensional settings relevant to real-world data. Previous\nworks have shown that CFG modifies the target distribution, steering it towards\na distribution sharper than the target one, more shifted towards the boundary\nof the class. In this work, we provide a high-dimensional analysis of CFG,\nshowing that these distortions vanish as the data dimension grows. We present a\nblessing-of-dimensionality result demonstrating that in sufficiently high and\ninfinite dimensions, CFG accurately reproduces the target distribution. Using\nour high-dimensional theory, we show that there is a large family of guidances\nenjoying this property, in particular non-linear CFG generalizations. We study\na simple non-linear power-law version, for which we demonstrate improved\nrobustness, sample fidelity and diversity. Our findings are validated with\nexperiments on class-conditional and text-to-image generation using\nstate-of-the-art diffusion and flow-matching models.\n","authors":["Krunoslav Lehman Pavasovic","Jakob Verbeek","Giulio Biroli","Marc Mezard"],"pdf_url":"https://arxiv.org/pdf/2502.07849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16752v1","updated":"2025-05-22T14:58:53Z","published":"2025-05-22T14:58:53Z","title":"Action is All You Need: Dual-Flow Generative Ranking Network for\n  Recommendation","summary":"  We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm.\n","authors":["Hao Guo","Erpeng Xue","Lei Huang","Shichao Wang","Xiaolei Wang","Lei Wang","Jinpeng Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2505.16752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16743v1","updated":"2025-05-22T14:53:53Z","published":"2025-05-22T14:53:53Z","title":"TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative\n  Metric-driven Pruning","summary":"  Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM\n","authors":["Florentin Beck","William Rudman","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2505.16743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04598v3","updated":"2025-05-22T14:53:31Z","published":"2025-03-06T16:40:48Z","title":"HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization","summary":"  Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.\n","authors":["Zhijian Zhuo","Yutao Zeng","Ya Wang","Sijun Zhang","Jian Yang","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04598v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15091v2","updated":"2025-05-22T14:53:00Z","published":"2025-05-21T04:25:18Z","title":"ThinkRec: Thinking-based recommendation via LLM","summary":"  Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.\n","authors":["Qihang Yu","Kairui Fu","Shengyu Zhang","Zheqi Lv","Fan Wu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2505.15091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16740v1","updated":"2025-05-22T14:52:59Z","published":"2025-05-22T14:52:59Z","title":"Robust Vision-Based Runway Detection through Conformal Prediction and\n  Conformal mAP","summary":"  We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain.\n","authors":["Alya Zouzou","Léo andéol","Mélanie Ducoffe","Ryma Boumazouza"],"pdf_url":"https://arxiv.org/pdf/2505.16740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16737v1","updated":"2025-05-22T14:52:10Z","published":"2025-05-22T14:52:10Z","title":"Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing\n  Optimization","summary":"  The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.\n","authors":["Chengcan Wu","Zhixin Zhang","Zeming Wei","Yihao Zhang","Meng Sun"],"pdf_url":"https://arxiv.org/pdf/2505.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16735v1","updated":"2025-05-22T14:49:46Z","published":"2025-05-22T14:49:46Z","title":"Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in\n  Open-Vocabulary Keyword Spotting","summary":"  For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct comprehensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach.\n","authors":["Youngmoon Jung","Yong-Hyeok Lee","Myunghun Jung","Jaeyoung Roh","Chang Woo Han","Hoon-Young Cho"],"pdf_url":"https://arxiv.org/pdf/2505.16735v1.pdf","comment":"5 pages, 1 figures, Accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2306.07999v3","updated":"2025-05-22T14:46:11Z","published":"2023-06-13T00:45:54Z","title":"Artificial intelligence in digital pathology: a systematic review and\n  meta-analysis of diagnostic test accuracy","summary":"  Ensuring diagnostic performance of AI models before clinical use is key to\nthe safe and successful adoption of these technologies. Studies reporting AI\napplied to digital pathology images for diagnostic purposes have rapidly\nincreased in number in recent years. The aim of this work is to provide an\noverview of the diagnostic accuracy of AI in digital pathology images from all\nareas of pathology. This systematic review and meta-analysis included\ndiagnostic accuracy studies using any type of artificial intelligence applied\nto whole slide images (WSIs) in any disease type. The reference standard was\ndiagnosis through histopathological assessment and / or immunohistochemistry.\nSearches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We\nidentified 2976 studies, of which 100 were included in the review and 48 in the\nfull meta-analysis. Risk of bias and concerns of applicability were assessed\nusing the QUADAS-2 tool. Data extraction was conducted by two investigators and\nmeta-analysis was performed using a bivariate random effects model. 100 studies\nwere identified for inclusion, equating to over 152,000 whole slide images\n(WSIs) and representing many disease types. Of these, 48 studies were included\nin the meta-analysis. These studies reported a mean sensitivity of 96.3% (CI\n94.1-97.7) and mean specificity of 93.3% (CI 90.5-95.4) for AI. There was\nsubstantial heterogeneity in study design and all 100 studies identified for\ninclusion had at least one area at high or unclear risk of bias. This review\nprovides a broad overview of AI performance across applications in whole slide\nimaging. However, there is huge variability in study design and available\nperformance data, with details around the conduct of the study and make up of\nthe datasets frequently missing. Overall, AI offers good accuracy when applied\nto WSIs but requires more rigorous evaluation of its performance.\n","authors":["Clare McGenity","Emily L Clarke","Charlotte Jennings","Gillian Matthews","Caroline Cartlidge","Henschel Freduah-Agyemang","Deborah D Stocken","Darren Treanor"],"pdf_url":"https://arxiv.org/pdf/2306.07999v3.pdf","comment":"26 pages, 5 figures, 8 tables + Supplementary materials Preprint is\n  pre-peer review version. Please see link for updated, peer reviewed article\n  to see latest version"},{"id":"http://arxiv.org/abs/2505.16732v1","updated":"2025-05-22T14:45:46Z","published":"2025-05-22T14:45:46Z","title":"Sequential Monte Carlo for Policy Optimization in Continuous POMDPs","summary":"  Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty.\n","authors":["Hany Abdulsamad","Sahel Iqbal","Simo Särkkä"],"pdf_url":"https://arxiv.org/pdf/2505.16732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16724v1","updated":"2025-05-22T14:32:56Z","published":"2025-05-22T14:32:56Z","title":"Advancing Brainwave Modeling with a Codebook-Based Foundation Model","summary":"  Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs.\n","authors":["Konstantinos Barmpas","Na Lee","Yannis Panagakis","Dimitrios A. Adamos","Nikolaos Laskaris","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2505.16724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16722v1","updated":"2025-05-22T14:30:14Z","published":"2025-05-22T14:30:14Z","title":"Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification","summary":"  As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.\n","authors":["Himanshu Beniwal","Youngwoo Kim","Maarten Sap","Soham Dan","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2505.16722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03360v3","updated":"2025-05-22T14:27:33Z","published":"2025-03-05T10:40:09Z","title":"Transformers for molecular property prediction: Domain adaptation\n  efficiently improves performance","summary":"  Over the past six years, molecular transformer models have become key tools\nin drug discovery. Most existing models are pre-trained on large, unlabeled\ndatasets such as ZINC or ChEMBL. However, the extent to which large-scale\npre-training improves molecular property prediction remains unclear. This study\nevaluates transformer models for this task while addressing their limitations.\nWe explore how pre-training dataset size and chemically informed objectives\nimpact performance. Our results show that increasing the dataset beyond\napproximately 400K to 800K molecules from large-scale unlabeled databases does\nnot enhance performance across seven datasets covering five ADME endpoints:\nlipophilicity, permeability, solubility (two datasets), microsomal stability\n(two datasets), and plasma protein binding. In contrast, domain adaptation on a\nsmall, domain-specific dataset (less than or equal 4K molecules) using\nmulti-task regression of physicochemical properties significantly boosts\nperformance (P-value less than 0.001). A model pre-trained on 400K molecules\nand adapted with domain-specific data outperforms larger models such as\nMolFormer and performs comparably to MolBERT. Benchmarks against Random Forest\n(RF) baselines using descriptors and Morgan fingerprints show that chemically\nand physically informed features consistently yield better performance across\nmodel types. While RF remains a strong baseline, we identify concrete practices\nto enhance transformer performance. Aligning pre-training and adaptation with\nchemically meaningful tasks and domain-relevant data presents a promising\ndirection for molecular property prediction. Our models are available on\nHuggingFace for easy use and adaptation.\n","authors":["Afnan Sultan","Max Rausch-Dupont","Shahrukh Khan","Olga Kalinina","Dietrich Klakow","Andrea Volkamer"],"pdf_url":"https://arxiv.org/pdf/2503.03360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17045v2","updated":"2025-05-22T14:21:39Z","published":"2024-09-25T15:57:59Z","title":"GeoBiked: A Dataset with Geometric Features and Automated Labeling\n  Techniques to Enable Deep Generative Models in Engineering Design","summary":"  We provide a dataset for enabling Deep Generative Models (DGMs) in\nengineering design and propose methods to automate data labeling by utilizing\nlarge-scale foundation models. GeoBiked is curated to contain 4 355 bicycle\nimages, annotated with structural and technical features and is used to\ninvestigate two automated labeling techniques: The utilization of consolidated\nlatent features (Hyperfeatures) from image-generation models to detect\ngeometric correspondences (e.g. the position of the wheel center) in structural\nimages and the generation of diverse text descriptions for structural images.\nGPT-4o, a vision-language-model (VLM), is instructed to analyze images and\nproduce diverse descriptions aligned with the system-prompt. By representing\ntechnical images as Diffusion-Hyperfeatures, drawing geometric correspondences\nbetween them is possible. The detection accuracy of geometric points in unseen\nsamples is improved by presenting multiple annotated source images. GPT-4o has\nsufficient capabilities to generate accurate descriptions of technical images.\nGrounding the generation only on images leads to diverse descriptions but\ncauses hallucinations, while grounding it on categorical labels restricts the\ndiversity. Using both as input balances creativity and accuracy. Successfully\nusing Hyperfeatures for geometric correspondence suggests that this approach\ncan be used for general point-detection and annotation tasks in technical\nimages. Labeling such images with text descriptions using VLMs is possible, but\ndependent on the models detection capabilities, careful prompt-engineering and\nthe selection of input information. Applying foundation models in engineering\ndesign is largely unexplored. We aim to bridge this gap with a dataset to\nexplore training, finetuning and conditioning DGMs in this field and suggesting\napproaches to bootstrap foundation models to process technical images.\n","authors":["Phillip Mueller","Sebastian Mueller","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2409.17045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16710v1","updated":"2025-05-22T14:11:34Z","published":"2025-05-22T14:11:34Z","title":"Training Long-Context LLMs Efficiently via Chunk-wise Optimization","summary":"  While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}.\n","authors":["Wenhao Li","Yuxin Zhang","Gen Luo","Daohai Yu","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2505.16710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02693v2","updated":"2025-05-22T14:09:01Z","published":"2024-10-03T17:18:37Z","title":"Discovering Spoofing Attempts on Language Model Watermarks","summary":"  LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. Despite recent work\ndemonstrating that state-of-the-art schemes are, in fact, vulnerable to\nspoofing, no prior work has focused on post-hoc methods to discover spoofing\nattempts. In this work, we for the first time propose a reliable statistical\nmethod to distinguish spoofed from genuinely watermarked text, suggesting that\ncurrent spoofing attacks are less effective than previously thought. In\nparticular, we show that regardless of their underlying approach, all current\nlearning-based spoofing methods consistently leave observable artifacts in\nspoofed texts, indicative of watermark forgery. We build upon these findings to\npropose rigorous statistical tests that reliably reveal the presence of such\nartifacts and thus demonstrate that a watermark has been spoofed. Our\nexperimental evaluation shows high test power across all learning-based\nspoofing methods, providing insights into their fundamental limitations and\nsuggesting a way to mitigate this threat. We make all our code available at\nhttps://github.com/eth-sri/watermark-spoofing-detection .\n","authors":["Thibaud Gloaguen","Nikola Jovanović","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.02693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20798v4","updated":"2025-05-22T14:09:00Z","published":"2024-12-30T08:43:28Z","title":"Reconciling Privacy and Explainability in High-Stakes: A Systematic\n  Inquiry","summary":"  Deep learning's preponderance across scientific domains has reshaped\nhigh-stakes decision-making, making it essential to follow rigorous operational\nframeworks that include both Right-to-Privacy (RTP) and Right-to-Explanation\n(RTE). This paper examines the complexities of combining these two\nrequirements. For RTP, we focus on `Differential privacy` (DP), which is\nconsidered the current gold standard for privacy-preserving machine learning\ndue to its strong quantitative guarantee of privacy. For RTE, we focus on\npost-hoc explainers: they are the go-to option for model auditing as they\noperate independently of model training. We formally investigate DP models and\nvarious commonly-used post-hoc explainers: how to evaluate these explainers\nsubject to RTP, and analyze the intrinsic interactions between DP models and\nthese explainers. Furthermore, our work throws light on how RTP and RTE can be\neffectively combined in high-stakes applications. Our study concludes by\noutlining an industrial software pipeline, with the example of a wildly used\nuse-case, that respects both RTP and RTE requirements.\n","authors":["Supriya Manna","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2412.20798v4.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2505.13176v2","updated":"2025-05-22T14:08:07Z","published":"2025-05-19T14:30:46Z","title":"ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models","summary":"  While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.\n","authors":["Zihao Cheng","Hongru Wang","Zeming Liu","Yuhang Guo","Yuanfang Guo","Yunhong Wang","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2505.13176v2.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.16705v1","updated":"2025-05-22T14:06:55Z","published":"2025-05-22T14:06:55Z","title":"An Analysis of Concept Bottleneck Models: Measuring, Understanding, and\n  Mitigating the Impact of Noisy Annotations","summary":"  Concept bottleneck models (CBMs) ensure interpretability by decomposing\npredictions into human interpretable concepts. Yet the annotations used for\ntraining CBMs that enable this transparency are often noisy, and the impact of\nsuch corruption is not well understood. In this study, we present the first\nsystematic study of noise in CBMs and show that even moderate corruption\nsimultaneously impairs prediction performance, interpretability, and the\nintervention effectiveness. Our analysis identifies a susceptible subset of\nconcepts whose accuracy declines far more than the average gap between noisy\nand clean supervision and whose corruption accounts for most performance loss.\nTo mitigate this vulnerability we propose a two-stage framework. During\ntraining, sharpness-aware minimization stabilizes the learning of\nnoise-sensitive concepts. During inference, where clean labels are unavailable,\nwe rank concepts by predictive entropy and correct only the most uncertain\nones, using uncertainty as a proxy for susceptibility. Theoretical analysis and\nextensive ablations elucidate why sharpness-aware training confers robustness\nand why uncertainty reliably identifies susceptible concepts, providing a\nprincipled basis that preserves both interpretability and resilience in the\npresence of noise.\n","authors":["Seonghwan Park","Jueun Mun","Donghyun Oh","Namhoon Lee"],"pdf_url":"https://arxiv.org/pdf/2505.16705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16700v1","updated":"2025-05-22T14:02:37Z","published":"2025-05-22T14:02:37Z","title":"MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use\n  Capabilities in Large Language Models","summary":"  As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.\n","authors":["Xuanqi Gao","Siyi Xie","Juan Zhai","Shqing Ma","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2505.16700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16694v1","updated":"2025-05-22T13:59:30Z","published":"2025-05-22T13:59:30Z","title":"Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase\n  Circuit Emergence","summary":"  Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.\n","authors":["Gouki Minegishi","Hiroki Furuta","Shohei Taniguchi","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2505.16694v1.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2505.16691v1","updated":"2025-05-22T13:57:02Z","published":"2025-05-22T13:57:02Z","title":"EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion","summary":"  Voice Conversion research in recent times has increasingly focused on\nimproving the zero-shot capabilities of existing methods. Despite remarkable\nadvancements, current architectures still tend to struggle in zero-shot\ncross-lingual settings. They are also often unable to generalize for speakers\nof unseen languages and accents. In this paper, we adopt a simple yet effective\napproach that combines discrete speech representations from self-supervised\nmodels with a non-autoregressive Diffusion-Transformer based conditional flow\nmatching speech decoder. We show that this architecture allows us to train a\nvoice-conversion model in a purely textless, self-supervised fashion. Our\ntechnique works without requiring multiple encoders to disentangle speech\nfeatures. Our model also manages to excel in zero-shot cross-lingual settings\neven for unseen languages.\n","authors":["Advait Joglekar","Divyanshu Singh","Rooshil Rohit Bhatia","S. Umesh"],"pdf_url":"https://arxiv.org/pdf/2505.16691v1.pdf","comment":"Submitted to EMNLP 2025, 7 pages, 2 figures, 5 Tables"},{"id":"http://arxiv.org/abs/2505.16690v1","updated":"2025-05-22T13:55:39Z","published":"2025-05-22T13:55:39Z","title":"Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator","summary":"  Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.\n","authors":["Beier Luo","Shuoyuan Wang","Yixuan Li","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2505.16690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16686v1","updated":"2025-05-22T13:53:50Z","published":"2025-05-22T13:53:50Z","title":"SPaRC: A Spatial Pathfinding Reasoning Challenge","summary":"  Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving.\n","authors":["Lars Benedikt Kaesberg","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2505.16686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02435v2","updated":"2025-05-22T13:51:17Z","published":"2025-05-05T08:01:56Z","title":"A New Approach to Backtracking Counterfactual Explanations: A Unified\n  Causal Framework for Efficient Model Interpretability","summary":"  Counterfactual explanations enhance interpretability by identifying\nalternative inputs that produce different outputs, offering localized insights\ninto model decisions. However, traditional methods often neglect causal\nrelationships, leading to unrealistic examples. While newer approaches\nintegrate causality, they are computationally expensive. To address these\nchallenges, we propose an efficient method called BRACE based on backtracking\ncounterfactuals that incorporates causal reasoning to generate actionable\nexplanations. We first examine the limitations of existing methods and then\nintroduce our novel approach and its features. We also explore the relationship\nbetween our method and previous techniques, demonstrating that it generalizes\nthem in specific scenarios. Finally, experiments show that our method provides\ndeeper insights into model outputs.\n","authors":["Pouria Fatemi","Ehsan Sharifian","Mohammad Hossein Yassaee"],"pdf_url":"https://arxiv.org/pdf/2505.02435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09619v3","updated":"2025-05-22T13:49:33Z","published":"2025-04-07T14:07:05Z","title":"Machine Learning Solutions Integrated in an IoT Healthcare Platform for\n  Heart Failure Risk Stratification","summary":"  The management of chronic Heart Failure (HF) presents significant challenges\nin modern healthcare, requiring continuous monitoring, early detection of\nexacerbations, and personalized treatment strategies. In this paper, we present\na predictive model founded on Machine Learning (ML) techniques to identify\npatients at HF risk. This model is an ensemble learning approach, a modified\nstacking technique, that uses two specialized models leveraging clinical and\nechocardiographic features and then a meta-model to combine the predictions of\nthese two models. We initially assess the model on a real dataset and the\nobtained results suggest that it performs well in the stratification of\npatients at HR risk. Specifically, we obtained high sensitivity (95\\%),\nensuring that nearly all high-risk patients are identified. As for accuracy, we\nobtained 84\\%, which can be considered moderate in some ML contexts. However,\nit is acceptable given our priority of identifying patients at risk of HF\nbecause they will be asked to participate in the telemonitoring program of the\nPrediHealth research project on which some of the authors of this paper are\nworking. The initial findings also suggest that ML-based risk stratification\nmodels can serve as valuable decision-support tools not only in the PrediHealth\nproject but also for healthcare professionals, aiding in early intervention and\npersonalized patient management. To have a better understanding of the value\nand of potentiality of our predictive model, we also contrasted its results\nwith those obtained by using three baseline models. The preliminary results\nindicate that our predictive model outperforms these baselines that flatly\nconsider features, \\ie not grouping them in clinical and echocardiographic\nfeatures.\n","authors":["Pietro Cassieri","Aiman Faiz","Anna Maria De Roberto","Claudio Pascarelli","Gianvito Mitrano","Gianluca Fimiani","Marina Garofano","Genoveffa Tortora","Mariangela Lazoi","Claudio Passino","Alessia Bramanti","Giuseppe Scanniello"],"pdf_url":"https://arxiv.org/pdf/2505.09619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16679v1","updated":"2025-05-22T13:45:35Z","published":"2025-05-22T13:45:35Z","title":"Semantic Compression of 3D Objects for Open and Collaborative Virtual\n  Worlds","summary":"  Traditional methods for 3D object compression operate only on structural\ninformation within the object vertices, polygons, and textures. These methods\nare effective at compression rates up to 10x for standard object sizes but\nquickly deteriorate at higher compression rates with texture artifacts,\nlow-polygon counts, and mesh gaps. In contrast, semantic compression ignores\nstructural information and operates directly on the core concepts to push to\nextreme levels of compression. In addition, it uses natural language as its\nstorage format, which makes it natively human-readable and a natural fit for\nemerging applications built around large-scale, collaborative projects within\naugmented and virtual reality. It deprioritizes structural information like\nlocation, size, and orientation and predicts the missing information with\nstate-of-the-art deep generative models. In this work, we construct a pipeline\nfor 3D semantic compression from public generative models and explore the\nquality-compression frontier for 3D object compression. We apply this pipeline\nto achieve rates as high as 105x for 3D objects taken from the Objaverse\ndataset and show that semantic compression can outperform traditional methods\nin the important quality-preserving region around 100x compression.\n","authors":["Jordan Dotzel","Tony Montes","Mohamed S. Abdelfattah","Zhiru Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16679v1.pdf","comment":"First two authors have equal contribution"},{"id":"http://arxiv.org/abs/2505.16673v1","updated":"2025-05-22T13:39:32Z","published":"2025-05-22T13:39:32Z","title":"R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO","summary":"  In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.\n","authors":["Huanjin Yao","Qixiang Yin","Jingyi Zhang","Min Yang","Yibo Wang","Wenhao Wu","Fei Su","Li Shen","Minghui Qiu","Dacheng Tao","Jiaxing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.16673v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2505.16670v1","updated":"2025-05-22T13:36:00Z","published":"2025-05-22T13:36:00Z","title":"BitHydra: Towards Bit-flip Inference Cost Attack against Large Language\n  Models","summary":"  Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs.\n","authors":["Xiaobei Yan","Yiming Li","Zhaoxin Fan","Han Qiu","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16667v1","updated":"2025-05-22T13:32:39Z","published":"2025-05-22T13:32:39Z","title":"ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive\n  Programming","summary":"  While recent research increasingly emphasizes the value of human-LLM\ncollaboration in competitive programming and proposes numerous empirical\nmethods, a comprehensive understanding remains elusive due to the fragmented\nnature of existing studies and their use of diverse, application-specific human\nfeedback. Thus, our work serves a three-fold purpose: First, we present the\nfirst taxonomy of human feedback consolidating the entire programming process,\nwhich promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a\nnovel programming dataset specifically designed for human-LLM collaboration,\nmeticulously annotated to enable large-scale simulated human feedback and\nfacilitate costeffective real human interaction studies. Third, we introduce\nELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM\ncompetitive programming. With ELABORATION, we pinpoint strengthes and\nweaknesses of existing methods, thereby setting the foundation for future\nimprovement. Our code and dataset are available at\nhttps://github.com/SCUNLP/ELABORATION\n","authors":["Xinwei Yang","Zhaofeng Liu","Chen Huang","Jiashuai Zhang","Tong Zhang","Yifan Zhang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2505.16667v1.pdf","comment":"ACL 2025 Main. Our code and dataset are available at\n  https://github.com/SCUNLP/ELABORATION"},{"id":"http://arxiv.org/abs/2505.16664v1","updated":"2025-05-22T13:28:18Z","published":"2025-05-22T13:28:18Z","title":"End-to-End Framework for Predicting the Remaining Useful Life of\n  Lithium-Ion Batteries","summary":"  Accurate prediction of the Remaining Useful Life (RUL) is essential for\nenabling timely maintenance of lithium-ion batteries, impacting the operational\nefficiency of electric applications that rely on them. This paper proposes a\nRUL prediction approach that leverages data from recent charge-discharge cycles\nto estimate the number of remaining usable cycles. The approach introduces both\na novel signal processing pipeline and a deep learning prediction model. In the\nsignal preprocessing pipeline, a derived capacity feature is computed based on\ncurrent and capacity signals. Alongside original capacity, voltage and current,\nthese features are denoised and enhanced using statistical metrics and a\ndelta-based method to capture differences between the current and previous\ncycles. In the prediction model, the processed features are then fed into a\nhybrid deep learning architecture composed of 1D Convolutional Neural Networks\n(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential\nEquation-based LSTM (ODE-LSTM) modules. This architecture is designed to\ncapture both local signal characteristics and long-range temporal dependencies\nwhile modeling the continuous-time dynamics of battery degradation. The model\nis further evaluated using transfer learning across different learning\nstrategies and target data partitioning scenarios. Results indicate that the\nmodel maintains robust performance, even when fine-tuned on limited target\ndata. Experimental results on two publicly available large-scale datasets\ndemonstrate that the proposed method outperforms a baseline deep learning\napproach and machine learning techniques, achieving an RMSE of 101.59,\nhighlighting its strong potential for real-world RUL prediction applications.\n","authors":["Khoa Tran","Tri Le","Bao Huynh","Hung-Cuong Trinh","Vy-Rin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2505.16664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10477v2","updated":"2025-05-22T13:27:22Z","published":"2025-02-13T19:32:41Z","title":"Knowledge Integration Strategies in Autonomous Vehicle Prediction and\n  Planning: A Comprehensive Survey","summary":"  This comprehensive survey examines the integration of knowledge-based\napproaches in autonomous driving systems, specifically focusing on trajectory\nprediction and planning. We extensively analyze various methodologies for\nincorporating domain knowledge, traffic rules, and commonsense reasoning into\nautonomous driving systems. The survey categorizes and analyzes approaches\nbased on their knowledge representation and integration methods, ranging from\npurely symbolic to hybrid neuro-symbolic architectures. We examine recent\ndevelopments in logic programming, foundation models for knowledge\nrepresentation, reinforcement learning frameworks, and other emerging\ntechnologies incorporating domain knowledge. This work systematically reviews\nrecent approaches, identifying key challenges, opportunities, and future\nresearch directions in knowledge-enhanced autonomous driving systems. Our\nanalysis reveals emerging trends in the field, including the increasing\nimportance of interpretable AI, the role of formal verification in\nsafety-critical systems, and the potential of hybrid approaches that combine\ntraditional knowledge representation with modern machine learning techniques.\n","authors":["Kumar Manas","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2502.10477v2.pdf","comment":"Accepted for publication in Proceedings of the IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca - Romania, 22-25 June 2025"},{"id":"http://arxiv.org/abs/2410.03055v2","updated":"2025-05-22T13:26:41Z","published":"2024-10-04T00:25:43Z","title":"Permissive Information-Flow Analysis for Large Language Models","summary":"  Large Language Models (LLMs) are rapidly becoming commodity components of\nlarger software systems. This poses natural security and privacy problems:\npoisoned data retrieved from one component can change the model's behavior and\ncompromise the entire system, including coercing the model to spread\nconfidential data to untrusted components. One promising approach is to tackle\nthis problem at the system level via dynamic information flow (aka taint)\ntracking. Unfortunately, this approach of propagating the most restrictive\ninput label to the output is too conservative for applications where LLMs\noperate on inputs retrieved from diverse sources. In this paper, we propose a\nnovel, more permissive approach to propagate information flow labels through\nLLM queries. The key idea behind our approach is to propagate only the labels\nof the samples that were influential in generating the model output and to\neliminate the labels of unnecessary inputs. We implement and investigate the\neffectiveness of two variations of this approach, based on (i) prompt-based\nretrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We\ncompare these with a baseline that uses introspection to predict the output\nlabel. Our experimental results in an LLM agent setting show that the\npermissive label propagator improves over the baseline in more than 85% of the\ncases, which underscores the practicality of our approach.\n","authors":["Shoaib Ahmed Siddiqui","Radhika Gaonkar","Boris Köpf","David Krueger","Andrew Paverd","Ahmed Salem","Shruti Tople","Lukas Wutschitz","Menglin Xia","Santiago Zanella-Béguelin"],"pdf_url":"https://arxiv.org/pdf/2410.03055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16660v1","updated":"2025-05-22T13:24:52Z","published":"2025-05-22T13:24:52Z","title":"Can reasoning models comprehend mathematical problems in Chinese ancient\n  texts? An empirical study based on data from Suanjing Shishu","summary":"  This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.\n","authors":["Liu Chang","Wang Dongbo","Liu liu","Zhao Zhixiao"],"pdf_url":"https://arxiv.org/pdf/2505.16660v1.pdf","comment":"29pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.16648v1","updated":"2025-05-22T13:18:45Z","published":"2025-05-22T13:18:45Z","title":"Collaboration among Multiple Large Language Models for Medical Question\n  Answering","summary":"  Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.\n","authors":["Kexin Shang","Chia-Hsuan Chang","Christopher C. Yang"],"pdf_url":"https://arxiv.org/pdf/2505.16648v1.pdf","comment":"Accepted to IEEE International Conference on Healthcare Informatics\n  2025"},{"id":"http://arxiv.org/abs/2505.16647v1","updated":"2025-05-22T13:18:44Z","published":"2025-05-22T13:18:44Z","title":"Point, Detect, Count: Multi-Task Medical Image Understanding with\n  Instruction-Tuned Vision-Language Models","summary":"  We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.\n","authors":["Sushant Gautam","Michael A. Riegler","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2505.16647v1.pdf","comment":"Accepted as a full paper at the 38th IEEE International Symposium on\n  Computer-Based Medical Systems (CBMS) 2025"},{"id":"http://arxiv.org/abs/2505.16646v1","updated":"2025-05-22T13:18:24Z","published":"2025-05-22T13:18:24Z","title":"SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment\n  for LLMs' Mathematical Problem Solving","summary":"  Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance.\n","authors":["Yujie Hou","Ting Zhang","Mei Wang","Xuetao Ma","Hu Huang"],"pdf_url":"https://arxiv.org/pdf/2505.16646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13865v3","updated":"2025-05-22T13:18:14Z","published":"2024-11-21T06:01:47Z","title":"Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for\n  Exploration and Exploitation in Recommender Systems","summary":"  Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a semantic-enhanced\nhierarchical mechanism that aligns rich textual descriptions processed by large\nlanguage models (LLMs) with collaborative information directly in hyperbolic\nspace, allowing for more nuanced updates that respect the underlying\nhierarchical structure in user-item profiles; (2) an automatic hierarchical\nrepresentation by optimizing Dasgupta's cost, which discovers hierarchical\nstructures without requiring predefined hyperparameters, enabling\nuser-adjustable exploration-exploitation trade-offs. Extensive experiments\ndemonstrate that HERec consistently outperforms both Euclidean and hyperbolic\nbaselines, achieving up to 5.49% improvement in utility metrics and 11.39%\nincrease in diversity metrics, effectively mitigating information cocoons. We\nopen-source our model implementation at https://github.com/Martin-qyma/HERec.\n","authors":["Qiyao Ma","Menglin Yang","Mingxuan Ju","Tong Zhao","Neil Shah","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2411.13865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16643v1","updated":"2025-05-22T13:16:53Z","published":"2025-05-22T13:16:53Z","title":"From Evaluation to Defense: Advancing Safety in Video Large Language\n  Models","summary":"  While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}\n","authors":["Yiwei Sun","Peiqi Jiang","Chuanbin Liu","Luohao Lin","Zhiying Lu","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2505.16643v1.pdf","comment":"49 pages, 12 figures, 17 tables"},{"id":"http://arxiv.org/abs/2505.16640v1","updated":"2025-05-22T13:12:46Z","published":"2025-05-22T13:12:46Z","title":"BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via\n  Objective-Decoupled Optimization","summary":"  Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/.\n","authors":["Xueyang Zhou","Guiyao Tie","Guowen Zhang","Hechang Wang","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2505.16640v1.pdf","comment":"19 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.16637v1","updated":"2025-05-22T13:08:25Z","published":"2025-05-22T13:08:25Z","title":"SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine\n  Translation","summary":"  Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.\n","authors":["Wenjie Yang","Mao Zheng","Mingyang Song","Zheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.16637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01008v4","updated":"2025-05-22T13:05:38Z","published":"2023-12-13T17:05:37Z","title":"Fast Sampling Through The Reuse Of Attention Maps In Diffusion Models","summary":"  Text-to-image diffusion models have demonstrated unprecedented capabilities\nfor flexible and realistic image synthesis. Nevertheless, these models rely on\na time-consuming sampling procedure, which has motivated attempts to reduce\ntheir latency. When improving efficiency, researchers often use the original\ndiffusion model to train an additional network designed specifically for fast\nimage generation. In contrast, our approach seeks to reduce latency directly,\nwithout any retraining, fine-tuning, or knowledge distillation. In particular,\nwe find the repeated calculation of attention maps to be costly yet redundant,\nand instead suggest reusing them during sampling. Our specific reuse strategies\nare based on ODE theory, which implies that the later a map is reused, the\nsmaller the distortion in the final image. We empirically compare our reuse\nstrategies with few-step sampling procedures of comparable latency, finding\nthat reuse generates images that are closer to those produced by the original\nhigh-latency diffusion model.\n","authors":["Rosco Hunter","Łukasz Dudziak","Mohamed S. Abdelfattah","Abhinav Mehrotra","Sourav Bhattacharya","Hongkai Wen"],"pdf_url":"https://arxiv.org/pdf/2401.01008v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16630v1","updated":"2025-05-22T13:01:51Z","published":"2025-05-22T13:01:51Z","title":"SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game\n  Understanding","summary":"  The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat\n","authors":["Sushant Gautam","Cise Midoglu","Vajira Thambawita","Michael A. Riegler","Pål Halvorsen","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2505.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07581v2","updated":"2025-05-22T13:01:39Z","published":"2025-05-12T14:05:17Z","title":"YuLan-OneSim: Towards the Next Generation of Social Simulator with Large\n  Language Models","summary":"  Leveraging large language model (LLM) based agents to simulate human social\nbehaviors has recently gained significant attention. In this paper, we\nintroduce a novel social simulator called YuLan-OneSim. Compared to previous\nworks, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free\nscenario construction: Users can simply describe and refine their simulation\nscenarios through natural language interactions with our simulator. All\nsimulation code is automatically generated, significantly reducing the need for\nprogramming expertise. (2) Comprehensive default scenarios: We implement 50\ndefault simulation scenarios spanning 8 domains, including economics,\nsociology, politics, psychology, organization, demographics, law, and\ncommunication, broadening access for a diverse range of social researchers. (3)\nEvolvable simulation: Our simulator is capable of receiving external feedback\nand automatically fine-tuning the backbone LLMs, significantly enhancing the\nsimulation quality. (4) Large-scale simulation: By developing a fully\nresponsive agent framework and a distributed simulation architecture, our\nsimulator can handle up to 100,000 agents, ensuring more stable and reliable\nsimulation results. (5) AI social researcher: Leveraging the above features, we\ndevelop an AI social researcher. Users only need to propose a research topic,\nand the AI researcher will automatically analyze the input, construct\nsimulation environments, summarize results, generate technical reports, review\nand refine the reports--completing the social science research loop. To\ndemonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate\nthe quality of the automatically generated scenarios, the reliability,\nefficiency, and scalability of the simulation process, as well as the\nperformance of the AI social researcher.\n","authors":["Lei Wang","Heyang Gao","Xiaohe Bo","Xu Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.07581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16619v1","updated":"2025-05-22T12:52:34Z","published":"2025-05-22T12:52:34Z","title":"Open and Sustainable AI: challenges, opportunities and the road ahead in\n  the life sciences","summary":"  Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.\n","authors":["Gavin Farrell","Eleni Adamidi","Rafael Andrade Buono","Mihail Anton","Omar Abdelghani Attafi","Salvador Capella Gutierrez","Emidio Capriotti","Leyla Jael Castro","Davide Cirillo","Lisa Crossman","Christophe Dessimoz","Alexandros Dimopoulos","Raul Fernandez-Diaz","Styliani-Christina Fragkouli","Carole Goble","Wei Gu","John M. Hancock","Alireza Khanteymoori","Tom Lenaerts","Fabio G. Liberante","Peter Maccallum","Alexander Miguel Monzon","Magnus Palmblad","Lucy Poveda","Ovidiu Radulescu","Denis C. Shields","Shoaib Sufi","Thanasis Vergoulis","Fotis Psomopoulos","Silvio C. E. Tosatto"],"pdf_url":"https://arxiv.org/pdf/2505.16619v1.pdf","comment":"1 PDF, 24 Pages, 2 figures within. Co-corresponding authors:\n  Institute of Applied Biosciences, Centre for Research and Technology Hellas,\n  Thessaloniki, Greece and Department of Biomedical Sciences, University of\n  Padova, Padova, Italy. E-mails: fpsom@certh.gr, silvio.tosatto@unipd.it"},{"id":"http://arxiv.org/abs/2410.10481v3","updated":"2025-05-22T12:52:24Z","published":"2024-10-14T13:18:20Z","title":"Model-based Large Language Model Customization as Service","summary":"  Prominent Large Language Model (LLM) services from providers like OpenAI and\nGoogle excel at general tasks but often underperform on domain-specific\napplications. Current customization services for these LLMs typically require\nusers to upload data for fine-tuning, posing significant privacy risks. While\ndifferentially private (DP) data synthesis presents a potential alternative,\nits application commonly results in low effectiveness due to the introduction\nof excessive noise on data for DP. To overcome this, we introduce Llamdex, a\nnovel framework that facilitates LLM customization as a service, where the\nclient uploads pre-trained domain-specific models rather than data. This\nclient-uploaded model, optionally protected by DP with much lower noise, is\ninserted into the base LLM via connection modules. Significantly, these\nconnecting modules are trained without requiring sensitive domain data,\nenabling clients to customize LLM services while preserving data privacy.\nExperiments demonstrate that Llamdex improves domain-specific accuracy by up to\n26\\% over state-of-the-art private data synthesis methods under identical\nprivacy constraints and, by obviating the need for users to provide domain\ncontext within queries, maintains inference efficiency comparable to the\noriginal LLM service.\n","authors":["Zhaomin Wu","Jizhou Guo","Junyi Hou","Bingsheng He","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10063v2","updated":"2025-05-22T12:49:21Z","published":"2025-04-14T10:06:27Z","title":"Hallucination Detection in LLMs with Topological Divergence on Attention\n  Graphs","summary":"  Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments - including evaluation on\nquestion answering and summarization tasks - show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks while requiring\nminimal annotated data and computational resources. Our findings suggest that\nanalyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.\n","authors":["Alexandra Bazarova","Aleksandr Yugay","Andrey Shulga","Alina Ermilova","Andrei Volodichev","Konstantin Polev","Julia Belikova","Rauf Parchiev","Dmitry Simakov","Maxim Savchenko","Andrey Savchenko","Serguei Barannikov","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2504.10063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16612v1","updated":"2025-05-22T12:47:16Z","published":"2025-05-22T12:47:16Z","title":"Steering Large Language Models for Machine Translation Personalization","summary":"  High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.\n","authors":["Daniel Scalena","Gabriele Sarti","Arianna Bisazza","Elisabetta Fersini","Malvina Nissim"],"pdf_url":"https://arxiv.org/pdf/2505.16612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18225v3","updated":"2025-05-22T12:46:31Z","published":"2025-02-23T20:50:08Z","title":"Liver Cirrhosis Stage Estimation from MRI with Deep Learning","summary":"  We present an end-to-end deep learning framework for automated liver\ncirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe\nscarring (fibrosis) of the liver and a common endpoint of various chronic liver\ndiseases. Early diagnosis is vital to prevent complications such as\ndecompensation and cancer, which significantly decreases life expectancy.\nHowever, diagnosing cirrhosis in its early stages is challenging, and patients\noften present with life-threatening complications. Our approach integrates\nmulti-scale feature learning with sequence-specific attention mechanisms to\ncapture subtle tissue variations across cirrhosis progression stages. Using\nCirrMRI600+, a large-scale publicly available dataset of 628 high-resolution\nMRI scans from 339 patients, we demonstrate state-of-the-art performance in\nthree-stage cirrhosis classification. Our best model achieves 72.8% accuracy on\nT1W and 63.8% on T2W sequences, significantly outperforming traditional\nradiomics-based approaches. Through extensive ablation studies, we show that\nour architecture effectively learns stage-specific imaging biomarkers. We\nestablish new benchmarks for automated cirrhosis staging and provide insights\nfor developing clinically applicable deep learning systems. The source code\nwill be available at https://github.com/JunZengz/CirrhosisStage.\n","authors":["Jun Zeng","Debesh Jha","Ertugrul Aktas","Elif Keles","Alpay Medetalibeyoglu","Matthew Antalek","Federica Proietto Salanitri","Amir A. Borhani","Daniela P. Ladner","Gorkem Durak","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2502.18225v3.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.07523v2","updated":"2025-05-22T12:39:27Z","published":"2025-02-11T12:55:32Z","title":"Scaling Off-Policy Reinforcement Learning with Batch and Weight\n  Normalization","summary":"  Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics, which are\nemphasized by higher UTD ratios. To address these, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nhas been shown to prevent potential loss of plasticity and keeps the effective\nlearning rate constant. Our proposed approach reliably scales with increasing\nUTD ratios, achieving competitive performance across 25 challenging continuous\ncontrol tasks on the DeepMind Control Suite and Myosuite benchmarks, notably\nthe complex dog and humanoid environments. This work eliminates the need for\ndrastic interventions, such as network resets, and offers a simple yet robust\npathway for improving sample efficiency and scalability in model-free\nreinforcement learning.\n","authors":["Daniel Palenicek","Florian Vogt","Joe Watson","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.07523v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16596v1","updated":"2025-05-22T12:31:18Z","published":"2025-05-22T12:31:18Z","title":"Safe Uncertainty-Aware Learning of Robotic Suturing","summary":"  Robot-Assisted Minimally Invasive Surgery is currently fully manually\ncontrolled by a trained surgeon. Automating this has great potential for\nalleviating issues, e.g., physical strain, highly repetitive tasks, and\nshortages of trained surgeons. For these reasons, recent works have utilized\nArtificial Intelligence methods, which show promising adaptability. Despite\nthese advances, there is skepticism of these methods because they lack\nexplainability and robust safety guarantees. This paper presents a framework\nfor a safe, uncertainty-aware learning method. We train an Ensemble Model of\nDiffusion Policies using expert demonstrations of needle insertion. Using an\nEnsemble model, we can quantify the policy's epistemic uncertainty, which is\nused to determine Out-Of-Distribution scenarios. This allows the system to\nrelease control back to the surgeon in the event of an unsafe scenario.\nAdditionally, we implement a model-free Control Barrier Function to place\nformal safety guarantees on the predicted action. We experimentally evaluate\nour proposed framework using a state-of-the-art robotic suturing simulator. We\nevaluate multiple scenarios, such as dropping the needle, moving the camera,\nand moving the phantom. The learned policy is robust to these perturbations,\nshowing corrective behaviors and generalization, and it is possible to detect\nOut-Of-Distribution scenarios. We further demonstrate that the Control Barrier\nFunction successfully limits the action to remain within our specified safety\nset in the case of unsafe predictions.\n","authors":["Wilbert Peter Empleo","Yitaek Kim","Hansoul Kim","Thiusius Rajeeth Savarimuthu","Iñigo Iturrate"],"pdf_url":"https://arxiv.org/pdf/2505.16596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13079v3","updated":"2025-05-22T12:28:06Z","published":"2024-11-20T07:07:42Z","title":"Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback","summary":"  Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.\n","authors":["Feng Gao","Chao Yu","Yu Wang","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13079v3.pdf","comment":"Accepted by RA-L"},{"id":"http://arxiv.org/abs/2502.11018v2","updated":"2025-05-22T12:26:08Z","published":"2025-02-16T07:06:00Z","title":"GRIFFIN: Effective Token Alignment for Faster Speculative Decoding","summary":"  Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating multiple draft tokens simultaneously. However, existing methods\noften struggle with token misalignment between the training and decoding\nphases, limiting their performance. To address this, we propose GRIFFIN, a\nnovel framework that incorporates a token-alignable training strategy and a\ntoken-alignable draft model to mitigate misalignment. The training strategy\nemploys a loss masking mechanism to exclude highly misaligned tokens during\ntraining, preventing them from negatively impacting the draft model's\noptimization. The token-alignable draft model introduces input tokens to\ncorrect inconsistencies in generated features. Experiments on LLaMA, Vicuna,\nQwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance\nlength improvement of over 8% and a speedup ratio exceeding 7%, outperforming\ncurrent speculative decoding state-of-the-art methods. Our code and GRIFFIN's\ndraft models are released publicly in https://github.com/hsj576/GRIFFIN.\n","authors":["Shijing Hu","Jingyang Li","Xingyu Xie","Zhihui Lu","Kim-Chuan Toh","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.11018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03565v3","updated":"2025-05-22T12:24:39Z","published":"2024-10-04T16:15:31Z","title":"Exploration Implies Data Augmentation: Reachability and Generalisation\n  in Contextual MDPs","summary":"  In the zero-shot policy transfer (ZSPT) setting for contextual Markov\ndecision processes (MDP), agents train on a fixed set of contexts and must\ngeneralise to new ones. Recent work has argued and demonstrated that increased\nexploration can improve this generalisation, by training on more states in the\ntraining contexts. In this paper, we demonstrate that training on more states\ncan indeed improve generalisation, but can come at a cost of reducing the\naccuracy of the learned value function which should not benefit generalisation.\nWe hypothesise and demonstrate that using exploration to increase the agent's\ncoverage while also increasing the accuracy improves generalisation even more.\nInspired by this, we propose a method Explore-Go that implements an exploration\nphase at the beginning of each episode, which can be combined with existing on-\nand off-policy RL algorithms and significantly improves generalisation even in\npartially observable MDPs. We demonstrate the effectiveness of Explore-Go when\ncombined with several popular algorithms and show an increase in generalisation\nperformance across several environments. With this, we hope to provide\npractitioners with a simple modification that can improve the generalisation of\ntheir agents.\n","authors":["Max Weltevrede","Caroline Horsch","Matthijs T. J. Spaan","Wendelin Böhmer"],"pdf_url":"https://arxiv.org/pdf/2410.03565v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.08069"},{"id":"http://arxiv.org/abs/2505.16582v1","updated":"2025-05-22T12:17:13Z","published":"2025-05-22T12:17:13Z","title":"O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering","summary":"  Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.\n","authors":["Jianbiao Mei","Tao Hu","Daocheng Fu","Licheng Wen","Xuemeng Yang","Rong Wu","Pinlong Cai","Xing Gao","Yu Yang","Chengjun Xie","Botian Shi","Yong Liu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2505.16582v1.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.16581v1","updated":"2025-05-22T12:15:52Z","published":"2025-05-22T12:15:52Z","title":"How Ensembles of Distilled Policies Improve Generalisation in\n  Reinforcement Learning","summary":"  In the zero-shot policy transfer setting in reinforcement learning, the goal\nis to train an agent on a fixed set of training environments so that it can\ngeneralise to similar, but unseen, testing environments. Previous work has\nshown that policy distillation after training can sometimes produce a policy\nthat outperforms the original in the testing environments. However, it is not\nyet entirely clear why that is, or what data should be used to distil the\npolicy. In this paper, we prove, under certain assumptions, a generalisation\nbound for policy distillation after training. The theory provides two practical\ninsights: for improved generalisation, you should 1) train an ensemble of\ndistilled policies, and 2) distil it on as much data from the training\nenvironments as possible. We empirically verify that these insights hold in\nmore general settings, when the assumptions required for the theory no longer\nhold. Finally, we demonstrate that an ensemble of policies distilled on a\ndiverse dataset can generalise significantly better than the original agent.\n","authors":["Max Weltevrede","Moritz A. Zanger","Matthijs T. J. Spaan","Wendelin Böhmer"],"pdf_url":"https://arxiv.org/pdf/2505.16581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16579v1","updated":"2025-05-22T12:14:23Z","published":"2025-05-22T12:14:23Z","title":"Bridging the Dynamic Perception Gap: Training-Free Draft\n  Chain-of-Thought for Dynamic Multimodal Spatial Reasoning","summary":"  While chains-of-thought (CoT) have advanced complex reasoning in multimodal\nlarge language models (MLLMs), existing methods remain confined to text or\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\ninput images, significantly outperforms conventional approaches, offering new\ninsights into spatial reasoning in evolving environments. To generalize this\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\nframework that seamlessly integrates textual CoT with corresponding visual\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\nenhances performance across diverse tasks, establishing a robust baseline for\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\nat https://github.com/Cratileo/D2R.\n","authors":["Siqu Ou","Hongcheng Liu","Pingjie Wang","Yusheng Liao","Chuan Xuan","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16579v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.16573v1","updated":"2025-05-22T12:04:10Z","published":"2025-05-22T12:04:10Z","title":"From Local Patterns to Global Understanding: Cross-Stock Trend\n  Integration for Enhanced Predictive Modeling","summary":"  Stock price prediction is a critical area of financial forecasting,\ntraditionally approached by training models using the historical price data of\nindividual stocks. While these models effectively capture single-stock\npatterns, they fail to leverage potential correlations among stock trends,\nwhich could improve predictive performance. Current single-stock learning\nmethods are thus limited in their ability to provide a broader understanding of\nprice dynamics across multiple stocks. To address this, we propose a novel\nmethod that merges local patterns into a global understanding through\ncross-stock pattern integration. Our strategy is inspired by Federated Learning\n(FL), a paradigm designed for decentralized model training. FL enables\ncollaborative learning across distributed datasets without sharing raw data,\nfacilitating the aggregation of global insights while preserving data privacy.\nIn our adaptation, we train models on individual stock data and iteratively\nmerge them to create a unified global model. This global model is subsequently\nfine-tuned on specific stock data to retain local relevance. The proposed\nstrategy enables parallel training of individual stock models, facilitating\nefficient utilization of computational resources and reducing overall training\ntime. We conducted extensive experiments to evaluate the proposed method,\ndemonstrating that it outperforms benchmark models and enhances the predictive\ncapabilities of state-of-the-art approaches. Our results highlight the efficacy\nof Cross-Stock Trend Integration (CSTI) in advancing stock price prediction,\noffering a robust alternative to traditional single-stock learning\nmethodologies.\n","authors":["Yi Hu","Hanchi Ren","Jingjing Deng","Xianghua Xie"],"pdf_url":"https://arxiv.org/pdf/2505.16573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03188v2","updated":"2025-05-22T12:00:52Z","published":"2024-12-04T10:20:21Z","title":"Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for\n  Traffic Prediction","summary":"  In smart mobility, large networks of geographically distributed sensors\nproduce vast amounts of high-frequency spatio-temporal data that must be\nprocessed in real time to avoid major disruptions. Traditional centralized\napproaches are increasingly unsuitable to this task, as they struggle to scale\nwith expanding sensor networks, and reliability issues in central components\ncan easily affect the whole deployment. To address these challenges, we explore\nand adapt semi-decentralized training techniques for Spatio-Temporal Graph\nNeural Networks (ST-GNNs) in smart mobility domain. We implement a simulation\nframework where sensors are grouped by proximity into multiple cloudlets, each\nhandling a subgraph of the traffic graph, fetching node features from other\ncloudlets to train its own local ST-GNN model, and exchanging model updates\nwith other cloudlets to ensure consistency, enhancing scalability and removing\nreliance on a centralized aggregator. We perform extensive comparative\nevaluation of four different ST-GNN training setups -- centralized, traditional\nFL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the\nMETR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed\npredictions. Experimental results show that semi-decentralized setups are\ncomparable to centralized approaches in performance metrics, while offering\nadvantages in terms of scalability and fault tolerance. In addition, we\nhighlight often overlooked issues in existing literature for distributed\nST-GNNs, such as the variation in model performance across different\ngeographical areas due to region-specific traffic patterns, and the significant\ncommunication overhead and computational costs that arise from the large\nreceptive field of GNNs, leading to substantial data transfers and increased\ncomputation of partial embeddings.\n","authors":["Ivan Kralj","Lodovico Giaretta","Gordan Ježić","Ivana Podnar Žarko","Šarūnas Girdzijauskas"],"pdf_url":"https://arxiv.org/pdf/2412.03188v2.pdf","comment":"9 pages, 4 figures, 3 tables, conference"},{"id":"http://arxiv.org/abs/2505.16567v1","updated":"2025-05-22T11:59:44Z","published":"2025-05-22T11:59:44Z","title":"Finetuning-Activated Backdoors in LLMs","summary":"  Finetuning openly accessible Large Language Models (LLMs) has become standard\npractice for achieving task-specific performance improvements. Until now,\nfinetuning has been regarded as a controlled and secure process in which\ntraining on benign datasets led to predictable behaviors. In this paper, we\ndemonstrate for the first time that an adversary can create poisoned LLMs that\ninitially appear benign but exhibit malicious behaviors once finetuned by\ndownstream users. To this end, our proposed attack, FAB (Finetuning-Activated\nBackdoor), poisons an LLM via meta-learning techniques to simulate downstream\nfinetuning, explicitly optimizing for the emergence of malicious behaviors in\nthe finetuned models. At the same time, the poisoned LLM is regularized to\nretain general capabilities and to exhibit no malicious behaviors prior to\nfinetuning. As a result, when users finetune the seemingly benign model on\ntheir own datasets, they unknowingly trigger its hidden backdoor behavior. We\ndemonstrate the effectiveness of FAB across multiple LLMs and three target\nbehaviors: unsolicited advertising, refusal, and jailbreakability.\nAdditionally, we show that FAB-backdoors are robust to various finetuning\nchoices made by the user (e.g., dataset, number of steps, scheduler). Our\nfindings challenge prevailing assumptions about the security of finetuning,\nrevealing yet another critical attack vector exploiting the complexities of\nLLMs.\n","authors":["Thibaud Gloaguen","Mark Vero","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2505.16567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10945v2","updated":"2025-05-22T11:54:30Z","published":"2025-05-16T07:30:22Z","title":"Semantic Aware Linear Transfer by Recycling Pre-trained Language Models\n  for Cross-lingual Transfer","summary":"  Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures.\n","authors":["Seungyoon Lee","Seongtae Hong","Hyeonseok Moon","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2505.10945v2.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.16561v1","updated":"2025-05-22T11:52:16Z","published":"2025-05-22T11:52:16Z","title":"Auto-nnU-Net: Towards Automated Medical Image Segmentation","summary":"  Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet.\n","authors":["Jannis Becktepe","Leona Hennig","Steffen Oeltze-Jafra","Marius Lindauer"],"pdf_url":"https://arxiv.org/pdf/2505.16561v1.pdf","comment":"31 pages, 19 figures. Accepted for publication at AutoML 2025"},{"id":"http://arxiv.org/abs/2505.16547v1","updated":"2025-05-22T11:37:39Z","published":"2025-05-22T11:37:39Z","title":"Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for\n  Occlusion Aware Plant Manipulation","summary":"  This paper presents an end-to-end deep reinforcement learning (RL) framework\nfor occlusion-aware robotic manipulation in cluttered plant environments. Our\napproach enables a robot to interact with a deformable plant to reveal hidden\nobjects of interest, such as fruits, using multimodal observations. We decouple\nthe kinematic planning problem from robot control to simplify zero-shot\nsim2real transfer for the trained policy. Our results demonstrate that the\ntrained policy, deployed using our framework, achieves up to 86.7% success in\nreal-world trials across diverse initial conditions. Our findings pave the way\ntoward autonomous, perception-driven agricultural robots that intelligently\ninteract with complex foliage plants to \"find the fruit\" in challenging\noccluded scenarios, without the need for explicitly designed geometric and\ndynamic models of every plant scenario.\n","authors":["Nitesh Subedi","Hsin-Jung Yang","Devesh K. Jha","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2505.16547v1.pdf","comment":"18 Pages, 15 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2505.16540v1","updated":"2025-05-22T11:31:56Z","published":"2025-05-22T11:31:56Z","title":"TextureSAM: Towards a Texture Aware Foundation Model for Segmentation","summary":"  Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.\n","authors":["Inbal Cohen","Boaz Meivar","Peihan Tu","Shai Avidan","Gal Oren"],"pdf_url":"https://arxiv.org/pdf/2505.16540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16530v1","updated":"2025-05-22T11:16:46Z","published":"2025-05-22T11:16:46Z","title":"DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection","summary":"  Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.\n","authors":["Yuliang Yan","Haochun Tang","Shuo Yan","Enyan Dai"],"pdf_url":"https://arxiv.org/pdf/2505.16530v1.pdf","comment":null}]}}